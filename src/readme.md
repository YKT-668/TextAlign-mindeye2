å¤ªæ£’äº†ï¼Œå…ˆæŠŠé˜¶æ®µæ€§æˆæœå’Œæ”¹åŠ¨ç»™ä½ ä¸€æ¬¡æ€§ç›˜æ¸…æ¥šğŸ‘‡

# æˆ‘ä»¬å·²ç»å®Œæˆçš„äº‹

* **è®­ç»ƒè·‘é€šï¼ˆå•è¢«è¯•Â·1hï¼‰**
  æˆåŠŸè®­ç»ƒå¹¶ä¿å­˜å¤šç»„é…ç½®ï¼š

  * `s1_ps1p5_h512_e5_cycle`ï¼ˆbaselineï¼‰
  * `s1_ps1p5_h512_e5_cycle_mix`ï¼ˆmixup=0.33ï¼‰
  * å­¦ä¹ ç‡æ‰«æï¼š`â€¦_lr1e4` æˆåŠŸã€`â€¦_lr2e4` å› ç£ç›˜æ»¡æŠ¥é”™ï¼Œéšåç”¨ `â€¦_lr2e4_nockpt`ï¼ˆç¦ç”¨ä¸­é—´ ckpt è½ç›˜ï¼‰è·‘é€šã€‚
* **ç¯å¢ƒ/èµ„æºé—®é¢˜æ’éšœ**

  * è§£å†³äº† `sd_image_var_autoenc.pth` ç¼ºå¤± â†’ æ”¹ä¸º `--no-blurry_recon`ã€‚
  * è§£å†³ç£ç›˜å†™æ»¡ï¼ˆ/ 100%ï¼‰â†’ æ¸…ç† `train_logs`/HF ç¼“å­˜å¹¶æ¢å¤åˆ° 94%â†’ç»§ç»­å¯å†™ã€‚
  * è§£å†³ `nbconvert` å†…æ ¸ç¼ºå¤±ï¼ˆ`mindeye` æ‰¾ä¸åˆ°ï¼‰â†’ æ”¹ç”¨ `--ExecutePreprocessor.kernel_name=python3` æˆ–ç›´æ¥æ”¹ä¸ºè„šæœ¬æ¨ç†ã€‚
  * è§£å†³ `transformers`/`torchvision`/`torch.onnx`/`diffusers` å…¼å®¹é—®é¢˜ä¸ `huggingface_hub.cached_download` å¯¼å…¥é—®é¢˜ï¼ˆé€šè¿‡ç‰ˆæœ¬å¯¹é½/æ›¿ä»£è°ƒç”¨ï¼‰ã€‚
  * å¤„ç†è·¯å¾„é”™è¯¯ï¼šæŠŠ hardcode çš„ `/weka/...` æ”¹ä¸ºæœ¬åœ° `src/...`ï¼Œyaml è·¯å¾„æ”¹ä¸º `src/generative_models/configs/unclip6.yaml`ã€‚
* **æ¨ç†æµç¨‹è·‘é€šå¹¶æ‹¿åˆ°ç»“æœ**

  * ä¸‹è½½ **MindEye2 æä¾›çš„ unCLIP6** æƒé‡ï¼ˆçº¦ 17GBï¼‰ï¼š`src/train_logs/unclip6_epoch0_step110000.ckpt`ã€‚
  * æ”¹é€ /æ‰§è¡Œ `recon_inference_run.py` å®Œæˆ**æ–°æµ‹è¯•é›†**æ¨ç†ï¼ˆæ”¯æŒ `--new_test`ã€`--plot`ã€å¯æ§ä¿å­˜æ•°é‡/æ ¼å¼/ç›®å½•ï¼‰ã€‚
  * å·²ä¿å­˜å¹¶éªŒè¯ç”Ÿæˆç»“æœï¼ˆä½ ç”¨ `--save_images --max_save 10 --image_format png --output_dir ...` çš„æ–¹å¼ç¡®è®¤äº†è½ç›˜è·¯å¾„ï¼‰ã€‚

# å¯¹åŸæ¨¡å‹/ä»“åº“åšè¿‡çš„å…³é”®ä¿®æ”¹

> æ³¨ï¼šå‡ä¸ºâ€œ**æœ€å°ä¾µå…¥å¼**â€çš„å·¥ç¨‹åŒ–æ”¹é€ ï¼Œä¿æŒä¸»å¹²è®­ç»ƒé€»è¾‘ä¸å˜ã€‚

1. **å…³é—­ blurry åˆ†æ”¯**

   * è®­ç»ƒä¸æ¨ç†ç»Ÿä¸€åŠ  `--no-blurry_recon`ï¼Œç»•è¿‡å¯¹ `sd_image_var_autoenc.pth` çš„ä¾èµ–ï¼›åŒæ—¶åœ¨æ—¥å¿—ä¸­æŠŠä¸ blurry ç›¸å…³çš„ loss ç½®é›¶ï¼ˆå¯è§ `train/loss_blurry_* = 0`ï¼‰ã€‚
2. **æ£€æŸ¥ç‚¹å†™ç›˜ç­–ç•¥**

   * é’ˆå¯¹é«˜ LR è¯•éªŒé€ æˆçš„å¤§ ckpt è½ç›˜è§¦å‘ â€œNo space left on deviceâ€ï¼Œå¢åŠ  `--no-ckpt_saving` æ–¹æ¡ˆï¼Œä¿è¯å¤§ LR å®éªŒèƒ½å®Œæ•´è·‘å®Œï¼›åŒæ—¶æˆ‘ä»¬æ¸…ç†äº†å†å² `train_logs/*` å¹¶æŠŠ hf/hub ç¼“å­˜è½¬ç§»åˆ° `$HF_HOME` ä»¥æ§ç›˜ã€‚
3. **è·¯å¾„ä¸é…ç½®ä¿®å¤**

   * æŠŠæ¨ç†è„šæœ¬ä¸­ç¡¬ç¼–ç çš„è¿œç¨‹æ•°æ®è·¯å¾„æ”¹æˆæœ¬åœ°ï¼š

     * betasï¼š`/home/vipuser/MindEyeV2_Project/src/betas_all_subj01_fp32_renorm.hdf5`
     * wdsï¼š`/home/vipuser/MindEyeV2_Project/src/wds/subj01/new_test/0.tar`
   * unCLIP é…ç½®ä¸æƒé‡ï¼š

     * YAML ç”¨ `src/generative_models/configs/unclip6.yaml`
     * CKPT æŒ‡å‘ `src/train_logs/unclip6_epoch0_step110000.ckpt`ï¼ˆè€Œé `$HF_HOME`ï¼‰ã€‚
4. **æ¨ç†è„šæœ¬å¢å¼ºï¼ˆå®ç”¨å‚æ•°ï¼‰**

   * å¢åŠ /ä¿®å¤ï¼š`--plot`ã€`--save_images`ã€`--max_save`ã€`--image_format`ã€`--output_dir` ç­‰å‚æ•°è§£æå’Œé»˜è®¤å€¼ï¼Œé¿å… `NameError: plotting`/è¾“å‡ºä¸è½ç›˜ç­‰é—®é¢˜ã€‚
   * ç¡®ä¿**å‰ 10 å¼ **/è‡ªå®šä¹‰æ•°é‡å¯æ§ä¿å­˜ï¼Œå¹¶æ”¯æŒ PNG/JPG ä¸æŒ‡å®šç›®å½•ï¼ˆå¦‚ `/home/vipuser/MindEyeV2_Project`ï¼‰ã€‚
5. **å…¼å®¹æ€§å¤„ç†**

   * å¤„ç† `torchvision/transformers/diffusers/huggingface_hub` çš„ç‰ˆæœ¬ API å·®å¼‚é—®é¢˜ï¼ˆå¦‚ `cached_download` å·²åºŸå¼ƒã€`_pytree.register_pytree_node` ç­‰ï¼‰å¹¶å¯¹è„šæœ¬åšå…¼å®¹æ”¹å†™ï¼Œä¿è¯åœ¨ä½ å½“å‰ `mindeye` ç¯å¢ƒä¸­å¯ç›´æ¥è¿è¡Œã€‚

# æˆ‘ä»¬â€œå‡†å¤‡è¦åš/æ­£åœ¨åšâ€çš„æ‹“å±•ï¼ˆä¸ä½ çš„æ–¹æ¡ˆå¯¹é½ï¼‰

> è¿™äº›æ˜¯ä¸ºä½ çš„â€œ**Brainâ†’CLIPâ†’æ–‡æœ¬åº“ Top-Kâ†’LLM èåˆç»“æ„åŒ–æç¤ºâ†’unCLIP/SDXL ç”Ÿæˆï¼ˆå¯é€‰ ControlNetï¼‰**â€æ–¹æ¡ˆé“ºè·¯çš„**æ¥å£åŒ–**æ”¹é€ ï¼Œå·²ç»™å‡ºå¯ç›´æ¥åˆ›å»ºçš„è„šæœ¬æ ·æ¿ã€‚

* **è„‘â†’CLIP-image å‘é‡å¯¼å‡ºï¼ˆé¢„æµ‹åµŒå…¥ï¼‰**ï¼š
  æ–°å»º `src/semantic_prompting/brain2clip.py`ï¼ŒæŠŠæ¨¡å‹åœ¨ new_test ä¸Šçš„ **é¢„æµ‹ CLIP-image å‘é‡ [N,1664]** å¯¼å‡ºï¼ˆåç»­åšæ£€ç´¢/LLM èåˆï¼‰ã€‚
* **æ–‡æœ¬åº“ç´¢å¼•æ„å»ºï¼ˆCLIP-text å‘é‡ï¼‰**ï¼š
  æ–°å»º `src/semantic_prompting/text_index.py`ï¼Œç”¨ OpenCLIP å¯¹ `evals/all_captions.pt` ç¼–ç ï¼Œä¿å­˜ `text_index.pt`ã€‚
* **Top-K æ–‡æœ¬æ£€ç´¢**ï¼š
  æ–°å»º `src/semantic_prompting/retrieve_topk.py`ï¼Œç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä»æ–‡æœ¬åº“å–æ¯ä¸ªæ ·æœ¬çš„ Top-K å€™é€‰æè¿°ï¼Œç”Ÿæˆ `topk_texts.jsonl`ï¼ˆRAG çš„è¾“å…¥ï¼‰ã€‚

> ä»¥ä¸Š 3 ä¸ªè„šæœ¬æˆ‘å·²ç»ç»™äº†**å¯ç›´æ¥ç²˜è´´è¿è¡Œ**çš„ç‰ˆæœ¬ï¼ˆä¸Šæ¡æ¶ˆæ¯ï¼‰ã€‚å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘ä»¬ç°åœ¨å°±æŠŠå®ƒä»¬è½ç›˜ã€è·‘ä¸€ä¸‹ï¼Œé©¬ä¸Šå¾—åˆ° Top-K æè¿°å€™é€‰ã€‚


============================
å¥½çš„ï¼Œæˆ‘ç»™ä½ æŠŠè¿™è½®æŠ˜è…¾åšä¸ªâ€œæ¸…æ¥šä¸”å¯å¤ç°â€çš„å°ç»“ğŸ‘‡

# æˆ‘ä»¬å·²å®Œæˆçš„äº‹

* **æ¨ç†ç»Ÿä¸€åŒ–ï¼ˆå…ˆè·‘é€š 1 å¥—ï¼‰**

  * æˆåŠŸå¯¹ `s1_ps1p5_h512_e5_cycle` ä»¥ç»Ÿä¸€å‚æ•°è·‘æ–°æµ‹è¯•é›†å‰ 10 å¼ ï¼šä¿å­˜äº† `images/`ã€`blurry_images/`ã€`captions.txt`ã€‚
  * å…¶ä½™å‡ å¥—ï¼ˆ`*_mix / *_lr1e4 / *_lr2e4_nockpt`ï¼‰ç”±äºç¼ºå°‘ DeepSpeed ZeRO åˆ†ç‰‡ç›®å½•ï¼ˆåªæœ‰ `last.pth` ä¸å«åˆ†ç‰‡ï¼‰ï¼Œå½“å‰**æ— æ³•ç›´æ¥æ¢å¤**ï¼Œæš‚æ—¶è·³è¿‡ã€‚

* **è¯„ä¼°æ•°æ®å‡†å¤‡**

  * ä» HuggingFace ä¸‹è½½å®˜æ–¹è¯„ä¼°åŸºå‡†ï¼š`all_images.pt`ã€`all_captions.pt` åˆ° `src/evals/`ï¼Œå¹¶ä¿®æ­£äº†å¤šä¸€å±‚ `evals/` å­ç›®å½•çš„é—®é¢˜ã€‚
  * ç”¨ OpenCLIP `ViT-L-14@openai` å¯¹ `all_images.pt` ç¼–ç ï¼Œå¾—åˆ° **1000Ã—768** çš„å›¾åƒåµŒå…¥ï¼š`all_images_ViT-L-14_openai.pt`ï¼ˆå·²éªŒè¯å½¢çŠ¶ä¸ dtype æ­£å¸¸ï¼‰ã€‚

* **æ‰“åŒ…ä¸è‡ªæ£€**

  * å°†ä½ å¯¼å‡ºçš„ 10 å¼ é‡å»ºèšåˆä¸º `recons.pt`ï¼ˆ**10Ã—768**ï¼‰å’Œ `ids.json`ï¼ˆå½“å‰æ˜¯ `[0..9]`ï¼Œä»£è¡¨ä¿å­˜é¡ºåºï¼Œå¹¶éçœŸå® GT è¡Œå·ï¼‰ã€‚
  * å¿«é€Ÿæ•°å€¼è‡ªæ£€æ˜¾ç¤ºä¸¤ä¾§å¼ é‡å‡ä¸º `float32`ã€æ—  NaN/Infï¼Œæ•°å€¼èŒƒå›´åˆç†ã€‚

* **è¯„ä¼°ä¸æŒ‡æ ‡å›ºåŒ–ï¼ˆå‘½ä»¤è¡Œç‰ˆï¼‰**

  * ç”±äº Notebook å¤ªå¡ & `ids` æœªå¯¹é½ï¼Œæˆ‘ä»¬é‡‡ç”¨ **ä¸¤ç§å¯å¤ç°æŒ‡æ ‡**ï¼š

    1. **Nearest-Neighbor CLIP ä½™å¼¦ï¼ˆnn_clipï¼‰**ï¼šæ¯ä¸ªé‡å»ºåœ¨ 1000 ä¸ª GT åµŒå…¥ä¸­å–æœ€è¿‘é‚»ç›¸ä¼¼åº¦åæ±‚å‡å€¼ï¼ˆ**æ— éœ€ ids å¯¹é½**ï¼Œé€‚åˆå½“å‰åªè¯„å‰ 10 å¼ ï¼‰ã€‚
    2. **å¯¹è§’å‡å€¼ï¼ˆclip_cosine_diagï¼‰**ï¼šå‡è®¾é¡ºåºå¯¹é½çš„å‚è€ƒå€¼ï¼ˆä»…ä½œ sanityï¼Œä¸è®¡å…¥æ’åï¼‰ã€‚
  * è¿™å¥—æ¨¡å‹çš„ç»“æœï¼š

    * `[sim min/max/mean] = 0.0776 / 0.7612 / 0.5732`
    * **nn_clip_cosine â‰ˆ 0.7437**
    * **clip_cosine_diag â‰ˆ 0.5559**
  * å·²å†™å…¥ï¼š

    * `/train_logs/s1_ps1p5_h512_e5_cycle/metrics_nn.json`
    * å¹¶ç”Ÿæˆæ±‡æ€»ï¼š`/train_logs/metrics_summary.csv`ã€`metrics_summary.md`

# å¯¹åŸæ¨¡å‹/ä»“åº“åšè¿‡çš„â€œä¿®æ”¹â€

* **æ²¡æœ‰æ”¹åŠ¨æ¨¡å‹ç»“æ„ã€æƒé‡æˆ–è®­ç»ƒ/æ¨ç†è„šæœ¬æ ¸å¿ƒé€»è¾‘ã€‚**
  ï¼ˆæ›¾å°è¯•ç”¨ `sitecustomize.py` ç»™ DeepSpeed æ‰“è¡¥ä¸ä»¥å…¼å®¹ `last.pth`ï¼Œä½†å·²åˆ é™¤ï¼›ç›®å‰ä»“åº“å¤„äºåŸå§‹ä»£ç è·¯å¾„ï¼Œ**æ— æŒä¹…æ€§æ”¹åŠ¨**ã€‚ï¼‰

* **æ–°å¢/è½åœ°çš„è¾…åŠ©èµ„äº§ä¸è„šæœ¬**ï¼ˆéƒ½ä¸è¯„ä¼°ç›¸å…³ï¼Œéæ¨¡å‹æœ¬ä½“ï¼‰ï¼š

  * è¯„ä¼°èµ„äº§ï¼š`src/evals/all_images.pt`ã€`src/evals/all_captions.pt`ã€`src/evals/all_images_ViT-L-14_openai.pt`
  * å¿«é€Ÿè¯„ä¼°å°è„šæœ¬ï¼ˆæ”¾åœ¨ `/home/vipuser/`ï¼Œæˆ–é€šè¿‡ä¸€æ¬¡æ€§å‘½ä»¤æ‰§è¡Œï¼‰ï¼š

    * `quick_eval.py` / ä¸€æ¬¡æ€§å‘½ä»¤ï¼šç”¨äº**å½’ä¸€åŒ– â†’ è®¡ç®— Top-k/å¯¹è§’/nn æŒ‡æ ‡ â†’ å†™ metrics**
    * `nn_eval.py`ï¼ˆæ—©æœŸç‰ˆæœ¬æ›¾è¢«ç»ˆç«¯æˆªæ–­å¯¼è‡´æœªå½’ä¸€åŒ–ï¼Œåæ”¹ä¸ºä¸€æ¬¡æ€§å‘½ä»¤æ­£ç¡®è®¡ç®—ï¼›æ–‡ä»¶å¯å¿½ç•¥ï¼‰
  * æ±‡æ€»æ–‡ä»¶ï¼š`/home/vipuser/train_logs/metrics_summary.csv`ã€`metrics_summary.md`

> å°ç»“ï¼š**æ¨¡å‹æœ¬ä½“é›¶æ”¹åŠ¨**ï¼›åªæ˜¯è¡¥é½äº†è¯„ä¼°åŸºå‡†ã€æ‰“é€šäº†å‘½ä»¤è¡Œè¯„ä¼°é“¾è·¯ï¼Œå¹¶äº§å‡ºäº†å¯æ”¶å½•åˆ°è®ºæ–‡/PPTçš„**å®šé‡æ•°å­—**ä¸**å¯å¤ç°å®éªŒè½¨è¿¹**ã€‚

# ç°çŠ¶åˆ¤æ–­

* `s1_ps1p5_h512_e5_cycle`ï¼šNN æŒ‡æ ‡ **0.7437**ï¼Œè¡¨ç°å¥åº·ï¼Œå¯ä½œä¸ºå½“å‰çš„**åŸºçº¿/æœ€ä½³**ï¼ˆåœ¨å…¶å®ƒæ¨¡å‹æœªèƒ½æ¢å¤å‰ï¼‰ã€‚
* `ids.json` ç›®å‰æ˜¯ä¿å­˜åºå·ï¼Œå¹¶éçœŸå® GT è¡Œå· â†’ æ ‡å‡† Top-1/Top-5 å…ˆä¸åšç»“è®ºï¼›å¾…ï¼š

  1. è·‘æ»¡ 1000 å¼ å¹¶ç”¨å®˜æ–¹ notebook/è„šæœ¬è¯„ä¼°ï¼›æˆ–
  2. æ¢å¤æ ·æœ¬é”®ï¼ˆWebDataset keyï¼‰å»ºç«‹ **é‡å»ºâ†”GT** æ˜ å°„åï¼Œå†ç»™å‡º Top-kã€‚

# æ¨èçš„ä¸‹ä¸€æ­¥

1. **æ‰©å±•æ¨ªå‘å¯¹æ¯”**ï¼ˆå¯é€‰ï¼‰
   ä¸€æ—¦å…¶å®ƒæ¨¡å‹èƒ½å¯¼å‡º `recons.pt`ï¼Œç›´æ¥ç”¨æˆ‘ä»¬çš„ä¸€æ¡å‘½ä»¤å‡º `nn_clip` å¹¶**è¿½åŠ **åˆ° `metrics_summary.*`ï¼Œå†åšâ€œæŒ‘ä¼˜â€ã€‚

2. **è¿›å…¥ Phase-2ï¼ˆRAG+LLMï¼‰ç¬¬ 1 æ­¥**
   æˆ‘è¿™è¾¹å·²ç»å‡†å¤‡å¥½â€œæœ€å°å¯è·‘â€çš„ä¸‰ä»¶å¥—ï¼ˆå…¨æ˜¯ `.py`ï¼Œä¸æ”¹æ¨¡å‹ï¼‰ï¼š

   * ä»æ¨¡å‹æš´éœ² `brain_to_clip()` æ¥å£ï¼ˆæˆ–ç­‰ä»·å‡½æ•°ï¼‰ï¼›
   * å°å‹ caption åº“çš„ **Top-K æ£€ç´¢**ï¼ˆOpenCLIP æ–‡æœ¬ç¼–ç ï¼‰ï¼›
   * äº§å‡º**ç»“æ„åŒ–æç¤º JSON**ï¼ˆæ­£/è´Ÿé¢/é£æ ¼æ§½ä½ï¼‰ï¼Œåç»­ç›´è¿ **unCLIP/SDXL** åšå¢å¼ºé‡å»ºã€‚

   ä½ è¦çš„è¯æˆ‘ç›´æ¥æŠŠ **â€œRAG+LLM ç¬¬ 1 æ­¥æ”¹é€ æ¸…å•ï¼ˆæ–‡ä»¶ä½ç½® + å‡½æ•°ç­¾å + æœ€å° Demoï¼‰â€** å‘ä½ ï¼Œç…§è´´å³å¯è·‘é€šã€‚


conda activate /data/mindeye_final ï¼ˆæ–°ç¯å¢ƒï¼Œç»™gen_sdxl_ipadapter_load.pyæ–°å»ºçš„ï¼Œå› ä¸ºç‰ˆæœ¬å†²çªï¼Œåº”è¯¥æ˜¯å»ºåœ¨æ–°åŠ çš„ç¬¬400Gé‚£ä¸ªç›˜ï¼‰
mv /home/vipuser/models/IP-Adapter/sdxl_models/image_encoder \
   /home/vipuser/models/IP-Adapter/sdxl_models/image_encoder_bigg_bak #å¤‡ä»½å½“å‰ï¼ˆ1664, bigGï¼‰ç¼–ç å™¨
   
   (ä½ çš„ image_encoder/config.json æ˜¾ç¤º hidden_size=1664ï¼ˆOpenCLIP ViT-bigGï¼‰ï¼Œè€Œä½ åŠ è½½çš„é€‚é…å™¨æƒé‡æ˜¯ ip-adapter-plus_sdxl_vit-h.safetensorsï¼ˆæœŸæœ› ViT-H/14=1024 ç»´ï¼‰ã€‚è¿™å¯¹ä¸é½å¯¼è‡´çŸ©é˜µä¹˜ç»´åº¦é”™è¯¯ã€‚)

'# æ–¹æ¡ˆ1ï¼ˆç»§ç»­â€œå‚è€ƒå›¾æ¨¡å¼â€ï¼Œä¿®å¤ç£ç›˜ä¸è¶³å†æ‹‰å¯¹é½çš„ ViT-H ç¼–ç å™¨ï¼‰
mkdir -p /data/huggingface_cache /data/tmp
export HF_HOME=/data/huggingface_cache
export HF_HUB_CACHE=/data/huggingface_cache
export TRANSFORMERS_CACHE=/data/huggingface_cache
export TMPDIR=/data/tmp
python - <<'PY'
from huggingface_hub import hf_hub_download
p = hf_hub_download("h94/IP-Adapter","sdxl_models/image_encoder/model.safetensors",
    local_dir="/home/vipuser/models/IP-Adapter", local_dir_use_symlinks=False, force_download=True)
print("saved:", p)
PY
# æˆåŠŸåï¼Œé‡è·‘ä½ åˆšæ‰çš„å‚è€ƒå›¾å‘½ä»¤ï¼ˆref_dir æŒ‡å‘ imagesï¼‰
'åŸæœ¬ç£ç›˜æ»¡äº†æ¢æˆæ–°çš„ç¬¬400Gçš„é‚£ä¸ªç£ç›˜
# 1) ç”¨ä»“åº“é‡Œçš„æœ€æ–°ç‰ˆè¦†ç›– config.jsonï¼ˆä½ åˆšæ‰åªä¸‹äº† model.safetensorsï¼‰
python - <<'PY'
from huggingface_hub import hf_hub_download
p = hf_hub_download("h94/IP-Adapter","sdxl_models/image_encoder/config.json",
    local_dir="/home/vipuser/models/IP-Adapter", local_dir_use_symlinks=False, force_download=True)
print("saved:", p)
PY

# 1) å…ˆæŠŠç°åœ¨çš„ bigG ç¼–ç å™¨æŒªå¼€ï¼Œç•™å‡ºä¸€ä¸ªå¹²å‡€ç›®å½•
mv /home/vipuser/models/IP-Adapter/sdxl_models/image_encoder \
   /home/vipuser/models/IP-Adapter/sdxl_models/image_encoder_bigg_bak
mkdir -p /home/vipuser/models/IP-Adapter/sdxl_models/image_encoder_vith

åˆ›å»º gen_sdxl_ipadapter_plus_embeds.py è„šæœ¬
ä¸€å¼ å›¾çš„é…å¥—å‘½ä»¤
python /home/vipuser/MindEyeV2_Project/tools/gen_sdxl_ipadapter_plus_embeds.py \
  --adapter_dir /home/vipuser/models/IP-Adapter \
  --prompts    /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/prompt_bigG_1.json \
  --out_dir    /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/gen_ip_vith_ref \
  --ref_dir    /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/images \
  --ids_json   /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/ids_local.json \
  --steps 28 --cfg 5.0 --w 1024 --h 1024 --seed 42 --dtype fp16

tools/text_index.py ä¸ºä½ åºå¤§çš„æ–‡æœ¬çŸ¥è¯†åº“å»ºç«‹ä¸€ä¸ªåŸºäºAIè¯­ä¹‰ç†è§£çš„ã€å¯ä¾›å¿«é€Ÿæ£€ç´¢çš„æ•°å­—ç´¢å¼•ã€‚å¯ä»¥æ‹¥æœ‰äº†ä¸€ä¸ªåä¸º text_index_vith.pt çš„æ–‡ä»¶ï¼Œå®ƒåŒ…å«äº† all_captions.pt ä¸­æ‰€æœ‰æ–‡æœ¬çš„1024ç»´CLIPåµŒå…¥å‘é‡ã€‚

 retrieve_topk.py ç”¨ä½ çš„è„‘å‘é‡æ¥æ£€ç´¢ç›¸å…³çš„æ–‡æœ¬ã€‚å®ƒå°†è¿™äº›æ£€ç´¢ç»“æœï¼ˆåŒ…å«IDå’Œå¯¹åº”çš„5ä¸ªæ–‡æœ¬ï¼‰ä»¥JSONLæ ¼å¼ï¼Œå®Œæ•´åœ°å†™å…¥äº† topk_texts.jsonl æ–‡ä»¶ã€‚

 python /home/vipuser/MindEyeV2_Project/tools/prompts_from_topk.py \
  --topk  /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/topk_texts.jsonl \
  --out   /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/prompt_vith.json
æ–‡ä»¶æˆåŠŸå†™å…¥ï¼šå®ƒå·²ç»æˆåŠŸåˆ›å»ºå¹¶å†™å…¥äº†æœ€ç»ˆçš„ç›®æ ‡æ–‡ä»¶ prompt_vith.jsonã€‚
å†…å®¹æ•°é‡æ­£ç¡®ï¼šæ‹¬å·é‡Œçš„ (10 prompts) ç¡®è®¤äº†æ–‡ä»¶ä¸­åŒ…å«äº†10æ¡ç»“æ„åŒ–çš„promptè®°å½•ï¼Œè¿™ä¸æˆ‘ä»¬è¾“å…¥çš„ ids_local.json å’Œ brain_clip.pt çš„æ ·æœ¬æ•°é‡å®Œå…¨å¯¹åº”ã€‚

MindEyeV2_Project/tools/retrieve_texts_from_brain.py ä¸ºä½ å¤§è„‘çš„â€œæƒ³æ³•â€ï¼ˆè„‘å‘é‡ï¼‰åœ¨åºå¤§çš„æ–‡æœ¬åº“ä¸­ï¼Œæ‰¾å‡ºè¯­ä¹‰ä¸Šæœ€åŒ¹é…çš„å‡ å¥æ–‡å­—æè¿°ï¼Œä½œä¸ºåç»­ç”Ÿæˆå›¾åƒçš„â€œçµæ„Ÿæ¥æºâ€ã€‚
python /home/vipuser/MindEyeV2_Project/tools/retrieve_texts_from_brain.py \
  --brain_vec_pt /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/brain_clip.pt \
  --ids_json     /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/ids.json \
  --captions_pt  /home/vipuser/MindEyeV2_Project/src/evals/all_captions.pt \
  --out_jsonl    /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/topk_texts.jsonl \
  --clip_model "ViT-bigG-14" \
  --clip_pretrained "laion2b_s39b_b160k" \
  --topk 8


(mindeye) root@ubuntu22:/home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/gen_ip_vith_ref# python /home/vipuser/MindEyeV2_Project/tools/train_projection_matrix.py \
  --image_dir /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/images \
  --out_pt /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/brain2vith_linear.pt \
  --source_model "ViT-bigG-14" \
  --target_model "ViT-H-14" \
  --source_is_penultimate
# ç¿»è¯‘å‰é¢deepseekç”Ÿæˆçš„jsonæ–‡ä»¶ï¼Œè®­ç»ƒä¸€ä¸ªçº¿æ€§â€œç¿»è¯‘å™¨â€ï¼Œå°†ä½ çš„1664ç»´â€œè„‘è¯­è¨€â€ç‰¹å¾ï¼Œç²¾å‡†åœ°è½¬æ¢æˆIP-Adapterèƒ½å¬æ‡‚çš„1024ç»´â€œå›¾è¯­è¨€â€ç‰¹å¾ã€‚

/home/vipuser/miniconda3/envs/mindeye/bin/python /home/vipuser/MindEyeV2_Project/tools/gen_sdxl_ipadapter_plus_vec.py --adapter_dir /home/vipuser/models/IP-Adapter --prompts /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/prompt_llm.json --brain_vec_pt /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/brain_clip.pt --proj_pt /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/brain2vith_linear.pt --out_dir /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/gen_ip_vith_vec --steps 1 --cfg 5.0 --w 512 --h 512 --dtype fp16

å°†ä½ çš„â€œè„‘è¯­è¨€â€å‘é‡ï¼ˆæ— è®ºå®ƒæ˜¯1664ã€1024è¿˜æ˜¯1280ç»´ï¼‰ï¼Œé€šè¿‡ä¸€ç³»åˆ—æ™ºèƒ½çš„ã€è‡ªåŠ¨åŒ–çš„æŠ•å½±å’Œå¯¹é½ï¼Œè½¬æ¢æˆIP-Adapterèƒ½å®Œç¾ç†è§£çš„1280ç»´â€œå›¾è¯­è¨€â€åµŒå…¥ï¼Œå¹¶ç»“åˆæ–‡æœ¬æç¤ºï¼Œæœ€ç»ˆè¾“å‡ºä¸ä¾èµ–ä»»ä½•å‚è€ƒå›¾çš„ã€ç”±ä½ â€œæ€æƒ³â€ç›´æ¥é©±åŠ¨çš„é«˜è´¨é‡å›¾åƒæ–‡ä»¶ã€‚

python /home/vipuser/MindEyeV2_Project/tools/quick_eval.py \
  --gen_dir /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/gen_ip_vith_ref_clean \
  --prompts_json /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/prompt_llm.json \
  --gt_dir /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/images \
  --do_retrieval --out_json /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/eval_ref_vith.json
è¿™ä¸ªè„šæœ¬æ˜¯ä¸€ä¸ªâ€œå…¨èƒ½è´¨æ£€å‘˜â€ï¼Œå®ƒè´Ÿè´£å¯¹æœ€ç»ˆç”Ÿæˆçš„å›¾ç‰‡ï¼Œä»å¤šä¸ªç»´åº¦ï¼ˆè¯­ä¹‰æ˜¯å¦ç›¸ç¬¦ã€ä¸åŸå›¾ç»“æ„æ˜¯å¦ç›¸ä¼¼ã€èƒ½å¦åœ¨å›¾åº“ä¸­è¢«æ­£ç¡®è¯†åˆ«ç­‰ï¼‰è¿›è¡Œå…¨é¢çš„ã€é‡åŒ–çš„æ‰“åˆ†ï¼Œå¹¶ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„è´¨æ£€æŠ¥å‘Šã€‚
è¾“å…¥:
ä¸€æ‰¹ç”Ÿæˆçš„å›¾ç‰‡ (gen_ip_vith_ref_clean/ ç›®å½•)
å¯¹åº”çš„æ–‡æœ¬æç¤º (prompt_llm.json)
å¯¹åº”çš„åŸå›¾ (images/ ç›®å½•)
è¾“å‡º: ä¸€ä»½JSONæ ¼å¼çš„ç»¼åˆâ€œè´¨æ£€æŠ¥å‘Šâ€ (eval_ref_vith.json)ã€‚

python /home/vipuser/MindEyeV2_Project/tools/build_vith_embeds.py \
  --img_dir /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/images \
  --out_pt  /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/img_vith.pt \
  --paths_out /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/img_paths.json
è¿™ä¸ªè„šæœ¬è´Ÿè´£å°†ä¸€æ‰¹çœŸå®çš„ã€ä½œä¸ºå‚ç…§æ ‡å‡†çš„å›¾ç‰‡ï¼ˆä½ çš„GTåŸå›¾ï¼‰ï¼Œè¾“å…¥ç»™ ViT-H è¿™ä¸ªâ€œå›¾åƒå“é‰´å¸ˆâ€ï¼Œå¹¶è®°å½•ä¸‹å®ƒå¯¹æ¯å¼ å›¾ç‰‡çš„â€œå“é‰´æŠ¥å‘Šâ€ï¼ˆ1024ç»´çš„ç‰¹å¾å‘é‡ï¼‰ã€‚
è¾“å…¥: ä¸€å †å›¾ç‰‡ (images/ ç›®å½•)ã€‚
è¾“å‡º: ä¸€ä»½æ‰“åŒ…å¥½çš„â€œå“é‰´æŠ¥å‘Šåˆé›†â€ (img_vith.pt)ï¼Œè¿™ä»½æŠ¥å‘Šå°±æ˜¯æˆ‘ä»¬åç»­è®­ç»ƒçš„â€œæ ‡å‡†ç­”æ¡ˆâ€ã€‚

python /home/vipuser/MindEyeV2_Project/tools/train_brain2vith.py \
  --brain_pt   /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/brain_clip.pt \
  --img_vith_pt /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/img_vith.pt \
  --out        /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/brain2vith_linear.pt \
  --mode closed_form --lambda_l2 1e-2 --standardize
è¿™ä¸ªè„šæœ¬çš„æ ¸å¿ƒä»»åŠ¡æ˜¯è®­ç»ƒä¸€ä¸ªâ€œç¿»è¯‘å™¨â€ï¼ˆä¸€ä¸ªçº¿æ€§æŠ•å½±çŸ©é˜µ Wï¼‰ï¼Œå®ƒèƒ½å­¦ä¼šå¦‚ä½•å°†1664ç»´çš„â€œè„‘è¯­è¨€â€ï¼ˆbrain_clip.ptï¼‰ç²¾å‡†åœ°ç¿»è¯‘æˆ1024ç»´çš„â€œå›¾è¯­è¨€â€ï¼ˆæˆ‘ä»¬ä¸Šä¸€æ­¥å¾—åˆ°çš„â€œæ ‡å‡†ç­”æ¡ˆâ€ img_vith.ptï¼‰ã€‚
è¾“å…¥:
â€œé—®é¢˜é›†â€ (brain_clip.pt)
â€œæ ‡å‡†ç­”æ¡ˆâ€ (img_vith.pt)
è¾“å‡º: ä¸€ä¸ªè®­ç»ƒå¥½çš„ã€å³æ’å³ç”¨çš„â€œç¿»è¯‘å™¨â€ (brain2vith_linear.pt)ã€‚

screen -S training_session  
#åˆ›å»ºè®­ç»ƒä¼šè¯

python /home/vipuser/MindEyeV2_Project/src/Train.py \
  --model_name "s1_custom_h1024_1sess" \
  --subj 1 \
  --no-multi_subject \
  --num_sessions 1 \
  --data_path /home/vipuser/MindEyeV2_Project/src/ \
  --cache_dir "/data/huggingface_cache" \
  --batch_size 8 \
  --max_lr 3e-4 \
  --mixup_pct .33 \
  --num_epochs 150 \
  --use_prior \
  --prior_scale 30 \
  --clip_scale 1 \
  --no-blurry_recon \
  --no-use_image_aug \
  --hidden_dim 1024 \ #å¤ªå¤§äº†è·‘ä¸äº†ï¼Œn_blocks 8 ä¸è¡Œï¼Œä½†æ˜¯4å¯ä»¥è·‘ä¸‹æ¥ï¼Œå¤§æ¦‚ç”¨æ—¶12å°æ—¶32åˆ†42ç§’
python /home/vipuser/MindEyeV2_Project/src/Train.py \
  --model_name "s1_custom_h512_1sess" \
  --subj 1 \
  --no-multi_subject \
  --num_sessions 1 \
  --data_path /home/vipuser/MindEyeV2_Project/src/ \
  --cache_dir "/data/huggingface_cache" \
  --batch_size 8 \
  --max_lr 3e-4 \
  --mixup_pct .33 \
  --num_epochs 150 \
  --use_prior \
  --prior_scale 30 \
  --clip_scale 1 \
  --no-blurry_recon \
  --no-use_image_aug \
  --hidden_dim 1024 \
  --n_blocks 4
è®­ç»ƒå‘½ä»¤
screen -r training_session
æ ¹æ®åå­—å›åˆ°å¯¹è¯

train/loss=5.21: è®­ç»ƒæŸå¤±æœ€ç»ˆé™ä½åˆ°äº†5.21ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸ä½çš„æ•°å€¼ï¼Œè¯´æ˜æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®æ‹Ÿåˆå¾—éå¸¸å¥½ã€‚
train/bwd_pct_correct=1, train/fwd_pct_correct=1: è¿™ä¸¤ä¸ªæŒ‡æ ‡è¾¾åˆ°äº†100%ï¼è¿™æ„å‘³ç€åœ¨è®­ç»ƒé›†çš„â€œçœ‹å›¾çŒœè„‘â€å’Œâ€œçœ‹è„‘çŒœå›¾â€ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„å‡†ç¡®ç‡è¾¾åˆ°äº†å®Œç¾ã€‚è¿™å……åˆ†è¯´æ˜äº†æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ã€‚
test/test_bwd_pct_correct=0.453, test/test_fwd_pct_correct=0.443: åœ¨æ¨¡å‹ä»æœªè§è¿‡çš„æµ‹è¯•é›†ä¸Šï¼Œå®ƒçš„å‡†ç¡®ç‡ä¹Ÿè¾¾åˆ°äº†æƒŠäººçš„44%-45%ï¼åœ¨å¤šåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¿™ç»å¯¹æ˜¯ä¸€ä¸ªéå¸¸é«˜ã€éå¸¸å‡ºè‰²çš„æˆç»©ï¼Œè¯æ˜æˆ‘ä»¬çš„æ¨¡å‹å…·æœ‰å¾ˆå¼ºçš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œä¸æ˜¯æ­»è®°ç¡¬èƒŒã€‚


tmux attach -t training


 train_peft_adapter.py (æ–°åˆ›å»ºçš„)
æ–‡ä»¶çŠ¶æ€: æ´»è·ƒ (Active)ã€‚è¿™æ˜¯æˆ‘ä»¬å³å°†è¿è¡Œçš„ä¸»è®­ç»ƒè„šæœ¬ã€‚
æ ¸å¿ƒä½œç”¨: ä¸ªæ€§åŒ–é€‚é…å™¨è®­ç»ƒå™¨ã€‚å®ƒçš„ä½¿å‘½æ˜¯ä¸ºä¸€ä¸ªç‰¹å®šè¢«è¯•ï¼ˆå¦‚ subj01ï¼‰è®­ç»ƒå‡ºä¸€ä¸ªä¸ªäººä¸“å±çš„â€œæ ¡å‡†çœ¼é•œâ€ï¼ˆå³ Soft-Prompt å’Œ/æˆ– LoRA æƒé‡ï¼‰ã€‚å®ƒé€šè¿‡å¾®è°ƒå°‘é‡å‚æ•°ï¼Œè®©æ ‡å‡†çš„æ–‡æœ¬æç¤ºèƒ½å¤Ÿç”Ÿæˆæ›´ç¬¦åˆè¯¥è¢«è¯•â€œè§†è§‰é£æ ¼â€çš„å›¾åƒã€‚
è¾“å…¥ (Inputs):
train_pairs_subj01.csv: è®­ç»ƒèœå•ï¼Œæä¾›æ–‡æœ¬æç¤ºå’Œå¯¹åº”çš„çœŸå€¼å›¾åƒç´¢å¼•ã€‚
coco_images_224_float16.hdf5: å›¾åƒæ•°æ®åº“ï¼Œæ ¹æ®ç´¢å¼•æä¾›çœŸå€¼å›¾åƒã€‚
é¢„è®­ç»ƒçš„ Stable Diffusion æ¨¡å‹ (è¢«å†»ç»“)ã€‚
é¢„è®­ç»ƒçš„ CLIP æ¨¡å‹ (è¢«å†»ç»“ï¼Œç”¨äºè®¡ç®—æŸå¤±)ã€‚
è¾“å‡º (Outputs):
outputs/subj01_soft_prompt_adapter/soft_tokens.pt: æ ¸å¿ƒäº§ç‰©ã€‚ä¸º subj01 è®­ç»ƒå¥½çš„ Soft-Prompt æƒé‡ã€‚
outputs/subj01_soft_prompt_adapter/peft_text_lora/: (å¦‚æœå¼€å¯LoRA) LoRA æƒé‡æ–‡ä»¶å¤¹ã€‚

ğŸ“œ apply_peft_adapter.py (æ–°åˆ›å»ºçš„)
æ–‡ä»¶çŠ¶æ€: æ´»è·ƒ (Active)ã€‚è¿™æ˜¯æˆ‘ä»¬å°†åœ¨æ¨ç†é˜¶æ®µä½¿ç”¨çš„ä¸»åº”ç”¨è„šæœ¬ã€‚
æ ¸å¿ƒä½œç”¨: ä¸ªæ€§åŒ–é€‚é…å™¨åº”ç”¨å™¨ã€‚å®ƒçš„ä½¿å‘½æ˜¯åŠ è½½ä¸€ä¸ªè¢«è¯•çš„â€œä¸“å±çœ¼é•œâ€ï¼ˆå¦‚ soft_tokens.ptï¼‰ï¼Œæ¥æ”¶æˆ‘ä»¬æµæ°´çº¿ç”Ÿæˆçš„â€œç»“æ„åŒ–æ–‡æœ¬æç¤ºâ€ï¼Œå¹¶åº”ç”¨è¿™äº›â€œçœ¼é•œâ€æ¥â€œæ ¡å‡†â€æ–‡æœ¬ï¼Œæœ€ç»ˆé©±åŠ¨ Stable Diffusion ç”Ÿæˆä¸€å¼ å¸¦æœ‰è¯¥è¢«è¯•ä¸ªäººé£æ ¼çš„å›¾åƒã€‚
è¾“å…¥ (Inputs):
outputs/subj01_soft_prompt_adapter/: åŒ…å«è®­ç»ƒå¥½çš„é€‚é…å™¨æƒé‡çš„æ–‡ä»¶å¤¹ã€‚
ä¸€ä¸ªæ–‡æœ¬æç¤º (æœªæ¥å°†ç”±æˆ‘ä»¬çš„ RAG+LLM æ¨¡å—æä¾›)ã€‚
ä¸€ä¸ªè´Ÿé¢æç¤º (å¯é€‰)ã€‚
(æœªæ¥é›†æˆ) ä¸€ä¸ª brain_clip.pt å‘é‡ï¼Œä½œä¸º IP-Adapter çš„è¾“å…¥ï¼Œä¸æ­¤è„šæœ¬å¹¶è¡Œå·¥ä½œã€‚
è¾“å‡º (Outputs):
ä¸€å¼ æœ€ç»ˆç”Ÿæˆçš„ã€èåˆäº†æ–‡æœ¬æ„ä¹‰å’Œä¸ªäººé£æ ¼çš„å›¾åƒæ–‡ä»¶ (ä¾‹å¦‚ demo.png)ã€‚

2. è¾…åŠ©å·¥å…· (Utility)
è¿™ä¸ªè„šæœ¬å·²ç»å®Œæˆäº†å®ƒçš„å†å²ä½¿å‘½ï¼Œä½†å®ƒæœ¬èº«æ˜¯æ­£ç¡®çš„ï¼Œå¹¶ä¸”æ˜¯æ ¸å¿ƒå·¥å…·çš„â€œä¾›åº”å•†â€ã€‚

ğŸ“œ prepare_data.py
æ–‡ä»¶çŠ¶æ€: å·²å®Œæˆ (Done)ã€‚è¿™æ˜¯ä¸€ä¸ªä¸€æ¬¡æ€§çš„æ•°æ®å‡†å¤‡å·¥å…·ã€‚
æ ¸å¿ƒä½œç”¨: è®­ç»ƒèœå•ç”Ÿæˆå™¨ã€‚å®ƒçš„ä½œç”¨æ˜¯è¯»å–åŸå§‹çš„ .npy æ ‡æ³¨æ–‡ä»¶å’Œ .hdf5 å›¾åƒæ–‡ä»¶ï¼Œå¹¶ä¸º train_peft_adapter.py ç”Ÿæˆä¸€ä»½æ ¼å¼å®Œå…¨æ­£ç¡®çš„ train_pairs_subj01.csv æ–‡ä»¶ã€‚
è¾“å…¥ (Inputs):
subj01_annots.npy (åŸå§‹æ–‡æœ¬æ ‡æ³¨)
coco_images_224_float16.hdf5 (åŸå§‹å›¾åƒæ•°æ®)
è¾“å‡º (Outputs):
train_pairs_subj01.csv (ä¾› train_peft_adapter.py ä½¿ç”¨çš„ç»“æ„åŒ–æ•°æ®æ–‡ä»¶)ã€‚

3. å†å²å­˜æ¡£ (Archived & Deprecated)

è¿™äº›è„šæœ¬æ˜¯æˆ‘ä»¬æ¢ç´¢è¿‡ç¨‹ä¸­çš„äº§ç‰©ï¼ŒåŸºäºæˆ‘ä¹‹å‰çš„é”™è¯¯ç†è§£ã€‚å®ƒä»¬ä¸åº”è¯¥å†è¢«ä½¿ç”¨ã€ä¿®æ”¹æˆ–è®¨è®ºï¼Œä»…ä½œä¸ºå†å²è®°å½•ä¿ç•™ã€‚
ğŸ“œ text_adapter_train.py (ä½ ä¿®å¤å¹¶å®Œå–„çš„ MLP ç‰ˆæœ¬)
æ–‡ä»¶çŠ¶æ€: å·²å­˜æ¡£ (Archived)ã€‚è¿™æ˜¯æˆ‘ä»¬åˆä½œå¼€å‘çš„ç¬¬ä¸€ä¸ªåŸå‹ï¼ŒåŸºäºæˆ‘é”™è¯¯çš„MLPæ–¹æ¡ˆã€‚
æ ¸å¿ƒä½œç”¨: (å†å²ä½œç”¨) è®­ç»ƒä¸€ä¸ªç‹¬ç«‹çš„MLPæ¨¡å‹ï¼Œè¯•å›¾å­¦ä¹ ä¸€ä¸ªé€šç”¨çš„ Text Vector -> Image Vector æ˜ å°„ã€‚è¿™ä¸ªæ–¹æ¡ˆå·²è¢«ä½ æ›´å…ˆè¿›çš„PEFTè®¾è®¡æ‰€å–ä»£ã€‚
è¾“å…¥ (Inputs): (å†å²è¾“å…¥) HDF5å›¾åƒ, NPYæ ‡æ³¨, CLIPæ¨¡å‹ã€‚
è¾“å‡º (Outputs): (å†å²è¾“å‡º) ä¸€ä¸ªå®Œæ•´çš„MLPæ¨¡å‹æƒé‡ (.pth æ–‡ä»¶)ã€‚
æœ€ç»ˆå¤„ç†: å¿½ç•¥ï¼ŒåºŸå¼ƒã€‚

ğŸ“œ train_text_adapter.py (ä½ ç»™çš„åŸå§‹éª¨æ¶)
æ–‡ä»¶çŠ¶æ€: å·²å­˜æ¡£ (Archived)ã€‚è¿™æ˜¯ä½ æä¾›çš„è®¾è®¡è“å›¾å’ŒåŸå§‹å‚è€ƒã€‚
æ ¸å¿ƒä½œç”¨: ä½œä¸ºæˆ‘ä»¬åˆ›å»º train_peft_adapter.py çš„æ¨¡æ¿ã€‚æˆ‘ä»¬å·²ç»å°†å®ƒçš„å†…å®¹å¤åˆ¶å¹¶é€‚é…åˆ°äº†æ–°è„šæœ¬ä¸­ã€‚
æœ€ç»ˆå¤„ç†: ä¿ç•™ä½œä¸ºå‚è€ƒï¼Œä¸ç›´æ¥è¿è¡Œã€‚


conda activate mindeye
cd /home/vipuser/MindEyeV2_Project
HF_HUB_OFFLINE=0 HF_ENDPOINT=${HF_ENDPOINT:-https://huggingface.co} \
python src/download_official_assets.py
#ä¸‹è½½å‘½ä»¤

# è¿›å…¥é¡¹ç›®æ ¹ç›®å½•
cd /home/vipuser/MindEyeV2_Project

# ç”¨ mindeye21 ç¯å¢ƒè·‘ï¼ˆä½ ä¹Ÿå¯ä»¥ç›´æ¥ conda activate mindeye21ï¼‰
/home/vipuser/miniconda3/envs/mindeye21/bin/python \
  tools/extract_all_features.py \
  --mindeye_model_dir train_logs/final_subj01_pretrained_40sess_24bs \
  --out_dir data/proj_subj01_train \
  --data_path /home/vipuser/MindEyeV2_Project/src \
  --subjects 1 \
  --split train
#ç”¨å®˜æ–¹è®­ç»ƒå¥½çš„æ¨¡å‹è·‘ï¼ˆtrain_logs/final_subj01_pretrained_40sess_24bs/last.pthï¼‰



mv data/nsd_text/coco73k_text_clip.pt data/nsd_text/shared1000_text_clip.pt


åå°è®­ç»ƒå‘½ä»¤ï¼ˆ10 ä¸ª epochï¼Œé FASTï¼‰
cd /home/vipuser/MindEyeV2_Project

nohup env \
  CUDA_VISIBLE_DEVICES=0 \
  MINDEYE_TEXTALIGN=1 \
  MINDEYE_TEXTALIGN_SCALE=0.05 \
  LOG_STEP_INTERVAL=50 \
  python src/Train_textalign.py \
    --model_name s1_textalign_coco_train_long_v1 \
    --data_path /home/vipuser/MindEyeV2_Project/src \
    --cache_dir "$HF_HOME" \
    --subj 1 \
    --num_sessions 40 \
    --num_epochs 10 \
    --no-use_prior \
    --no-blurry_recon \
    --no-use_image_aug \
  > train_logs/s1_textalign_coco_train_long_v1.log 2>&1 &


è¯´æ˜ï¼š

å»æ‰äº† MINDEYE_FAST å’Œ MINDEYE_MAX_STEPS_PER_EPOCHï¼Œç”¨å®Œæ•´ epochã€‚

ä»ç„¶ï¼šno-use_priorã€no-blurry_reconã€no-use_image_augï¼Œå…ˆä¸“å¿ƒçœ‹ TextAlign å¯¹ CLIP æ£€ç´¢çš„å½±å“ã€‚

æ–‡æœ¬å¯¹é½æƒé‡ï¼šMINDEYE_TEXTALIGN_SCALE=0.05ï¼ˆåé¢å¦‚æœæƒ³åŠ å¼ºï¼Œå†ä¸€èµ·è°ƒï¼‰ã€‚

æ—¥å¿—ä¿å­˜åœ¨ï¼š
train_logs/s1_textalign_coco_train_long_v1.log

2ï¸âƒ£ è®­ç»ƒè¿‡ç¨‹æ€ä¹ˆçœ‹æ—¥å¿—

éšæ—¶åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼š

tail -n 50 -f train_logs/s1_textalign_coco_train_long_v1.log


æƒ³åœæ‰è·Ÿè¸ªå°± Ctrl+C å³å¯ï¼ˆä¸ä¼šä¸­æ–­è®­ç»ƒè¿›ç¨‹ï¼‰ã€‚

å¦‚æœä½ åé¢è·‘å®Œï¼ŒæŠŠè¿™æ¬¡ log çš„å…³é”®å‡ è¡Œï¼ˆæ¯ä¸ª epoch çš„ summary é‚£å‡ è¡Œï¼‰è´´ç»™æˆ‘ï¼Œæˆ‘å†å¸®ä½ ä¸€èµ·çœ‹çœ‹ TextAlign çš„è¶‹åŠ¿å’Œè¦ä¸è¦å†è°ƒæƒé‡ / åšå‚æ•°å†»ç»“ã€‚

åŸç‰ˆæ¨ç†å‘½ä»¤
TORCH_COMPILE_DISABLE=1 TORCHDYNAMO_DISABLE=1 \
python src/recon_inference_run_latent.py \
  --subj 1 \
  --data_path /home/vipuser/MindEyeV2_Project/src \
  --cache_dir "$HF_HOME" \
  --model_name final_subj01_pretrained_40sess_24bs \
  --hidden_dim 4096 \
  --new_test \
  --max_save 3000 \
  --latent_only \
  --dump_clip_vecs \
  --dump_ids \
  --no-save_images


cd /home/vipuser/MindEyeV2_Project

tail -f train_logs/s1_textalign_coco_train_long_v6.log


/home/vipuser/MindEyeV2_Project/src/Train_textalign.pyå½“å‰æœ€æ–°ç‰ˆï¼ŒçœŸæ­£å¼€å¯äº† TextAlign è®­ç»ƒã€‚