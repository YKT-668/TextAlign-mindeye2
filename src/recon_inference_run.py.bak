#!/usr/bin/env python
# coding: utf-8

# In[1]:


import os
import sys
import json
import argparse
import numpy as np
import math
from einops import rearrange
import time
import random
import string
import h5py
from tqdm import tqdm
import webdataset as wds

import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torchvision import transforms
from torchvision.transforms import ToPILImage
from accelerate import Accelerator

# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main
# Ensure we prefer the local `src/generative_models` implementation (it contains newer signatures)
script_dir = os.path.dirname(os.path.abspath(__file__))
# Insert at front so it takes precedence over other installed/adjacent copies
local_generative_models = os.path.join(script_dir, 'generative_models')
if os.path.isdir(local_generative_models):
    if local_generative_models not in sys.path:
        sys.path.insert(0, local_generative_models)
else:
    # fallback to a generative-models folder at repo root if present
    repo_root_gen = os.path.join(os.path.dirname(script_dir), 'generative-models')
    if os.path.isdir(repo_root_gen) and repo_root_gen not in sys.path:
        sys.path.insert(0, repo_root_gen)
import sgm
from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenOpenCLIPEmbedder2
from generative_models.sgm.models.diffusion import DiffusionEngine
from generative_models.sgm.util import append_dims
from omegaconf import OmegaConf

import safetensors.torch

# tf32 data type is faster than standard float32
torch.backends.cuda.matmul.allow_tf32 = True

# custom functions #
import utils
from models import *

accelerator = Accelerator(split_batches=False, mixed_precision="fp16")
device = accelerator.device
print("device:",device)


# In[33]:


# if running this interactively, can specify jupyter_args here for argparser to use
if utils.is_interactive():
    model_name = "final_subj01_pretrained_40sess_24bs"
    print("model_name:", model_name)

    # other variables can be specified in the following string:
    jupyter_args = f"--data_path=/weka/proj-medarc/shared/mindeyev2_dataset \
                    --cache_dir=/weka/proj-medarc/shared/mindeyev2_dataset \
                    --model_name={model_name} --subj=1 \
                    --hidden_dim=4096 --n_blocks=4 --new_test"
    print(jupyter_args)
    jupyter_args = jupyter_args.split()
    
    from IPython.display import clear_output # function to clear print outputs in cell
    get_ipython().run_line_magic('load_ext', 'autoreload')
    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions
    get_ipython().run_line_magic('autoreload', '2')


# In[34]:


parser = argparse.ArgumentParser(description="Model Training Configuration")
parser.add_argument(
    "--model_name", type=str, default="testing",
    help="will load ckpt for model found in ../train_logs/model_name",
)
parser.add_argument(
    "--data_path", type=str, default=os.getcwd(),
    help="Path to where NSD data is stored / where to download it to",
)
parser.add_argument(
    "--cache_dir", type=str, default=os.getcwd(),
    help="Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.",
)
parser.add_argument(
    "--subj",type=int, default=1, choices=[1,2,3,4,5,6,7,8],
    help="Validate on which subject?",
)
parser.add_argument(
    "--blurry_recon",action=argparse.BooleanOptionalAction,default=True,
)
parser.add_argument(
    "--n_blocks",type=int,default=4,
)
parser.add_argument(
    "--hidden_dim",type=int,default=512,
)
parser.add_argument(
    "--new_test",action=argparse.BooleanOptionalAction,default=True,
)
parser.add_argument(
    "--seed",type=int,default=42,
)
parser.add_argument(
    "--output_dir", type=str, default=os.path.dirname(script_dir),
    help="Directory where outputs (recons, captions, etc.) will be saved. Defaults to repo root.",
)
parser.add_argument(
    "--save_images", action=argparse.BooleanOptionalAction, default=False,
    help="Also save individual reconstructions as image files (PNG/JPEG) into output_dir/images/",
)
parser.add_argument(
    "--image_format", type=str, default="png",
    choices=["png", "jpg", "jpeg"],
    help="Image file format to save (png or jpg).",
)
parser.add_argument(
    "--max_save", type=int, default=10,
    help="Maximum number of images to save when --save_images is set (default 10).",
)
if utils.is_interactive():
    args = parser.parse_args(jupyter_args)
else:
    args = parser.parse_args()

# create global variables without the args prefix
for attribute_name in vars(args).keys():
    globals()[attribute_name] = getattr(args, attribute_name)
    
# seed all random functions
utils.seed_everything(seed)

# make output directory
os.makedirs("evals",exist_ok=True)
# create model-specific subdir under evals and also ensure output_dir exists
os.makedirs(f"evals/{model_name}",exist_ok=True)
os.makedirs(output_dir, exist_ok=True)


# In[35]:


voxels = {}
# Load hdf5 data for betas
f = h5py.File(f'{data_path}/betas_all_subj0{subj}_fp32_renorm.hdf5', 'r')
betas = f['betas'][:]
betas = torch.Tensor(betas).to("cpu")
num_voxels = betas[0].shape[-1]
voxels[f'subj0{subj}'] = betas
print(f"num_voxels for subj0{subj}: {num_voxels}")

if not new_test: # using old test set from before full dataset released (used in original MindEye paper)
    if subj==3:
        num_test=2113
    elif subj==4:
        num_test=1985
    elif subj==6:
        num_test=2113
    elif subj==8:
        num_test=1985
    else:
        num_test=2770
    test_url = f"{data_path}/wds/subj0{subj}/test/" + "0.tar"
else: # using larger test set from after full dataset released
    if subj==3:
        num_test=2371
    elif subj==4:
        num_test=2188
    elif subj==6:
        num_test=2371
    elif subj==8:
        num_test=2188
    else:
        num_test=3000
    test_url = f"{data_path}/wds/subj0{subj}/new_test/" + "0.tar"
    
print(test_url)
def my_split_by_node(urls): return urls
test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\
                    .decode("torch")\
                    .rename(behav="behav.npy", past_behav="past_behav.npy", future_behav="future_behav.npy", olds_behav="olds_behav.npy")\
                    .to_tuple(*["behav", "past_behav", "future_behav", "olds_behav"])
test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)
print(f"Loaded test dl for subj{subj}!\n")


# In[36]:


# Prep images but don't load them all to memory
f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')
images = f['images']

# Prep test voxels and indices of test images
test_images_idx = []
test_voxels_idx = []
for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):
    test_voxels = voxels[f'subj0{subj}'][behav[:,0,5].cpu().long()]
    test_voxels_idx = np.append(test_images_idx, behav[:,0,5].cpu().numpy())
    test_images_idx = np.append(test_images_idx, behav[:,0,0].cpu().numpy())
test_images_idx = test_images_idx.astype(int)
test_voxels_idx = test_voxels_idx.astype(int)

assert (test_i+1) * num_test == len(test_voxels) == len(test_images_idx)
print(test_i, len(test_voxels), len(test_images_idx), len(np.unique(test_images_idx)))


# In[38]:


clip_img_embedder = FrozenOpenCLIPImageEmbedder(
    arch="ViT-bigG-14",
    version="laion2b_s39b_b160k",
    output_tokens=True,
    only_tokens=True,
)
clip_img_embedder.to(device)
clip_seq_dim = 256
clip_emb_dim = 1664

if blurry_recon:
    from diffusers import AutoencoderKL
    autoenc = AutoencoderKL(
        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],
        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],
        block_out_channels=[128, 256, 512, 512],
        layers_per_block=2,
        sample_size=256,
    )
    ckpt = safetensors.torch.load_file(f'{cache_dir}/vae-ft-mse-840000-ema-pruned.safetensors')
    autoenc.load_state_dict(ckpt, strict=False)
    autoenc.eval()
    autoenc.requires_grad_(False)
    autoenc.to(device)
    utils.count_params(autoenc)
    
class MindEyeModule(nn.Module):
    def __init__(self):
        super(MindEyeModule, self).__init__()
    def forward(self, x):
        return x
        
model = MindEyeModule()

class RidgeRegression(torch.nn.Module):
    # make sure to add weight_decay when initializing optimizer to enable regularization
    def __init__(self, input_sizes, out_features): 
        super(RidgeRegression, self).__init__()
        self.out_features = out_features
        self.linears = torch.nn.ModuleList([
                torch.nn.Linear(input_size, out_features) for input_size in input_sizes
            ])
    def forward(self, x, subj_idx):
        out = self.linears[subj_idx](x[:,0]).unsqueeze(1)
        return out
        
model.ridge = RidgeRegression([num_voxels], out_features=hidden_dim)

from diffusers.models.autoencoders.vae import Decoder
from models import BrainNetwork
model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, 
                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim) 
utils.count_params(model.ridge)
utils.count_params(model.backbone)
utils.count_params(model)

# setup diffusion prior network
out_dim = clip_emb_dim
depth = 6
dim_head = 52
heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim
timesteps = 100

prior_network = PriorNetwork(
        dim=out_dim,
        depth=depth,
        dim_head=dim_head,
        heads=heads,
        causal=False,
        num_tokens = clip_seq_dim,
        learned_query_mode="pos_emb"
    )

model.diffusion_prior = BrainDiffusionPrior(
    net=prior_network,
    image_embed_dim=out_dim,
    condition_on_text_encodings=False,
    timesteps=timesteps,
    cond_drop_prob=0.2,
    image_embed_scale=None,
)
model.to(device)

utils.count_params(model.diffusion_prior)
utils.count_params(model)

# Load pretrained model ckpt
tag='last'
outdir = os.path.abspath(f'../train_logs/{model_name}')
print(f"\n---loading {outdir}/{tag}.pth ckpt---\n")
pth_path = f'{outdir}/{tag}.pth'
if os.path.exists(pth_path):
    try:
        checkpoint = torch.load(pth_path, map_location='cpu')
        state_dict = checkpoint['model_state_dict']  # Êàñ checkpoint['state_dict']ÔºåÊ†πÊçÆ‰Ω†ÁöÑ ckpt ÁªìÊûÑ
        model.load_state_dict(state_dict, strict=False)  # Âä† strict=False Èò≤ÈîÆÈîô
        del checkpoint
        print(f"Loaded from single pth: {pth_path}")
    except Exception as e:
        print(f"Single pth load failed: {e}")
        raise
else:
    # DeepSpeed fallback
    import deepspeed
import os, torch
ckpt_pth = os.path.join(outdir, str(tag));
ckpt_dir = os.path.join(outdir, str(tag).replace(".pth",""));
if os.path.isfile(ckpt_pth) and not os.path.isdir(ckpt_dir):
    state_dict = torch.load(ckpt_pth, map_location="cpu");
else:
    state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)
    model.load_state_dict(state_dict, strict=False)
    del state_dict
    print("Loaded from DeepSpeed zero checkpoint")
print("ckpt loaded!")


# In[30]:


# setup text caption networks
from transformers import AutoProcessor, AutoModelForCausalLM
from modeling_git import GitForCausalLMClipEmb
processor = AutoProcessor.from_pretrained("microsoft/git-large-coco")
clip_text_model = GitForCausalLMClipEmb.from_pretrained("microsoft/git-large-coco")
clip_text_model.to(device) # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4
clip_text_model.eval().requires_grad_(False)
clip_text_seq_dim = 257
clip_text_emb_dim = 1024

class CLIPConverter(torch.nn.Module):
    def __init__(self):
        super(CLIPConverter, self).__init__()
        self.linear1 = nn.Linear(clip_seq_dim, clip_text_seq_dim)
        self.linear2 = nn.Linear(clip_emb_dim, clip_text_emb_dim)
    def forward(self, x):
        x = x.permute(0,2,1)
        x = self.linear1(x)
        x = self.linear2(x.permute(0,2,1))
        return x
        
clip_convert = CLIPConverter()
state_dict = torch.load(f"{cache_dir}/bigG_to_L_epoch8.pth", map_location='cpu')['model_state_dict']
clip_convert.load_state_dict(state_dict, strict=True)
clip_convert.to(device) # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4
del state_dict


# In[31]:


# prep unCLIP
def _resolve_config_path(*candidates):
    # Try several likely locations: cwd-relative, src/ prefix, script dir, and hyphen variant
    script_dir = os.path.dirname(os.path.abspath(__file__))
    cwd = os.getcwd()
    checked = []

    # helper to check a path and return absolute if exists
    def _check(p):
        if os.path.isabs(p):
            cand = p
        else:
            cand = os.path.join(cwd, p)
        checked.append(cand)
        if os.path.exists(cand):
            return cand
        # try relative to script dir
        cand2 = os.path.join(script_dir, p)
        checked.append(cand2)
        if os.path.exists(cand2):
            return cand2
        return None

    for c in candidates:
        found = _check(c)
        if found:
            print(f"Using config: {found}")
            return found

    # try src/ prefix
    for c in candidates:
        found = _check(os.path.join('src', c))
        if found:
            print(f"Using config: {found}")
            return found

    # try replacing underscore folder with hyphen variant
    for c in candidates:
        if 'generative_models' in c:
            c2 = c.replace('generative_models', 'generative-models')
            found = _check(c2)
            if found:
                print(f"Using config: {found}")
                return found

    raise FileNotFoundError(
        f"Could not find config file. Checked the following paths:\n" + "\n".join(checked)
    )

config_path = _resolve_config_path("generative_models/configs/unclip6.yaml")
config = OmegaConf.load(config_path)
config = OmegaConf.to_container(config, resolve=True)
unclip_params = config["model"]["params"]
network_config = unclip_params["network_config"]
denoiser_config = unclip_params["denoiser_config"]
first_stage_config = unclip_params["first_stage_config"]
conditioner_config = unclip_params["conditioner_config"]
sampler_config = unclip_params["sampler_config"]
scale_factor = unclip_params["scale_factor"]
disable_first_stage_autocast = unclip_params["disable_first_stage_autocast"]
offset_noise_level = unclip_params["loss_fn_config"]["params"]["offset_noise_level"]

first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'
sampler_config['params']['num_steps'] = 38

diffusion_engine = DiffusionEngine(network_config=network_config,
                       denoiser_config=denoiser_config,
                       first_stage_config=first_stage_config,
                       conditioner_config=conditioner_config,
                       sampler_config=sampler_config,
                       scale_factor=scale_factor,
                       disable_first_stage_autocast=disable_first_stage_autocast)
# set to inference
diffusion_engine.eval().requires_grad_(False)
diffusion_engine.to(device)

ckpt_path = f'{cache_dir}/unclip6_epoch0_step110000.ckpt'
if not os.path.exists(ckpt_path):
    # helpful error: list files in cache_dir and suggest fixes
    cache_dir_to_list = os.path.dirname(ckpt_path) or '.'
    try:
        files = os.listdir(cache_dir_to_list)
    except Exception:
        files = []
    raise FileNotFoundError(
        f"Checkpoint not found: {ckpt_path}\n"
        f"Searched cache dir: {cache_dir_to_list}\n"
        f"Files found there: {files}\n\n"
        "Fixes:\n"
        "  - Place `unclip6_epoch0_step110000.ckpt` into the cache dir and re-run, or\n"
        "  - Pass --cache_dir /path/to/dir-containing-checkpoint when running the script, or\n"
        "  - Download the checkpoint from its source (see project README) into the cache dir.\n"
    )
ckpt = torch.load(ckpt_path, map_location='cpu')
diffusion_engine.load_state_dict(ckpt['state_dict'])

batch={"jpg": torch.randn(1,3,1,1).to(device), # jpg doesnt get used, it's just a placeholder
      "original_size_as_tuple": torch.ones(1, 2).to(device) * 768,
      "crop_coords_top_left": torch.zeros(1, 2).to(device)}
out = diffusion_engine.conditioner(batch)
vector_suffix = out["vector"].to(device)
print("vector_suffix", vector_suffix.shape)


# In[39]:


# get all reconstructions
model.to(device)
model.eval().requires_grad_(False)

# ==================== MODIFICATION 1: SETUP FOR STREAMING ====================
# This section replaces the old memory-intensive lists (all_recons, etc.)
# with file handlers and counters to save results one by one.

# 1. Initialize counters and tools
piler = ToPILImage()
saved_count = 0
imsize = 256 # Define image size for resizing early

# 2. Create output directories beforehand
image_dir = os.path.join(output_dir, "images")
os.makedirs(image_dir, exist_ok=True)
if blurry_recon:
    blurry_dir = os.path.join(output_dir, "blurry_images")
    os.makedirs(blurry_dir, exist_ok=True)

# 3. Open the captions file for writing
caps_path = os.path.join(output_dir, f"{model_name}_captions.txt")
caps_file = open(caps_path, "w", encoding="utf-8")

# 4. Define other variables
minibatch_size = 1
num_samples_per_image = 1
assert num_samples_per_image == 1

# plotting flag: default False, enable when running interactively
plotting = False
if utils.is_interactive():
    plotting = True
# ==========================================================================

with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):
    for batch in tqdm(range(0,len(np.unique(test_images_idx)),minibatch_size)):
        uniq_imgs = np.unique(test_images_idx)[batch:batch+minibatch_size]
        voxel = None
        for uniq_img in uniq_imgs:
            locs = np.where(test_images_idx==uniq_img)[0]
            if len(locs)==1:
                locs = locs.repeat(3)
            elif len(locs)==2:
                locs = locs.repeat(2)[:3]
            assert len(locs)==3
            if voxel is None:
                voxel = test_voxels[None,locs] # 1, num_image_repetitions, num_voxels
            else:
                voxel = torch.vstack((voxel, test_voxels[None,locs]))
        voxel = voxel.to(device)
        
        for rep in range(3):
            voxel_ridge = model.ridge(voxel[:,[rep]],0) # 0th index of subj_list
            backbone0, clip_voxels0, blurry_image_enc0 = model.backbone(voxel_ridge)
            if rep==0:
                clip_voxels = clip_voxels0
                backbone = backbone0
                blurry_image_enc = blurry_image_enc0[0]
            else:
                clip_voxels += clip_voxels0
                backbone += backbone0
                blurry_image_enc += blurry_image_enc0[0]
        clip_voxels /= 3
        backbone /= 3
        blurry_image_enc /= 3
                
        # Save retrieval submodule outputs
        #if all_clipvoxels is None:
            #all_clipvoxels = clip_voxels.cpu()
        #else:
            #all_clipvoxels = torch.vstack((all_clipvoxels, clip_voxels.cpu()))
        
                # ==================== MODIFICATION 2: STREAMING SAVE LOGIC ====================
        # This block replaces the old logic that appended results to lists in memory.
        # Instead, it saves each result to disk as it's generated.

        # Check if we still need to save more images
        if saved_count < (max_save if max_save is not None else float('inf')):
            
            # --- Caption Generation ---
            prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, 
                            text_cond = dict(text_embed = backbone), 
                            cond_scale = 1., timesteps = 20)
            pred_caption_emb = clip_convert(prior_out)
            generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)
            generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)
            print(f"Sample {saved_count}: {generated_caption}")

            # --- Image Reconstruction and Saving ---
            for i in range(len(voxel)): # This loop will only run once since minibatch_size=1
                # Check again in case max_save is reached within a batch
                if saved_count >= (max_save if max_save is not None else float('inf')):
                    break

                # 1. Save the main reconstruction
                samples = utils.unclip_recon(prior_out[[i]],
                                 diffusion_engine,
                                 vector_suffix,
                                 num_samples=num_samples_per_image)
                
                im = transforms.Resize((imsize, imsize))(samples[0]).float().cpu()
                pil_img = piler(im)
                fname = os.path.join(image_dir, f"{model_name}_recon_{saved_count}.{image_format}")
                pil_img.save(fname)

                # 2. Save the corresponding caption
                caps_file.write(f"{saved_count}\t{generated_caption[i]}\n")

                # 3. Save the blurry reconstruction (if enabled)
                if blurry_recon:
                    blurred_image = (autoenc.decode(blurry_image_enc[i:i+1]/0.18215).sample/ 2 + 0.5).clamp(0,1)
                    im_blurry = transforms.Resize((imsize, imsize))(blurred_image[0]).float().cpu()
                    pil_blurry = piler(im_blurry)
                    fname_blurry = os.path.join(blurry_dir, f"{model_name}_blurry_{saved_count}.{image_format}")
                    pil_blurry.save(fname_blurry)

                saved_count += 1

        # --- Early Exit ---
        # If we have saved the desired number of images, break the main loop to save time.
        if max_save is not None and saved_count >= max_save:
            print(f"\nReached max_save limit of {max_save}. Stopping inference.")
            break
        # ==========================================================================


# resize outputs before saving
# ==================== MODIFICATION 3: FINAL CLEANUP AND SUMMARY ====================
# All saving logic is now complete. Close the file and print a final summary.

# 1. Close the captions file
caps_file.close()

# 2. Print a clear summary report
print("\n" + "="*60)
print("‚úÖ All tasks completed successfully!")
print("="*60)

if saved_count > 0:
    print(f"üñºÔ∏è  {saved_count} reconstruction images have been saved to:")
    print(f"    {os.path.abspath(image_dir)}")

    if blurry_recon and os.path.exists(blurry_dir):
        print(f"\nüå´Ô∏è  {saved_count} blurry reconstruction images have been saved to:")
        print(f"    {os.path.abspath(blurry_dir)}")

    print(f"\nüìù  Corresponding captions have been saved to:")
    print(f"    {os.path.abspath(caps_path)}")
else:
    print("ü§î No images were saved. Please check your --max_save parameter and script logic.")

print("="*60 + "\n")
# ===============================================================================

if not utils.is_interactive():
    sys.exit(0)

