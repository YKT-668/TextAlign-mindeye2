import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import open_clip
import h5py
import numpy as np
import os
from tqdm import tqdm
from datetime import datetime

# ==================================
# 1. å®šä¹‰æˆ‘ä»¬çš„æ ¸å¿ƒç»„ä»¶ï¼šMLPé€‚é…å™¨
# ==================================

class TextAdapter(nn.Module):
    def __init__(self, input_dim=1280, hidden_dim=2048, output_dim=1280):
        super(TextAdapter, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, text_embedding):
        return self.model(text_embedding)

# ==================================
# 2. å®šä¹‰æ•°æ®åŠ è½½å™¨
# ==================================

class COCOCaptionsDataset(Dataset):
    """
    ä¸€ä¸ªè‡ªå®šä¹‰çš„PyTorchæ•°æ®é›†ï¼Œç”¨äºåŠ è½½COCOå›¾åƒåŠå…¶å¯¹åº”çš„æ–‡æœ¬æè¿°ã€‚
    """
    def __init__(self, images_path, annots_path, tokenizer):
        """
        åˆå§‹åŒ–æ•°æ®é›†ã€‚
        
        å‚æ•°:
        - images_path: æŒ‡å‘ 'coco_images_224_float16.hdf5' æ–‡ä»¶çš„è·¯å¾„ã€‚
        - annots_path: æŒ‡å‘ 'subj01_annots.npy' æ–‡ä»¶çš„è·¯å¾„ã€‚
        - tokenizer: open_clip çš„åˆ†è¯å™¨ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºtokenã€‚
        """
        self.images_path = images_path
        self.tokenizer = tokenizer
        
        # åŠ è½½æ ‡æ³¨æ–‡ä»¶
        self.captions = np.load(annots_path, allow_pickle=True)
        
        print("\n--- æ•°æ®åŠ è½½æŠ¥å‘Š ---")
        print(f"æ ‡æ³¨æ•°ç»„å½¢çŠ¶: {self.captions.shape}")
        print(f"æ ‡æ³¨æ•°ç»„ç±»å‹: {self.captions.dtype}")
        print(f"æ€»å…±æœ‰ {len(self.captions)} æ¡æ–‡æœ¬æè¿°")
        print(f"ç¤ºä¾‹æ–‡æœ¬: '{self.captions[0]}'")
        
        # æ£€æŸ¥HDF5æ–‡ä»¶ä¸­çš„å›¾åƒæ•°é‡
        with h5py.File(self.images_path, 'r') as hf:
            num_images = len(hf['images'])
            print(f"HDF5æ–‡ä»¶ä¸­æœ‰ {num_images} å¼ å›¾åƒ")
        
        # ç¡®ä¿æ–‡æœ¬æ•°é‡å’Œå›¾åƒæ•°é‡åŒ¹é…
        if len(self.captions) != num_images:
            print(f"âš ï¸ è­¦å‘Š: æ–‡æœ¬æ•°é‡({len(self.captions)})å’Œå›¾åƒæ•°é‡({num_images})ä¸åŒ¹é…ï¼")
            self.dataset_size = min(len(self.captions), num_images)
            print(f"å°†ä½¿ç”¨å‰ {self.dataset_size} ä¸ªæ ·æœ¬")
        else:
            self.dataset_size = len(self.captions)
            print(f"âœ… æ–‡æœ¬å’Œå›¾åƒæ•°é‡åŒ¹é…ï¼")
        
        print("--- æŠ¥å‘Šç»“æŸ ---\n")

    def __len__(self):
        return self.dataset_size

    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ•°æ®ç‚¹"""
        caption = self.captions[idx]
        text_tokens = self.tokenizer(caption)
        
        with h5py.File(self.images_path, 'r') as hf:
            image_data = hf['images'][idx]
            image_tensor = torch.from_numpy(image_data.astype(np.float32))

        return {
            "image": image_tensor,
            "text_tokens": text_tokens.squeeze(),
            "caption": caption
        }

# ==================================
# 3. è®­ç»ƒå‡½æ•°
# ==================================

def train_one_epoch(text_adapter, clip_model, data_loader, optimizer, device, epoch):
    """
    è®­ç»ƒä¸€ä¸ªepoch
    
    å‚æ•°:
    - text_adapter: æˆ‘ä»¬è¦è®­ç»ƒçš„æ–‡æœ¬é€‚é…å™¨
    - clip_model: é¢„è®­ç»ƒçš„CLIPæ¨¡å‹ï¼ˆå†»ç»“ï¼‰
    - data_loader: æ•°æ®åŠ è½½å™¨
    - optimizer: ä¼˜åŒ–å™¨
    - device: è®¾å¤‡ï¼ˆcudaæˆ–cpuï¼‰
    - epoch: å½“å‰epochç¼–å·
    
    è¿”å›:
    - å¹³å‡æŸå¤±
    """
    text_adapter.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    clip_model.eval()     # CLIPæ¨¡å‹ä¿æŒè¯„ä¼°æ¨¡å¼ï¼ˆæˆ‘ä»¬ä¸è®­ç»ƒå®ƒï¼‰
    
    total_loss = 0.0
    num_batches = 0
    
    # ä½¿ç”¨tqdmåˆ›å»ºè¿›åº¦æ¡
    pbar = tqdm(data_loader, desc=f"Epoch {epoch+1}")
    
    for batch in pbar:
        # 1. è·å–æ•°æ®å¹¶ç§»åˆ°è®¾å¤‡ä¸Š
        images = batch["image"].to(device)
        text_tokens = batch["text_tokens"].to(device)
        
        # 2. ä½¿ç”¨CLIPæå–"é»„é‡‘æ ‡å‡†"çš„ç‰¹å¾å‘é‡
        with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœå†…å­˜
            # æå–å›¾åƒç‰¹å¾ï¼ˆæˆ‘ä»¬çš„ç›®æ ‡ï¼‰
            image_features = clip_model.encode_image(images)
            # å½’ä¸€åŒ–ï¼ˆCLIPçš„æ ‡å‡†åšæ³•ï¼‰
            image_features = F.normalize(image_features, dim=-1)
            
            # æå–åŸå§‹æ–‡æœ¬ç‰¹å¾
            text_features = clip_model.encode_text(text_tokens)
            # å½’ä¸€åŒ–
            text_features = F.normalize(text_features, dim=-1)
        
        # 3. é€šè¿‡æˆ‘ä»¬çš„é€‚é…å™¨è½¬æ¢æ–‡æœ¬ç‰¹å¾
        adapted_text_features = text_adapter(text_features)
        # å½’ä¸€åŒ–é€‚é…åçš„ç‰¹å¾
        adapted_text_features = F.normalize(adapted_text_features, dim=-1)
        
        # 4. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±
        # ä½™å¼¦ç›¸ä¼¼åº¦èŒƒå›´[-1, 1]ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒæ¥è¿‘1
        # æŸå¤± = 1 - ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œä½¿å¾—ç›¸ä¼¼åº¦è¶Šé«˜ï¼ŒæŸå¤±è¶Šä½
        cosine_sim = F.cosine_similarity(adapted_text_features, image_features, dim=-1)
        loss = (1 - cosine_sim).mean()
        
        # 5. åå‘ä¼ æ’­å’Œä¼˜åŒ–
        optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
        loss.backward()        # è®¡ç®—æ¢¯åº¦
        optimizer.step()       # æ›´æ–°æƒé‡
        
        # 6. è®°å½•ç»Ÿè®¡ä¿¡æ¯
        total_loss += loss.item()
        num_batches += 1
        
        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
        pbar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'avg_loss': f'{total_loss/num_batches:.4f}',
            'cos_sim': f'{cosine_sim.mean().item():.4f}'
        })
    
    avg_loss = total_loss / num_batches
    return avg_loss

# ==================================
# 4. ä¸»è®­ç»ƒæµç¨‹
# ==================================

def main():
    # ==================== é…ç½®å‚æ•° ====================
    # æ•°æ®è·¯å¾„
    IMAGES_HDF5_PATH = 'coco_images_224_float16.hdf5'
    ANNOTS_NPY_PATH = 'subj01_annots.npy'
    
    # è®­ç»ƒè¶…å‚æ•°
    BATCH_SIZE = 32          # æ‰¹æ¬¡å¤§å°ï¼Œæ ¹æ®GPUå†…å­˜è°ƒæ•´
    NUM_EPOCHS = 10          # è®­ç»ƒè½®æ•°
    LEARNING_RATE = 1e-4     # å­¦ä¹ ç‡
    
    # æ¨¡å‹å‚æ•°
    CLIP_DIM = 1280          # ViT-bigG-14çš„ç‰¹å¾ç»´åº¦ï¼ˆå®é™…æ˜¯1280ï¼‰
    HIDDEN_DIM = 2048        # é€‚é…å™¨éšè—å±‚ç»´åº¦
    
    # ä¿å­˜è·¯å¾„
    CHECKPOINT_DIR = 'checkpoints'
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\n{'='*60}")
    print(f"ğŸš€ è®­ç»ƒé…ç½®")
    print(f"{'='*60}")
    print(f"è®¾å¤‡: {device}")
    print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"è®­ç»ƒè½®æ•°: {NUM_EPOCHS}")
    print(f"å­¦ä¹ ç‡: {LEARNING_RATE}")
    print(f"æ¨¡å‹ç»´åº¦: {CLIP_DIM} -> {HIDDEN_DIM} -> {CLIP_DIM}")
    print(f"æ³¨æ„: ViT-bigG-14 çš„å®é™…ç‰¹å¾ç»´åº¦æ˜¯ 1280")
    print(f"æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•: {CHECKPOINT_DIR}")
    print(f"{'='*60}\n")
    
    # ==================== åŠ è½½CLIPæ¨¡å‹ ====================
    print("ğŸ“¦ æ­£åœ¨åŠ è½½é¢„è®­ç»ƒçš„CLIPæ¨¡å‹...")
    clip_model, _, preprocess = open_clip.create_model_and_transforms(
        'ViT-bigG-14', 
        pretrained='laion2b_s39b_b160k'
    )
    tokenizer = open_clip.get_tokenizer('ViT-bigG-14')
    
    # å°†CLIPç§»åˆ°è®¾å¤‡ä¸Šå¹¶å†»ç»“å‚æ•°
    clip_model = clip_model.to(device)
    for param in clip_model.parameters():
        param.requires_grad = False  # å†»ç»“CLIPï¼Œä¸è®­ç»ƒå®ƒ
    
    print("âœ… CLIPæ¨¡å‹åŠ è½½æˆåŠŸå¹¶å·²å†»ç»“å‚æ•°\n")
    
    # ==================== åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨ ====================
    print("ğŸ“Š æ­£åœ¨åˆ›å»ºæ•°æ®é›†...")
    dataset = COCOCaptionsDataset(
        images_path=IMAGES_HDF5_PATH,
        annots_path=ANNOTS_NPY_PATH,
        tokenizer=tokenizer
    )
    
    data_loader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=4,  # å¤šè¿›ç¨‹åŠ è½½ï¼ŒåŠ é€Ÿè®­ç»ƒ
        pin_memory=True if torch.cuda.is_available() else False
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    print(f"   - æ€»æ ·æœ¬æ•°: {len(dataset)}")
    print(f"   - æ¯epochæ‰¹æ¬¡æ•°: {len(data_loader)}\n")
    
    # ==================== åˆ›å»ºæ–‡æœ¬é€‚é…å™¨ ====================
    print("ğŸ§  æ­£åœ¨åˆå§‹åŒ–æ–‡æœ¬é€‚é…å™¨...")
    text_adapter = TextAdapter(
        input_dim=CLIP_DIM,
        hidden_dim=HIDDEN_DIM,
        output_dim=CLIP_DIM
    ).to(device)
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in text_adapter.parameters())
    trainable_params = sum(p.numel() for p in text_adapter.parameters() if p.requires_grad)
    print(f"âœ… æ–‡æœ¬é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ")
    print(f"   - æ€»å‚æ•°é‡: {total_params:,}")
    print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\n")
    
    # ==================== åˆ›å»ºä¼˜åŒ–å™¨ ====================
    optimizer = torch.optim.AdamW(
        text_adapter.parameters(),
        lr=LEARNING_RATE,
        weight_decay=0.01  # L2æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    )
    
    print(f"âš™ï¸ ä¼˜åŒ–å™¨: AdamW (lr={LEARNING_RATE}, weight_decay=0.01)\n")
    
    # ==================== è®­ç»ƒå¾ªç¯ ====================
    print(f"{'='*60}")
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒï¼")
    print(f"{'='*60}\n")
    
    best_loss = float('inf')
    start_time = datetime.now()
    
    for epoch in range(NUM_EPOCHS):
        print(f"\nğŸ“… Epoch {epoch+1}/{NUM_EPOCHS}")
        print("-" * 60)
        
        # è®­ç»ƒä¸€ä¸ªepoch
        avg_loss = train_one_epoch(
            text_adapter=text_adapter,
            clip_model=clip_model,
            data_loader=data_loader,
            optimizer=optimizer,
            device=device,
            epoch=epoch
        )
        
        print(f"\nâœ¨ Epoch {epoch+1} å®Œæˆï¼")
        print(f"   - å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(
            CHECKPOINT_DIR, 
            f'text_adapter_epoch{epoch+1}.pth'
        )
        torch.save({
            'epoch': epoch + 1,
            'model_state_dict': text_adapter.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': avg_loss,
        }, checkpoint_path)
        print(f"   - æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_model_path = os.path.join(CHECKPOINT_DIR, 'text_adapter_best.pth')
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': text_adapter.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }, best_model_path)
            print(f"   - ğŸ† æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼(loss: {best_loss:.4f})")
    
    # ==================== è®­ç»ƒå®Œæˆ ====================
    end_time = datetime.now()
    training_time = end_time - start_time
    
    print(f"\n{'='*60}")
    print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"{'='*60}")
    print(f"æ€»è®­ç»ƒæ—¶é—´: {training_time}")
    print(f"æœ€ä½³æŸå¤±: {best_loss:.4f}")
    print(f"æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: {os.path.join(CHECKPOINT_DIR, 'text_adapter_best.pth')}")
    print(f"{'='*60}\n")

# ==================================
# 5. ç¨‹åºå…¥å£
# ==================================

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()