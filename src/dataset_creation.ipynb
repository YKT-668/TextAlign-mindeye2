{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b0db3a-94ec-44df-9092-32baba702478",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nsd_access'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# nsd_access is from this repo: https://github.com/tknapen/nsd_access\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# also see https://cvnlab.slite.page/p/dC~rBTjqjb/How-to-get-the-data for how to download the NSD data!\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnsd_access\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NSDAccess\n\u001b[1;32m     19\u001b[0m nsd_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/scratch/gpfs/KNORMAN/natural-scenes-dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m nsda \u001b[38;5;241m=\u001b[39m NSDAccess(nsd_path)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nsd_access'"
     ]
    }
   ],
   "source": [
    "import time \n",
    "from subprocess import call\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import webdataset as wds\n",
    "import sys\n",
    "\n",
    "# nsd_access is from this repo: https://github.com/tknapen/nsd_access\n",
    "# also see https://cvnlab.slite.page/p/dC~rBTjqjb/How-to-get-the-data for how to download the NSD data!\n",
    "from nsd_access import NSDAccess\n",
    "nsd_path = '/scratch/gpfs/KNORMAN/natural-scenes-dataset'\n",
    "nsda = NSDAccess(nsd_path)\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b1c223-e4dc-495f-a38f-a342b4c89a5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp = '/scratch/gpfs/KNORMAN'\n",
    "shared1000 = np.load(\"shared1000.npy\") # download from https://huggingface.co/datasets/pscotti/mindeyev2/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89562194-524e-449f-8733-36347809c73a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sub in [0]: #,1,2,3,4,5,6,7]:\n",
    "    subject=f'subj0{sub+1}'\n",
    "    subj=subject\n",
    "    print(subject)\n",
    "    \n",
    "    abs_cnt = -1\n",
    "    abs_notshared1000_cnt = -1\n",
    "    abs_shared1000_cnt = -1\n",
    "    \n",
    "    # load coco 73k indices\n",
    "    indices_path = \"COCO_73k_subj_indices.hdf5\"\n",
    "    hdf5_file = h5py.File(indices_path, \"r\")\n",
    "    indices = hdf5_file[f\"{subj}\"][:]\n",
    "\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30]) \n",
    "    nsessions=nsessions_allsubj[sub];\n",
    "    ntrials = 750*nsessions\n",
    "    print(nsessions,ntrials)\n",
    "\n",
    "    print(time.strftime(\"\\nCurrent time: %H:%M:%S\", time.localtime())) \n",
    "    \n",
    "    file = f\"/scratch/gpfs/KNORMAN/natural-scenes-dataset/nsddata/ppdata/{subject}/func1pt8mm/roi/nsdgeneral.nii.gz\"\n",
    "    nifti = nib.load(file) \n",
    "    mask = nifti.get_data()\n",
    "    mask[mask<1] = 0 \n",
    "    nsdgeneral_mask = mask\n",
    "\n",
    "    for tar in tqdm(range(nsessions)):\n",
    "        sess=tar+1\n",
    "        \n",
    "        behav = nsda.read_behavior(subject=subject, \n",
    "                    session_index=sess, \n",
    "                    trial_index=[]) \n",
    "\n",
    "        # pull single-trial betas and mask them\n",
    "        betas = nsda.read_betas(subject=subject, \n",
    "                            session_index=sess, \n",
    "                            trial_index=[], # empty list as index means get all for this session\n",
    "                            data_type='betas_fithrf', # GLMSingle beta2\n",
    "                            data_format='func1pt8mm') \n",
    "\n",
    "        # betas = betas[mask]\n",
    "        betas = np.moveaxis(betas,-1,0)\n",
    "        \n",
    "        vox_include = copy.deepcopy(nsdgeneral_mask)\n",
    "        ncsnr = nib.load(f\"{subject}_ncsnr.nii.gz\").get_fdata()\n",
    "        ncsnr[ncsnr<.15] = np.nan \n",
    "        if tar==0: print(\"voxels left:\", len(vox_include[vox_include>0]))\n",
    "        vox_include[np.isnan(ncsnr)] -= 1 # keep all nsdgeneral voxels even if they are below the threshold\n",
    "        vox_include[vox_include<0] = 0\n",
    "        if tar==0: print(\"voxels left after ncsnr thresholding:\", len(vox_include[vox_include>0])) # subj01 = 49329\n",
    "        \n",
    "        betas = betas.reshape(len(betas),-1)\n",
    "        betas = betas[:,vox_include.flatten().astype(bool)]\n",
    "        shape = betas.shape\n",
    "        scalar = StandardScaler(with_mean=True, with_std=True).fit(betas) # YOU SHOULD EXCLUDE SHARED1000 FROM THIS (NOT DONE HERE BUT DONE IN ACTUAL MINDEYE2 PAPER)\n",
    "        betas_mean = scalar.mean_\n",
    "        betas_std = scalar.scale_\n",
    "        betas = (betas - betas_mean) / betas_std\n",
    "        betas = betas.reshape(shape).astype('float16') # (1, 15724)    \n",
    "        \n",
    "        globals()[f'betas_ses{sess}'] = betas  \n",
    "        globals()[f'behav_ses{sess}'] = behav   \n",
    "        print(betas.shape)\n",
    "        \n",
    "    for tar in range(nsessions):\n",
    "        sess=tar+1\n",
    "        \n",
    "        if sess==1:\n",
    "            betas_all = globals()[f'betas_ses{sess}']\n",
    "        else:\n",
    "            betas_all = np.vstack((betas_all,globals()[f'betas_ses{sess}']))\n",
    "        print(betas_all.shape)\n",
    "        \n",
    "    with h5py.File(f'betas_{subject}.hdf5', 'w') as f:\n",
    "        f.create_dataset('betas', data=betas_all)\n",
    "    print(f\"saved betas_{subject}.hdf5\")\n",
    "        \n",
    "    os.makedirs(f\"{tmp}/mindeyev2_wds/{subj}\",exist_ok=True)\n",
    "    os.makedirs(f\"{tmp}/mindeyev2_wds/{subj}/train\",exist_ok=True)\n",
    "    os.makedirs(f\"{tmp}/mindeyev2_wds/{subj}/test\",exist_ok=True)\n",
    "    sink1 = wds.TarWriter(f\"{tmp}/mindeyev2_wds/{subj}/test/0.tar\")\n",
    "    for tar in tqdm(range(nsessions)):\n",
    "        behav = globals()[f'behav_ses{tar+1}']\n",
    "        \n",
    "        sink2 = wds.TarWriter(f\"{tmp}/mindeyev2_wds/{subj}/train/{tar}.tar\")\n",
    "        for i in range(len(behav)):\n",
    "            abs_cnt += 1                \n",
    "\n",
    "            trial_numbers = np.where(indices==indices[abs_cnt])[0]\n",
    "            assert np.isin(abs_cnt,trial_numbers)\n",
    "            trial_numbers[trial_numbers == abs_cnt] = -1 # current trial becomes negative 1\n",
    "            if len(trial_numbers) == 1:\n",
    "                trial_numbers = np.append(trial_numbers, -1)\n",
    "                trial_numbers = np.append(trial_numbers, -1)\n",
    "            if len(trial_numbers) == 2:\n",
    "                trial_numbers = np.append(trial_numbers, -1)\n",
    "            assert len(trial_numbers) == 3\n",
    "\n",
    "            sess=tar+1\n",
    "            behav = globals()[f'behav_ses{sess}']\n",
    "            behav_matrix = np.ones((1, 17))*-1\n",
    "            jjj=-1\n",
    "            for j in range(1):\n",
    "                jj = i-j\n",
    "                jjj += 1\n",
    "\n",
    "                if jj >= 0:\n",
    "                    # change NaNs to negative-one integers\n",
    "                    iscorrect = behav.iloc[jj]['ISCORRECT']\n",
    "                    if np.isnan(iscorrect): iscorrect = -1\n",
    "\n",
    "                    isoldcurrent = behav.iloc[jj]['ISOLDCURRENT']\n",
    "                    if np.isnan(isoldcurrent): isoldcurrent = -1\n",
    "\n",
    "                    iscorrectcurrent = behav.iloc[jj]['ISCORRECTCURRENT']\n",
    "                    if np.isnan(iscorrectcurrent): iscorrectcurrent = -1\n",
    "\n",
    "                    rt = behav.iloc[jj]['RT']\n",
    "                    if np.isnan(rt): rt = -1\n",
    "\n",
    "                    changemind = behav.iloc[jj]['CHANGEMIND']\n",
    "                    if np.isnan(changemind): changemind = -1\n",
    "\n",
    "                    button = behav.iloc[jj]['BUTTON']\n",
    "                    if np.isnan(button): button = -1\n",
    "\n",
    "                    total1 = behav.iloc[jj]['TOTAL1']\n",
    "                    if np.isnan(total1): total1 = -1\n",
    "\n",
    "                    total2 = behav.iloc[jj]['TOTAL2']\n",
    "                    if np.isnan(total2): total2 = -1\n",
    "                    \n",
    "                    coco73 = int(behav.iloc[jj]['73KID'])-1\n",
    "                    assert coco73 >= 0 and coco73 < 730000\n",
    "\n",
    "                    behavior = {\n",
    "                        \"cocoidx\": coco73, #0\n",
    "                        \"subject\": sub+1,                          #1\n",
    "                        \"session\": int(behav.iloc[jj]['SESSION']), #2\n",
    "                        \"run\": int(behav.iloc[jj]['RUN']),         #3\n",
    "                        \"trial\": int(behav.iloc[jj]['TRIAL']),     #4\n",
    "                        \"global_trial\": (int(behav.iloc[jj]['SESSION'])-1)*750 + jj,        #5\n",
    "                        \"time\": int(behav.iloc[jj]['TIME']),       #6\n",
    "                        \"isold\": int(behav.iloc[jj]['ISOLD']),     #7\n",
    "                        \"iscorrect\": iscorrect,                    #8\n",
    "                        \"rt\": rt, # 0 = no RT                      #9\n",
    "                        \"changemind\": changemind,                  #10\n",
    "                        \"isoldcurrent\": isoldcurrent,              #11\n",
    "                        \"iscorrectcurrent\": iscorrectcurrent,      #12\n",
    "                        \"total1\": total1,   #13\n",
    "                        \"total2\": total2,   #14\n",
    "                        \"button\": button,                          #15\n",
    "                        \"shared1000\": shared1000[int(behav.iloc[jj]['73KID'])-1], #16\n",
    "                    }\n",
    "                    \n",
    "                    assert (int(behav.iloc[jj]['SESSION'])-1)*750 + jj >= 0\n",
    "                    assert (int(behav.iloc[jj]['SESSION'])-1)*750 + jj < 27750\n",
    "\n",
    "                    behav_matrix[jjj] = np.array(list(behavior.values()))\n",
    "                    \n",
    "            past_behav_matrix = np.ones((15, 17))*-1\n",
    "            jjj=-1\n",
    "            for j in range(1,16):\n",
    "                jj = i-j\n",
    "                jjj += 1\n",
    "\n",
    "                if jj >= 0:\n",
    "                    # change NaNs to negative-one integers\n",
    "                    iscorrect = behav.iloc[jj]['ISCORRECT']\n",
    "                    if np.isnan(iscorrect): iscorrect = -1\n",
    "\n",
    "                    isoldcurrent = behav.iloc[jj]['ISOLDCURRENT']\n",
    "                    if np.isnan(isoldcurrent): isoldcurrent = -1\n",
    "\n",
    "                    iscorrectcurrent = behav.iloc[jj]['ISCORRECTCURRENT']\n",
    "                    if np.isnan(iscorrectcurrent): iscorrectcurrent = -1\n",
    "\n",
    "                    rt = behav.iloc[jj]['RT']\n",
    "                    if np.isnan(rt): rt = -1\n",
    "\n",
    "                    changemind = behav.iloc[jj]['CHANGEMIND']\n",
    "                    if np.isnan(changemind): changemind = -1\n",
    "\n",
    "                    button = behav.iloc[jj]['BUTTON']\n",
    "                    if np.isnan(button): button = -1\n",
    "\n",
    "                    total1 = behav.iloc[jj]['TOTAL1']\n",
    "                    if np.isnan(total1): total1 = -1\n",
    "\n",
    "                    total2 = behav.iloc[jj]['TOTAL2']\n",
    "                    if np.isnan(total2): total2 = -1\n",
    "                    \n",
    "                    coco73 = int(behav.iloc[jj]['73KID'])-1\n",
    "                    assert coco73 >= 0 and coco73 < 730000\n",
    "\n",
    "                    behavior = {\n",
    "                        \"cocoidx\": coco73, #0\n",
    "                        \"subject\": sub+1,                          #1\n",
    "                        \"session\": int(behav.iloc[jj]['SESSION']), #2\n",
    "                        \"run\": int(behav.iloc[jj]['RUN']),         #3\n",
    "                        \"trial\": int(behav.iloc[jj]['TRIAL']),     #4\n",
    "                        \"global_trial\": (int(behav.iloc[jj]['SESSION'])-1)*750 + jj,        #5\n",
    "                        \"time\": int(behav.iloc[jj]['TIME']),       #6\n",
    "                        \"isold\": int(behav.iloc[jj]['ISOLD']),     #7\n",
    "                        \"iscorrect\": iscorrect,                    #8\n",
    "                        \"rt\": rt, # 0 = no RT                      #9\n",
    "                        \"changemind\": changemind,                  #10\n",
    "                        \"isoldcurrent\": isoldcurrent,              #11\n",
    "                        \"iscorrectcurrent\": iscorrectcurrent,      #12\n",
    "                        \"total1\": total1,   #13\n",
    "                        \"total2\": total2,   #14\n",
    "                        \"button\": button,                          #15\n",
    "                        \"shared1000\": shared1000[int(behav.iloc[jj]['73KID'])-1], #16\n",
    "                    }\n",
    "                    \n",
    "                    assert (int(behav.iloc[jj]['SESSION'])-1)*750 + jj >= 0\n",
    "                    assert (int(behav.iloc[jj]['SESSION'])-1)*750 + jj < 27750\n",
    "\n",
    "                    past_behav_matrix[jjj] = np.array(list(behavior.values()))\n",
    "                    \n",
    "            future_behav_matrix = np.ones((15, 17))*-1\n",
    "            jjj=-1\n",
    "            for j in range(1,16):\n",
    "                jj = i+j\n",
    "                jjj += 1\n",
    "\n",
    "                if jj >= 0 and jj<750:\n",
    "                    # change NaNs to negative-one integers\n",
    "                    iscorrect = behav.iloc[jj]['ISCORRECT']\n",
    "                    if np.isnan(iscorrect): iscorrect = -1\n",
    "\n",
    "                    isoldcurrent = behav.iloc[jj]['ISOLDCURRENT']\n",
    "                    if np.isnan(isoldcurrent): isoldcurrent = -1\n",
    "\n",
    "                    iscorrectcurrent = behav.iloc[jj]['ISCORRECTCURRENT']\n",
    "                    if np.isnan(iscorrectcurrent): iscorrectcurrent = -1\n",
    "\n",
    "                    rt = behav.iloc[jj]['RT']\n",
    "                    if np.isnan(rt): rt = -1\n",
    "\n",
    "                    changemind = behav.iloc[jj]['CHANGEMIND']\n",
    "                    if np.isnan(changemind): changemind = -1\n",
    "\n",
    "                    button = behav.iloc[jj]['BUTTON']\n",
    "                    if np.isnan(button): button = -1\n",
    "\n",
    "                    total1 = behav.iloc[jj]['TOTAL1']\n",
    "                    if np.isnan(total1): total1 = -1\n",
    "\n",
    "                    total2 = behav.iloc[jj]['TOTAL2']\n",
    "                    if np.isnan(total2): total2 = -1\n",
    "                    \n",
    "                    coco73 = int(behav.iloc[jj]['73KID'])-1\n",
    "                    assert coco73 >= 0 and coco73 < 730000\n",
    "\n",
    "                    behavior = {\n",
    "                        \"cocoidx\": coco73, #0\n",
    "                        \"subject\": sub+1,                          #1\n",
    "                        \"session\": int(behav.iloc[jj]['SESSION']), #2\n",
    "                        \"run\": int(behav.iloc[jj]['RUN']),         #3\n",
    "                        \"trial\": int(behav.iloc[jj]['TRIAL']),     #4\n",
    "                        \"global_trial\": (int(behav.iloc[jj]['SESSION'])-1)*750 + jj,        #5\n",
    "                        \"time\": int(behav.iloc[jj]['TIME']),       #6\n",
    "                        \"isold\": int(behav.iloc[jj]['ISOLD']),     #7\n",
    "                        \"iscorrect\": iscorrect,                    #8\n",
    "                        \"rt\": rt, # 0 = no RT                      #9\n",
    "                        \"changemind\": changemind,                  #10\n",
    "                        \"isoldcurrent\": isoldcurrent,              #11\n",
    "                        \"iscorrectcurrent\": iscorrectcurrent,      #12\n",
    "                        \"total1\": total1,   #13\n",
    "                        \"total2\": total2,   #14\n",
    "                        \"button\": button,                          #15\n",
    "                        \"shared1000\": shared1000[int(behav.iloc[jj]['73KID'])-1], #16\n",
    "                    }\n",
    "                    \n",
    "                    assert (int(behav.iloc[jj]['SESSION'])-1)*750 + jj >= 0\n",
    "                    assert (int(behav.iloc[jj]['SESSION'])-1)*750 + jj < 27750\n",
    "\n",
    "                    future_behav_matrix[jjj] = np.array(list(behavior.values()))\n",
    "\n",
    "            olds_behav_matrix = np.ones((3, 17))*-1\n",
    "            jjj=-1\n",
    "            for j in range(3):\n",
    "                jj = trial_numbers[j]\n",
    "\n",
    "                if jj>=0:\n",
    "                    jjj += 1\n",
    "                    old_session = int(np.floor(jj / 750)) + 1\n",
    "                    old_trial = jj % 750\n",
    "                    behav = globals()[f'behav_ses{old_session}']\n",
    "                    jj = old_trial\n",
    "\n",
    "                    # change NaNs to negative-one integers\n",
    "                    iscorrect = behav.iloc[jj]['ISCORRECT']\n",
    "                    if np.isnan(iscorrect): iscorrect = -1\n",
    "\n",
    "                    isoldcurrent = behav.iloc[jj]['ISOLDCURRENT']\n",
    "                    if np.isnan(isoldcurrent): isoldcurrent = -1\n",
    "\n",
    "                    iscorrectcurrent = behav.iloc[jj]['ISCORRECTCURRENT']\n",
    "                    if np.isnan(iscorrectcurrent): iscorrectcurrent = -1\n",
    "\n",
    "                    rt = behav.iloc[jj]['RT']\n",
    "                    if np.isnan(rt): rt = -1\n",
    "\n",
    "                    changemind = behav.iloc[jj]['CHANGEMIND']\n",
    "                    if np.isnan(changemind): changemind = -1\n",
    "\n",
    "                    button = behav.iloc[jj]['BUTTON']\n",
    "                    if np.isnan(button): button = -1\n",
    "\n",
    "                    total1 = behav.iloc[jj]['TOTAL1']\n",
    "                    if np.isnan(total1): total1 = -1\n",
    "\n",
    "                    total2 = behav.iloc[jj]['TOTAL2']\n",
    "                    if np.isnan(total2): total2 = -1\n",
    "                    \n",
    "                    coco73 = int(behav.iloc[jj]['73KID'])-1\n",
    "                    assert coco73 >= 0 and coco73 < 730000\n",
    "\n",
    "                    behavior = {\n",
    "                        \"cocoidx\": coco73, #0\n",
    "                        \"subject\": sub+1,                          #1\n",
    "                        \"session\": int(behav.iloc[jj]['SESSION']), #2\n",
    "                        \"run\": int(behav.iloc[jj]['RUN']),         #3\n",
    "                        \"trial\": int(behav.iloc[jj]['TRIAL']),     #4\n",
    "                        \"global_trial\": (int(behav.iloc[jj]['SESSION'])-1)*750 + jj,        #5\n",
    "                        \"time\": int(behav.iloc[jj]['TIME']),       #6\n",
    "                        \"isold\": int(behav.iloc[jj]['ISOLD']),     #7\n",
    "                        \"iscorrect\": iscorrect,                    #8\n",
    "                        \"rt\": rt, # 0 = no RT                      #9\n",
    "                        \"changemind\": changemind,                  #10\n",
    "                        \"isoldcurrent\": isoldcurrent,              #11\n",
    "                        \"iscorrectcurrent\": iscorrectcurrent,      #12\n",
    "                        \"total1\": total1,   #13\n",
    "                        \"total2\": total2,   #14\n",
    "                        \"button\": button,                          #15\n",
    "                        \"shared1000\": shared1000[int(behav.iloc[jj]['73KID'])-1], #16\n",
    "                    }\n",
    "                    \n",
    "                    assert (int(behav.iloc[jj]['SESSION'])-1)*750 + jj >= 0\n",
    "                    assert (int(behav.iloc[jj]['SESSION'])-1)*750 + jj < 27750\n",
    "\n",
    "                    olds_behav_matrix[jjj] = np.array(list(behavior.values()))\n",
    "\n",
    "            behav = globals()[f'behav_ses{sess}']\n",
    "            # Check if this is a shared1000 trial\n",
    "            if shared1000[int(behav.iloc[i]['73KID'])-1]:\n",
    "                abs_shared1000_cnt += 1\n",
    "            else:\n",
    "                abs_notshared1000_cnt += 1\n",
    "                \n",
    "            with torch.no_grad(): #https://cvnlab.slite.page/p/fRv4lz5V2F/Untitled\n",
    "                if shared1000[int(behav.iloc[i]['73KID'])-1]:\n",
    "                    sink1.write({\n",
    "                        \"__key__\": \"sample%09d\" % abs_shared1000_cnt,\n",
    "                        \"behav.npy\": behav_matrix,\n",
    "                        \"past_behav.npy\": past_behav_matrix,\n",
    "                        \"future_behav.npy\": future_behav_matrix,\n",
    "                        \"olds_behav.npy\": olds_behav_matrix,\n",
    "                    })\n",
    "                    assert behav_matrix[-1,0] < 73000\n",
    "                else:\n",
    "                    sink2.write({\n",
    "                        \"__key__\": \"sample%09d\" % abs_notshared1000_cnt,\n",
    "                        \"behav.npy\": behav_matrix,\n",
    "                        \"past_behav.npy\": past_behav_matrix,\n",
    "                        \"future_behav.npy\": future_behav_matrix,\n",
    "                        \"olds_behav.npy\": olds_behav_matrix,\n",
    "                    })\n",
    "                    assert behav_matrix[-1,0] < 73000\n",
    "        sink2.close()\n",
    "    sink1.close()\n",
    "    \n",
    "    print(time.strftime(\"\\nCurrent time: %H:%M:%S\", time.localtime())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21bdad64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Images: /home/vipuser/MindEyeV2_Project/src/evals/all_images.pt\n",
      "Ground Truth Captions: /home/vipuser/MindEyeV2_Project/src/evals/all_captions.pt\n",
      "Reconstructed Images Directory: /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/images\n"
     ]
    }
   ],
   "source": [
    "# --- 定义所有输入文件的绝对路径 ---\n",
    "\n",
    "# 评估用的标准图像和标题\n",
    "ALL_IMAGES_PT = \"/home/vipuser/MindEyeV2_Project/src/evals/all_images.pt\"\n",
    "ALL_CAPTIONS_PT = \"/home/vipuser/MindEyeV2_Project/src/evals/all_captions.pt\"\n",
    "\n",
    "# 你自己训练生成的重建图像所在的目录\n",
    "# 请确保这个路径是正确的\n",
    "RECONS_IMG_DIR = \"/home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/images\"\n",
    "\n",
    "# 打印路径以供检查\n",
    "print(\"Ground Truth Images:\", ALL_IMAGES_PT)\n",
    "print(\"Ground Truth Captions:\", ALL_CAPTIONS_PT)\n",
    "print(\"Reconstructed Images Directory:\", RECONS_IMG_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "295acfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在强制升级 open_clip_torch 库到最新版本...\n",
      "Requirement already satisfied: open_clip_torch in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (3.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1017)'))': /simple/open-clip-torch/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1017)'))': /simple/open-clip-torch/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1017)'))': /simple/open-clip-torch/\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch>=2.0 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from open_clip_torch) (2.1.0)\n",
      "Requirement already satisfied: torchvision in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from open_clip_torch) (0.16.0)\n",
      "Requirement already satisfied: regex in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from open_clip_torch) (2025.11.3)\n",
      "Requirement already satisfied: ftfy in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from open_clip_torch) (6.3.1)\n",
      "Requirement already satisfied: tqdm in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from open_clip_torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from open_clip_torch) (0.23.2)\n",
      "Requirement already satisfied: safetensors in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from open_clip_torch) (0.6.2)\n",
      "Requirement already satisfied: timm>=1.0.17 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from open_clip_torch) (1.0.21)\n",
      "Requirement already satisfied: pyyaml in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from timm>=1.0.17->open_clip_torch) (6.0.3)\n",
      "Requirement already satisfied: filelock in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (4.15.0)\n",
      "Requirement already satisfied: sympy in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torch>=2.0->open_clip_torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->open_clip_torch) (12.9.86)\n",
      "Requirement already satisfied: wcwidth in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from ftfy->open_clip_torch) (0.2.14)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
      "Requirement already satisfied: requests in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from huggingface-hub->open_clip_torch) (2.32.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from requests->huggingface-hub->open_clip_torch) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from sympy->torch>=2.0->open_clip_torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torchvision->open_clip_torch) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages (from torchvision->open_clip_torch) (12.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Pretrained tag or path (openai) for 'ViT-H-14' not found. Available tags: ['laion2b_s32b_b79k', 'metaclip_fullcc', 'metaclip_altogether', 'dfn5b']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "升级完成！\n",
      "Source PNG directory: /home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/images\n",
      "Loading CLIP model ViT-H-14 from OpenAI...\n",
      "❌ 加载模型失败: Pretrained value 'openai' is not a known tag or valid file path\n",
      "如果升级后仍然失败，请检查网络连接或重启 Kernel 再试。\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Pretrained value 'openai' is not a known tag or valid file path",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ 加载模型失败: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m如果升级后仍然失败，请检查网络连接或重启 Kernel 再试。\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# --- 2. 找到所有 PNG 文件并计算特征 ---\u001b[39;00m\n\u001b[1;32m     37\u001b[0m png_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(RECONS_IMG_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading CLIP model ViT-H-14 from OpenAI...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# 升级后，这行代码应该就能正常工作了\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     model, _, preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mopen_clip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model_and_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mViT-H-14\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIP model loaded successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages/open_clip/factory.py:930\u001b[0m, in \u001b[0;36mcreate_model_and_transforms\u001b[0;34m(model_name, pretrained, load_weights, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_context_length, image_mean, image_std, image_interpolation, image_resize_mode, aug_cfg, pretrained_image, pretrained_text, pretrained_image_path, pretrained_text_path, cache_dir, output_dict, weights_only, **model_kwargs)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;124;03mCreates a contrastive vision-language model along with preprocessing transforms for training and validation.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;124;03m    any random augmentation.\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    922\u001b[0m force_preprocess_cfg \u001b[38;5;241m=\u001b[39m merge_preprocess_kwargs(\n\u001b[1;32m    923\u001b[0m     {},\n\u001b[1;32m    924\u001b[0m     mean\u001b[38;5;241m=\u001b[39mimage_mean,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     resize_mode\u001b[38;5;241m=\u001b[39mimage_resize_mode,\n\u001b[1;32m    928\u001b[0m )\n\u001b[0;32m--> 930\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_custom_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_custom_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_image_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_image_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_context_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_context_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_image_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_image_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_text_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_text_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m pp_cfg \u001b[38;5;241m=\u001b[39m PreprocessCfg(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mvisual\u001b[38;5;241m.\u001b[39mpreprocess_cfg)\n\u001b[1;32m    955\u001b[0m preprocess_train \u001b[38;5;241m=\u001b[39m image_transform_v2(\n\u001b[1;32m    956\u001b[0m     pp_cfg,\n\u001b[1;32m    957\u001b[0m     is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    958\u001b[0m     aug_cfg\u001b[38;5;241m=\u001b[39maug_cfg,\n\u001b[1;32m    959\u001b[0m )\n",
      "File \u001b[0;32m/home/vipuser/miniconda3/envs/mindeye/lib/python3.10/site-packages/open_clip/factory.py:427\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, load_weights, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, force_context_length, pretrained_image, pretrained_text, pretrained_image_path, pretrained_text_path, cache_dir, output_dict, require_pretrained, weights_only, **model_kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    423\u001b[0m             logging\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    424\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrained tag or path (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_cleaned\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvailable tags: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlist_pretrained_tags_by_model(model_name_cleaned)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m             )\n\u001b[0;32m--> 427\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrained value \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a known tag or valid file path\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# Apply model config overrides\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Pretrained value 'openai' is not a known tag or valid file path"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# 卡点 A: 从 PNG 图片生成评估所需的 recons.pt 文件 (最终修正版)\n",
    "# ====================================================================\n",
    "\n",
    "# --- 0. 强制升级 open_clip_torch 库 ---\n",
    "# 这是解决问题的关键步骤\n",
    "print(\"正在强制升级 open_clip_torch 库到最新版本...\")\n",
    "!pip install --upgrade open_clip_torch\n",
    "print(\"升级完成！\")\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import open_clip # 重新导入\n",
    "\n",
    "# --- 重新确认路径 ---\n",
    "print(f\"Source PNG directory: {RECONS_IMG_DIR}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- 1. 加载 CLIP 模型 (使用 openai 标签) ---\n",
    "print(\"Loading CLIP model ViT-H-14 from OpenAI...\")\n",
    "try:\n",
    "    # 升级后，这行代码应该就能正常工作了\n",
    "    model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-H-14\", pretrained=\"openai\")\n",
    "    model = model.to(device).eval()\n",
    "    print(\"CLIP model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 加载模型失败: {e}\")\n",
    "    print(\"如果升级后仍然失败，请检查网络连接或重启 Kernel 再试。\")\n",
    "    raise e\n",
    "\n",
    "# --- 2. 找到所有 PNG 文件并计算特征 ---\n",
    "png_files = sorted(glob.glob(os.path.join(RECONS_IMG_DIR, \"*.png\")))\n",
    "if not png_files:\n",
    "    raise FileNotFoundError(f\"错误：在目录 {RECONS_IMG_DIR} 中没有找到任何 .png 文件。请检查路径！\")\n",
    "\n",
    "print(f\"Found {len(png_files)} PNG files to process.\")\n",
    "\n",
    "all_embeddings = []\n",
    "with torch.no_grad():\n",
    "    for file_path in tqdm(png_files, desc=\"Calculating embeddings\"):\n",
    "        image = Image.open(file_path).convert(\"RGB\")\n",
    "        image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "        embedding = model.encode_image(image_tensor)\n",
    "        embedding = embedding / embedding.norm(dim=-1, keepdim=True)\n",
    "        all_embeddings.append(embedding.cpu())\n",
    "\n",
    "recons_embeds = torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# --- 3. 保存特征文件 ---\n",
    "RECONS_PT_PATH = \"/home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/inference/recons.pt\"\n",
    "torch.save(recons_embeds, RECONS_PT_PATH)\n",
    "\n",
    "print(f\"\\nSuccessfully saved embeddings to: {RECONS_PT_PATH}\")\n",
    "print(f\"Shape of the saved tensor: {recons_embeds.shape}\")\n",
    "\n",
    "# --- 4. 将生成的特征加载到变量中，供后续使用 ---\n",
    "recons_embeds = torch.load(RECONS_PT_PATH).to(device)\n",
    "print(\"Reconstruction embeddings are loaded into 'recons_embeds' variable and ready for evaluation.\")\n",
    "\n",
    "# 清理显存\n",
    "del model\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6211eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 第 5 步: 加载 Ground Truth 数据并计算评估指标\n",
    "# ====================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. 加载 Ground Truth (GT) 图像和标题的特征 ---\n",
    "# 确保这些变量路径正确\n",
    "print(\"Loading Ground Truth embeddings...\")\n",
    "gt_images_pt = torch.load(ALL_IMAGES_PT, map_location=device)\n",
    "# gt_captions_pt = torch.load(ALL_CAPTIONS_PT, map_location=device) # 标题暂时不用，先注释掉\n",
    "\n",
    "# gt_images_pt 可能是一个字典，我们需要取出真正的特征张量\n",
    "# 常见的 key 是 'images' 或 'embeds'，我们检查一下\n",
    "if isinstance(gt_images_pt, dict):\n",
    "    if 'images' in gt_images_pt:\n",
    "        gt_embeds = gt_images_pt['images'].to(device)\n",
    "    elif 'embeds' in gt_images_pt:\n",
    "        gt_embeds = gt_images_pt['embeds'].to(device)\n",
    "    else:\n",
    "        raise KeyError(\"在 all_images.pt 文件中找不到 'images' 或 'embeds' 键\")\n",
    "else:\n",
    "    gt_embeds = gt_images_pt.to(device)\n",
    "\n",
    "print(f\"Ground Truth embeddings loaded. Shape: {gt_embeds.shape}\")\n",
    "print(f\"Reconstruction embeddings shape: {recons_embeds.shape}\")\n",
    "\n",
    "# --- 2. 检查并匹配样本数量 ---\n",
    "# 你的重建结果数量可能和GT数量不一致，这是正常的。评估时以前者为准。\n",
    "num_recons = recons_embeds.shape[0]\n",
    "if num_recons > gt_embeds.shape[0]:\n",
    "    print(f\"警告：重建图像数量 ({num_recons}) 大于GT图像数量 ({gt_embeds.shape[0]})。将只评估前 {gt_embeds.shape[0]} 个样本。\")\n",
    "    recons_embeds = recons_embeds[:gt_embeds.shape[0]]\n",
    "else:\n",
    "    # 从完整的GT中，只取出与你重建数量相匹配的前 N 个\n",
    "    gt_embeds = gt_embeds[:num_recons]\n",
    "\n",
    "print(f\"Embeddings matched for evaluation. Using {num_recons} samples.\")\n",
    "\n",
    "\n",
    "# --- 3. 计算指标 ---\n",
    "\n",
    "# 指标 1: CLIP Cosine Similarity (CLIP-Cos)\n",
    "# 逐个计算你的重建图像与对应的GT图像的相似度\n",
    "print(\"\\nCalculating CLIP Cosine Similarity...\")\n",
    "# (N, D) * (N, D) -> (N,)\n",
    "cos_sims = (recons_embeds * gt_embeds).sum(dim=1)\n",
    "clip_cos = cos_sims.mean().item()\n",
    "print(f\" => Average CLIP Cosine Similarity: {clip_cos:.4f}\")\n",
    "\n",
    "\n",
    "# 指标 2: Retrieval Accuracy (Top-1, Top-5)\n",
    "# 检查你的每个重建图像，在所有GT图像中，能否找回正确的那个\n",
    "print(\"\\nCalculating Retrieval Accuracy (Top-1, Top-5)...\")\n",
    "# 计算你的每个重建图像与 *所有* GT图像的相似度矩阵\n",
    "# (N, D) @ (D, N) -> (N, N)\n",
    "sim_matrix = torch.matmul(recons_embeds, gt_embeds.t())\n",
    "\n",
    "# 找到每个重建图像最相似的GT图像的索引\n",
    "_, top_indices = torch.topk(sim_matrix, k=5, dim=1)\n",
    "\n",
    "# 正确的索引应该是对角线 (0, 1, 2, ..., N-1)\n",
    "correct_indices = torch.arange(num_recons, device=device)\n",
    "\n",
    "# Top-1: 最相似的是否是正确的那个\n",
    "top1_correct = top_indices[:, 0] == correct_indices\n",
    "top1_new = top1_correct.float().mean().item()\n",
    "\n",
    "# Top-5: 前5个最相似的是否包含正确的那个\n",
    "top5_correct = (top_indices == correct_indices.unsqueeze(1)).any(dim=1)\n",
    "top5_new = top5_correct.float().mean().item()\n",
    "\n",
    "print(f\" => Top-1 Accuracy: {top1_new:.4f} ({int(top1_new*num_recons)}/{num_recons})\")\n",
    "print(f\" => Top-5 Accuracy: {top5_new:.4f} ({int(top5_new*num_recons)}/{num_recons})\")\n",
    "\n",
    "# 为了让最后一个单元格能用，我们把计算出的指标存为全局变量\n",
    "# （在Jupyter Notebook中，一个单元格的变量在运行后对其他单元格可见）\n",
    "print(\"\\nMetrics calculated and stored in variables: clip_cos, top1_new, top5_new\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5980b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# 第 6 步 (最终步): 将所有指标保存到 JSON 文件\n",
    "# ====================================================================\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- 1. 准备要保存的指标字典 ---\n",
    "# 我们从全局变量中获取上一步计算好的指标\n",
    "# 如果某些指标不存在（比如 mse, lpips），我们将其设为 None\n",
    "\n",
    "# 检查变量是否存在，不存在则设为 None\n",
    "# 这样做可以避免代码因缺少某个非核心指标而报错\n",
    "mse_value = float(mse) if 'mse' in globals() else None\n",
    "lpips_value = float(lpips) if 'lpips' in globals() else None\n",
    "latency_value = float(globals().get(\"latency_ms_per_img\")) if 'latency_ms_per_img' in globals() and globals().get(\"latency_ms_per_img\") is not None else None\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    \"top1_new\": float(top1_new),\n",
    "    \"top5_new\": float(top5_new),\n",
    "    \"clip_cosine\": float(clip_cos),\n",
    "    \"mse\": mse_value,\n",
    "    \"lpips\": lpips_value,\n",
    "    \"latency_ms\": latency_value,\n",
    "    \"set\": \"new_test\", # 标记这是哪次测试\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\") # 添加时间戳\n",
    "}\n",
    "\n",
    "# --- 2. 定义输出文件路径 ---\n",
    "# 我们将它保存在你的模型训练日志目录下，方便管理\n",
    "output_json_path = \"/home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/metrics.json\"\n",
    "\n",
    "# --- 3. 写入 JSON 文件 ---\n",
    "try:\n",
    "    with open(output_json_path, \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4) # indent=4 让文件格式更美观\n",
    "    print(f\"✅ 评估完成！指标已成功保存到: {output_json_path}\")\n",
    "    \n",
    "    # --- 4. 打印最终结果 ---\n",
    "    print(\"\\n--- Final Metrics ---\")\n",
    "    print(json.dumps(metrics, indent=4))\n",
    "    print(\"---------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 保存文件时出错: {e}\")\n",
    "    print(\"请检查路径 '/home/vipuser/train_logs/s1_ps1p5_h512_e5_cycle/' 是否存在且有写入权限。\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindeye",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
