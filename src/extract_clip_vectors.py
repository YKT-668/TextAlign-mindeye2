#!/usr/bin/env python
# coding: utf-8
"""
ç²¾ç®€ç‰ˆæ¨ç†è„šæœ¬ï¼šåªæå– brain->CLIP å‘é‡ï¼Œä¸åšå›¾åƒç”Ÿæˆ
é€‚ç”¨äºåç»­ RAG æ£€ç´¢å’Œ SD1.5/SDXL ç”Ÿæˆæµç¨‹
"""

import os
import sys
import json
import argparse
import numpy as np
import h5py
from tqdm import tqdm
import torch
import torch.nn as nn
import webdataset as wds

# æ·»åŠ æœ¬åœ°è·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, script_dir)

import utils
from models import BrainNetwork, PriorNetwork, BrainDiffusionPrior

# ç¦ç”¨ xformers é¿å…å…¼å®¹æ€§é—®é¢˜
os.environ["XFORMERS_DISABLED"] = "1"
os.environ["FLASH_ATTENTION_DISABLE"] = "1"

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

# ==================== å‚æ•°è§£æ ====================
parser = argparse.ArgumentParser(description="æå– brain->CLIP å‘é‡ï¼ˆç²¾ç®€ç‰ˆï¼‰")
parser.add_argument("--model_name", type=str, required=True, help="æ¨¡å‹åç§°")
parser.add_argument("--data_path", type=str, required=True, help="NSD æ•°æ®è·¯å¾„")
parser.add_argument("--subj", type=int, required=True, choices=[1,2,3,4,5,6,7,8], help="è¢«è¯•ç¼–å·")
parser.add_argument("--hidden_dim", type=int, default=1024, help="éšè—å±‚ç»´åº¦")
parser.add_argument("--n_blocks", type=int, default=4, help="Backbone å—æ•°")
parser.add_argument("--new_test", action="store_true", help="ä½¿ç”¨æ–°æµ‹è¯•é›†")
parser.add_argument("--clip_out", type=str, required=True, help="CLIP å‘é‡è¾“å‡ºè·¯å¾„ (.pt)")
parser.add_argument("--ids_out", type=str, default=None, help="å›¾åƒ ID è¾“å‡ºè·¯å¾„ (.json)")
parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")

args = parser.parse_args()

# ==================== æ ¹æ® checkpoint æ¨æ–­ hidden_dimï¼ˆå…¼å®¹å®˜æ–¹å¤§æ¨¡å‹ï¼‰ ====================
proj_root = os.path.dirname(script_dir)
candidate_model_dirs = [
    f"/home/vipuser/train_logs/{args.model_name}",
    f"/home/train_logs/{args.model_name}",
    os.path.join(proj_root, "train_logs", args.model_name),
]

model_dir_for_cfg = None
for od in candidate_model_dirs:
    if os.path.isdir(od):
        model_dir_for_cfg = od
        break
if model_dir_for_cfg is None:
    # å¦‚æœä¸€ä¸ªéƒ½æ²¡æ‰¾åˆ°ï¼Œå°±ç”¨æœ€åä¸€ä¸ªå€™é€‰è·¯å¾„ä½œä¸ºå…œåº•ï¼ˆæ–¹ä¾¿æœ¬åœ°/å…¶å®ƒæœºå™¨ï¼‰
    model_dir_for_cfg = candidate_model_dirs[-1]

def infer_hidden_dim(model_dir: str, default_h: int = 1024) -> int:
    """
    å°è¯•ä» args.json æˆ–æ¨¡å‹ç›®å½•åæ¨æ–­ hidden_dimï¼š
      1) è‹¥å­˜åœ¨ args.jsonï¼Œåˆ™ä¼˜å…ˆè¯»å– hidden_dim/h/H å­—æ®µï¼›
      2) è‹¥æ£€æµ‹åˆ°æ˜¯å®˜æ–¹ subj01 40sess æ¨¡å‹ï¼Œåˆ™ä½¿ç”¨ OFFICIAL_H=4096ï¼›
      3) å¦åˆ™é€€å›é»˜è®¤å€¼ default_hã€‚
    """
    args_path = os.path.join(model_dir, "args.json")
    if os.path.exists(args_path):
        try:
            with open(args_path, "r", encoding="utf-8") as f:
                cfg = json.load(f)
            for k in ("hidden_dim", "h", "H"):
                if k in cfg:
                    h = int(cfg[k])
                    print(f"   - ä» args.json è¯»å– hidden_dim={h}")
                    return h
            print(f"   - args.json ä¸­æœªæ‰¾åˆ° hidden_dim å­—æ®µï¼Œä½¿ç”¨é»˜è®¤ hidden_dim={default_h}")
            return default_h
        except Exception as e:
            print(f"   - è­¦å‘Š: è¯»å– args.json å¤±è´¥({e})ï¼Œä½¿ç”¨é»˜è®¤ hidden_dim={default_h}")
            return default_h

    base = os.path.basename(os.path.normpath(model_dir))
    if "final_subj01_pretrained_40sess_24bs" in base:
        OFFICIAL_H = 4096
        print("   - è­¦å‘Š: æ‰¾ä¸åˆ° args.jsonï¼Œæ£€æµ‹åˆ°æ˜¯å®˜æ–¹ subj01 40sess æ¨¡å‹ï¼Œä½¿ç”¨ OFFICIAL_H=4096.")
        return OFFICIAL_H

    print(f"   - è­¦å‘Š: æ‰¾ä¸åˆ° args.jsonï¼Œä½¿ç”¨é»˜è®¤ hidden_dim={default_h}")
    return default_h

# è¦†ç›–å‘½ä»¤è¡Œé‡Œçš„ hidden_dimï¼Œç¡®ä¿å®˜æ–¹å¤§æ¨¡å‹ç”¨ 4096
args.hidden_dim = infer_hidden_dim(model_dir_for_cfg, args.hidden_dim)

# è®¾ç½®éšæœºç§å­
utils.seed_everything(args.seed)

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(os.path.dirname(args.clip_out), exist_ok=True)
if args.ids_out:
    os.makedirs(os.path.dirname(args.ids_out), exist_ok=True)

print(f"\n{'='*60}")
print(f"ğŸ“‹ é…ç½®ä¿¡æ¯")
print(f"{'='*60}")
print(f"æ¨¡å‹åç§°: {args.model_name}")
print(f"è¢«è¯•: subj0{args.subj}")
print(f"éšè—å±‚ç»´åº¦: {args.hidden_dim}")
print(f"CLIP å‘é‡è¾“å‡º: {args.clip_out}")
if args.ids_out:
    print(f"å›¾åƒ ID è¾“å‡º: {args.ids_out}")
print(f"{'='*60}\n")

# ==================== åŠ è½½ fMRI æ•°æ® ====================
print("ğŸ“¦ åŠ è½½ fMRI ä½“ç´ æ•°æ®...")
voxels = {}
f = h5py.File(f'{args.data_path}/betas_all_subj0{args.subj}_fp32_renorm.hdf5', 'r')
betas = f['betas'][:]
betas = torch.Tensor(betas).to("cpu")
num_voxels = betas[0].shape[-1]
voxels[f'subj0{args.subj}'] = betas
print(f"âœ… åŠ è½½å®Œæˆï¼Œä½“ç´ æ•°: {num_voxels}")

# ==================== åŠ è½½æµ‹è¯•é›† ====================
print("\nğŸ“Š åŠ è½½æµ‹è¯•é›†...")
if not args.new_test:
    if args.subj in [3, 6]:
        num_test = 2113
    elif args.subj in [4, 8]:
        num_test = 1985
    else:
        num_test = 2770
    test_url = f"{args.data_path}/wds/subj0{args.subj}/test/0.tar"
else:
    if args.subj in [3, 6]:
        num_test = 2371
    elif args.subj in [4, 8]:
        num_test = 2188
    else:
        num_test = 3000
    test_url = f"{args.data_path}/wds/subj0{args.subj}/new_test/0.tar"

print(f"æµ‹è¯•é›†è·¯å¾„: {test_url}")

def my_split_by_node(urls):
    return urls

test_data = (
    wds.WebDataset(test_url, resampled=False, nodesplitter=my_split_by_node)
    .decode("torch")
    .rename(
        behav="behav.npy",
        past_behav="past_behav.npy",
        future_behav="future_behav.npy",
        olds_behav="olds_behav.npy",
    )
    .to_tuple(*["behav", "past_behav", "future_behav", "olds_behav"])
)

test_dl = torch.utils.data.DataLoader(
    test_data,
    batch_size=num_test,
    shuffle=False,
    drop_last=True,
    pin_memory=True,
)
print(f"âœ… æµ‹è¯•é›†åŠ è½½å®Œæˆï¼Œæ ·æœ¬æ•°: {num_test}")

# ==================== å‡†å¤‡æµ‹è¯•æ•°æ®ç´¢å¼• ====================
print("\nğŸ” å‡†å¤‡æµ‹è¯•æ•°æ®ç´¢å¼•...")
test_images_idx = []
test_voxels_idx = []

for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):
    test_voxels = voxels[f'subj0{args.subj}'][behav[:, 0, 5].cpu().long()]
    test_voxels_idx = np.append(test_voxels_idx, behav[:, 0, 5].cpu().numpy())
    test_images_idx = np.append(test_images_idx, behav[:, 0, 0].cpu().numpy())

test_images_idx = test_images_idx.astype(int)
test_voxels_idx = test_voxels_idx.astype(int)

unique_images = np.unique(test_images_idx)
print(f"âœ… ç´¢å¼•å‡†å¤‡å®Œæˆ")
print(f"   - æ€»ä½“ç´ æ ·æœ¬: {len(test_voxels)}")
print(f"   - å”¯ä¸€å›¾åƒæ•°: {len(unique_images)}")

# ==================== æ„å»º MindEye æ¨¡å‹ ====================
print(f"\nğŸ§  æ„å»º MindEye æ¨¡å‹...")

# CLIP å‚æ•°ï¼ˆå¯¹åº” ViT-bigG-14ï¼‰
clip_seq_dim = 256
clip_emb_dim = 1664

class MindEyeModule(nn.Module):
    def __init__(self):
        super(MindEyeModule, self).__init__()

    def forward(self, x):
        return x

model = MindEyeModule()

# Ridge Regression
class RidgeRegression(torch.nn.Module):
    def __init__(self, input_sizes, out_features):
        super(RidgeRegression, self).__init__()
        self.out_features = out_features
        self.linears = torch.nn.ModuleList(
            [torch.nn.Linear(input_size, out_features) for input_size in input_sizes]
        )

    def forward(self, x, subj_idx):
        out = self.linears[subj_idx](x[:, 0]).unsqueeze(1)
        return out

model.ridge = RidgeRegression([num_voxels], out_features=args.hidden_dim)

# Backbone Network
model.backbone = BrainNetwork(
    h=args.hidden_dim,
    in_dim=args.hidden_dim,
    seq_len=1,
    clip_size=clip_emb_dim,
    out_dim=clip_emb_dim * clip_seq_dim,
)

# Diffusion Prior
out_dim = clip_emb_dim
depth = 6
dim_head = 52
heads = clip_emb_dim // 52
timesteps = 100

prior_network = PriorNetwork(
    dim=out_dim,
    depth=depth,
    dim_head=dim_head,
    heads=heads,
    causal=False,
    num_tokens=clip_seq_dim,
    learned_query_mode="pos_emb",
)

model.diffusion_prior = BrainDiffusionPrior(
    net=prior_network,
    image_embed_dim=out_dim,
    condition_on_text_encodings=False,
    timesteps=timesteps,
    cond_drop_prob=0.2,
    image_embed_scale=None,
)

model.to(device)

# ç»Ÿè®¡å‚æ•°
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"âœ… æ¨¡å‹æ„å»ºå®Œæˆ")
print(f"   - æ€»å‚æ•°: {total_params:,}")
print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

# ==================== åŠ è½½é¢„è®­ç»ƒæƒé‡ ====================
print(f"\nğŸ“¥ åŠ è½½é¢„è®­ç»ƒæƒé‡...")

# ä¼˜å…ˆåœ¨å¸¸è§ä½ç½®æŸ¥æ‰¾ last.pthï¼ŒæŒ‰é¡ºåºå°è¯•ï¼Œä¾¿äºåœ¨ä¸åŒéƒ¨ç½²ä¸‹ç›´æ¥è¿è¡Œ
proj_root = os.path.dirname(script_dir)
candidate_outdirs = [
    f"/home/vipuser/train_logs/{args.model_name}",
    f"/home/train_logs/{args.model_name}",
    os.path.join(proj_root, "train_logs", args.model_name),
]

pth_path = None
for od in candidate_outdirs:
    # æ”¯æŒ .pth æˆ– .pt æ‰©å±•å
    for fname in ("last.pth", "last.pt"):
        pp = os.path.join(od, fname)
        if os.path.exists(pp):
            pth_path = pp
            outdir = od
            break
    if pth_path is not None:
        break

if pth_path is None:
    tried = "\n  - ".join(candidate_outdirs)
    raise FileNotFoundError(
        f"æœªæ‰¾åˆ° {args.model_name} çš„ last.pth/.ptã€‚å·²å°è¯•ä½ç½®:\n  - {tried}\n"
        f"è¯·ç¡®è®¤è®­ç»ƒè¾“å‡ºç›®å½•æˆ–å°† checkpoint æ”¾ç½®åˆ°ä¸Šè¿°ä»»ä¸€ä½ç½®ã€‚"
    )

print(f"ğŸ”  ä½¿ç”¨æ£€æŸ¥ç‚¹ç›®å½•: {outdir}")
checkpoint = torch.load(pth_path, map_location="cpu", weights_only=False)

# å°è¯•æ‰¾åˆ° state_dict
if "model_state_dict" in checkpoint:
    state_dict = checkpoint["model_state_dict"]
elif "state_dict" in checkpoint:
    state_dict = checkpoint["state_dict"]
else:
    state_dict = checkpoint

# ä¸¥æ ¼æ¨¡å¼å…³é—­ï¼Œå…è®¸å®˜æ–¹æ¨¡å‹é‡Œå¤šä¸€äº›/å°‘ä¸€äº›æ¨¡å—
model.load_state_dict(state_dict, strict=False)
del checkpoint, state_dict

print(f"âœ… æƒé‡åŠ è½½æˆåŠŸ: {pth_path}")

# ==================== å¼€å§‹æå– CLIP å‘é‡ ====================
print(f"\n{'='*60}")
print(f"ğŸš€ å¼€å§‹æå– brain->CLIP å‘é‡")
print(f"{'='*60}\n")

model.eval()
saved_vecs = []
saved_ids = []

with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):
    for uniq_img in tqdm(unique_images, desc="æå– CLIP å‘é‡"):
        # æ‰¾åˆ°è¯¥å›¾åƒçš„æ‰€æœ‰é‡å¤
        locs = np.where(test_images_idx == uniq_img)[0]

        # ç¡®ä¿æœ‰ 3 ä¸ªé‡å¤ï¼ˆMindEye2 çš„æ ‡å‡†åšæ³•ï¼‰
        if len(locs) == 1:
            locs = locs.repeat(3)
        elif len(locs) == 2:
            locs = np.concatenate((locs, locs[:1]))

        # è·å–å¯¹åº”çš„ä½“ç´ æ•°æ®
        voxel = test_voxels[None, locs].to(device)  # [1, 3, num_voxels]

        # å¯¹ 3 ä¸ªé‡å¤æ±‚å¹³å‡
        accum_clip_voxels = None

        for rep in range(3):
            voxel_ridge = model.ridge(voxel[:, [rep]], 0)
            backbone_out, clip_voxels_out, blurry_image_enc_out = model.backbone(voxel_ridge)

            if rep == 0:
                accum_clip_voxels = clip_voxels_out
            else:
                accum_clip_voxels += clip_voxels_out

        # å¹³å‡
        clip_voxels = accum_clip_voxels / 3  # [1, 256, 1664]

        # å¦‚æœæ˜¯åºåˆ—ï¼Œå–å¹³å‡æ± åŒ–å¾—åˆ°å•ä¸ªå‘é‡
        if clip_voxels.dim() == 3:
            vec = clip_voxels.mean(dim=1)  # [1, 1664]
        else:
            vec = clip_voxels

        # è½¬ä¸º CPU å¹¶ä¿å­˜
        vec = vec.squeeze(0).detach().float().cpu()  # [1664]
        saved_vecs.append(vec)
        saved_ids.append(int(uniq_img))

# ==================== ä¿å­˜ç»“æœ ====================
print(f"\n{'='*60}")
print(f"ğŸ’¾ ä¿å­˜ç»“æœ")
print(f"{'='*60}")

# ä¿å­˜ CLIP å‘é‡
V = torch.stack(saved_vecs, dim=0)  # [N, 1664]
torch.save(V, args.clip_out)
print(f"âœ… CLIP å‘é‡å·²ä¿å­˜")
print(f"   - è·¯å¾„: {args.clip_out}")
print(f"   - å½¢çŠ¶: {tuple(V.shape)}")

# ä¿å­˜å›¾åƒ ID
if args.ids_out:
    with open(args.ids_out, "w", encoding="utf-8") as f:
        json.dump(saved_ids, f, indent=2)
    print(f"âœ… å›¾åƒ ID å·²ä¿å­˜")
    print(f"   - è·¯å¾„: {args.ids_out}")
    print(f"   - æ•°é‡: {len(saved_ids)}")
else:
    # é»˜è®¤ä¿å­˜åˆ°ä¸ clip_out åŒç›®å½•
    default_ids_path = args.clip_out.replace(".pt", "_ids.json")
    with open(default_ids_path, "w", encoding="utf-8") as f:
        json.dump(saved_ids, f, indent=2)
    print(f"âœ… å›¾åƒ ID å·²ä¿å­˜ï¼ˆé»˜è®¤è·¯å¾„ï¼‰")
    print(f"   - è·¯å¾„: {default_ids_path}")
    print(f"   - æ•°é‡: {len(saved_ids)}")

print(f"\n{'='*60}")
print(f"ğŸ‰ æå–å®Œæˆï¼")
print(f"{'='*60}\n")

print("ğŸ“Š åç»­ä½¿ç”¨å»ºè®®ï¼š")
print("1. ä½¿ç”¨ brain_clip.pt ä½œä¸º RAG æ£€ç´¢çš„ query å‘é‡")
print("2. æ£€ç´¢ Top-K æœ€ç›¸ä¼¼çš„ COCO å›¾åƒåŠå…¶ captions")
print("3. å°†æ£€ç´¢åˆ°çš„ captions è¾“å…¥ LLM ç”Ÿæˆç»“æ„åŒ–æç¤º")
print("4. ä½¿ç”¨ç”Ÿæˆçš„æç¤º + IP-Adapter è¿›è¡Œ SD1.5/SDXL ç”Ÿæˆ")
