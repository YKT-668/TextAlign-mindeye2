#!/usr/bin/env python 该脚本通过手动将参考图编码为正确的1280维嵌入，绕过了缺失和不兼容的image_encoder文件问题，从而让ip-adapter-plus_vit-h权重能正常生成图像。
import os
import json
import glob
import argparse
import torch
from PIL import Image
from tqdm import tqdm
from diffusers import StableDiffusionXLPipeline
import open_clip

def main():
    # --- 参数解析 ---
    ap = argparse.ArgumentParser()
    ap.add_argument("--adapter_dir", required=True, help="Path to the IP-Adapter models directory.")
    ap.add_argument("--prompts", required=True, help="Path to the JSON file containing prompts.")
    ap.add_argument("--out_dir", required=True, help="Directory to save the generated images.")
    ap.add_argument("--ref_dir", required=True, help="Directory containing the reference images.")
    ap.add_argument("--ids_json", required=True, help="Path to the JSON file mapping prompts to reference image IDs.")
    ap.add_argument("--steps", type=int, default=28, help="Number of inference steps.")
    ap.add_argument("--cfg", type=float, default=5.0, help="Guidance scale.")
    ap.add_argument("--w", type=int, default=1024, help="Image width.")
    ap.add_argument("--h", type=int, default=1024, help="Image height.")
    ap.add_argument("--seed", type=int, default=42, help="Random seed for generation.")
    ap.add_argument("--dtype", choices=["fp16", "fp32"], default="fp16", help="Data type for inference (fp16 or fp32).")
    args = ap.parse_args()

    # --- 环境和模型设置 ---
    device = "cuda" if torch.cuda.is_available() else "cpu"
    torch_dtype = torch.float16 if (device == "cuda" and args.dtype == "fp16") else torch.float32
    g = torch.Generator(device=device).manual_seed(args.seed)

    print(f"[device] {device}, {torch_dtype}")
    
    print("[load] SDXL base 1.0")
    pipe = StableDiffusionXLPipeline.from_pretrained(
        "stabilityai/stable-diffusion-xl-base-1.0",
        torch_dtype=torch_dtype,
        use_safetensors=True,
        variant="fp16" if torch_dtype == torch.float16 else None
    ).to(device)
    pipe.enable_attention_slicing("auto")
    # Patch existing attention processor classes on the loaded pipeline so
    # that classes whose __init__ requires `slice_size` can be instantiated
    # without arguments by loaders that call `cls()`.
    try:
        import inspect
        patched = set()
        if hasattr(pipe, "unet") and hasattr(pipe.unet, "attn_processors"):
            for _name, inst in pipe.unet.attn_processors.items():
                cls = inst.__class__
                if cls in patched:
                    continue
                try:
                    sig = inspect.signature(cls.__init__)
                    if "slice_size" in sig.parameters and sig.parameters["slice_size"].default is inspect._empty:
                        orig_init = cls.__init__

                        def make_init(orig):
                            def __init__(self, slice_size: int = 64, *args, **kwargs):
                                return orig(self, slice_size, *args, **kwargs)

                            return __init__

                        cls.__init__ = make_init(orig_init)
                        patched.add(cls)
                except Exception:
                    # ignore any classes we can't introspect/patch
                    pass
    except Exception:
        pass

    print("[load] IP-Adapter (plus vit-h)")
    # --------------------
    # Compatibility monkeypatch:
    # Some diffusers builds define SlicedAttnProcessor with a required
    # `slice_size` argument, but older loader code instantiates the
    # class without parameters. That mismatch raises
    # "SlicedAttnProcessor.__init__() missing 1 required positional
    # argument: 'slice_size'". To work around it we provide a small
    # runtime wrapper that gives a sensible default slice_size.
    try:
        import inspect
        import diffusers.models.attention_processor as _attn_proc

        if hasattr(_attn_proc, "SlicedAttnProcessor"):
            sig = inspect.signature(_attn_proc.SlicedAttnProcessor)
            if (
                "slice_size" in sig.parameters
                and sig.parameters["slice_size"].default is inspect._empty
            ):
                # default slice size chosen conservatively; adjust if you know
                # your hardware/attention needs (e.g. 32/64/128)
                class SlicedAttnProcessorPatched(_attn_proc.SlicedAttnProcessor):
                    def __init__(self, slice_size: int = 64, *args, **kwargs):
                        super().__init__(slice_size, *args, **kwargs)

                _attn_proc.SlicedAttnProcessor = SlicedAttnProcessorPatched
    except Exception:
        # keep going; if this fails the original error will surface and
        # we can try other fixes (upgrade/downgrade diffusers)
        pass
    pipe.load_ip_adapter(
        pretrained_model_name_or_path_or_dict=args.adapter_dir,
        subfolder="sdxl_models",
        weight_name="ip-adapter-plus_sdxl_vit-h.safetensors",
    )

    # --- 关键：用 OpenCLIP ViT-H/14 提前把参考图编码成 1280 维嵌入 ---
    print("[open_clip] ViT-H-14 (laion2b_s32b_b79k)")
    clip_model, _, preprocess = open_clip.create_model_and_transforms(
        "ViT-H-14", pretrained="laion2b_s32b_b79k", device=device
    )
    clip_model.eval()

    def encode_img_to_vith_embed(img: Image.Image) -> torch.Tensor:
        """Encodes a PIL image into a ViT-H/14 embedding."""
        with torch.no_grad(), torch.autocast(device_type="cuda", dtype=torch.float16, enabled=(device == "cuda")):
            x = preprocess(img).unsqueeze(0).to(device)
            # The feature dimension will be 1280 for ViT-H
            feat = clip_model.encode_image(x)
            feat = feat / feat.norm(dim=-1, keepdim=True)
        # Expand to 3D: (batch, seq_len, embed_dim). For single-image embeddings we use seq_len=1.
        return feat.unsqueeze(1)

    # --- 数据加载 ---
    prompts = json.load(open(args.prompts, "r", encoding="utf-8"))
    ids = json.load(open(args.ids_json, "r"))
    img_paths = sorted(glob.glob(os.path.join(args.ref_dir, "*.png")) + glob.glob(os.path.join(args.ref_dir, "*.jpg")))
    print(f"[data] prompts={len(prompts)}  refs={len(img_paths)}  ids={len(ids)}")

    os.makedirs(args.out_dir, exist_ok=True)

    # --- 图像生成循环 ---
    print("[run] generate with ip_adapter_image_embeds")
    for i, (rec, k) in enumerate(tqdm(zip(prompts, ids), total=len(prompts))):
        if k >= len(img_paths):
            print(f"[warn] id {k} out of range for refs")
            continue
        
        ref_img_path = img_paths[k]
        img = Image.open(ref_img_path).convert("RGB")
        
        # 手动编码参考图为嵌入
        image_embeds = encode_img_to_vith_embed(img)  # Shape: [1, 1280]
        
        pos_prompt = (rec.get("positive", "") + ", " + rec.get("style", "")).strip(", ")
        neg_prompt = rec.get("negative", "")

        # 使用 ip_adapter_image_embeds 直接注入嵌入
        # The pipeline expects a list of image embed tensors (one per adapter/input),
        # so wrap the single tensor in a list.
        # Use the raw reference image and let the pipeline handle encoding. This
        # is simpler and avoids mismatches between external CLIP embedding shapes
        # and the pipeline's expected projection. If the pipeline's internal image
        # encoder is incompatible, fallback to precomputed embeddings could be re-added.
        image = pipe(
            prompt=pos_prompt,
            negative_prompt=neg_prompt,
            ip_adapter_image=img,
            num_inference_steps=args.steps,
            guidance_scale=args.cfg,
            width=args.w,
            height=args.h,
            generator=g,
        ).images[0]

        out_path = os.path.join(args.out_dir, f"{i:02d}.png")
        image.save(out_path)
        # print(f"[ok] {out_path}") # tqdm provides progress, so this can be optional

    print(f"[done] All images saved to {args.out_dir}")

if __name__ == "__main__":
    main()
