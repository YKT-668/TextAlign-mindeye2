#!/usr/bin/env python
import os, json, torch, argparse
import open_clip
from tqdm import tqdm

def main():
    ap = argparse.ArgumentParser(description="Retrieve top-K texts for brain vectors from a caption library using a specified CLIP model.")
    ap.add_argument("--brain_vec_pt", required=True, help="Path to the brain vectors .pt file.")
    ap.add_argument("--ids_json", required=True, help="Path to the JSON file containing sample IDs.")
    ap.add_argument("--captions_pt", required=True, help="Path to the all_captions.pt file.")
    ap.add_argument("--out_jsonl", required=True, help="Path to save the output .jsonl file.")
    ap.add_argument("--clip_model", type=str, default="ViT-bigG-14", help="Name of the CLIP model to use for encoding (e.g., ViT-bigG-14).")
    ap.add_argument("--clip_pretrained", type=str, default="laion2b_s39b_b160k", help="Name of the pretrained weights for the CLIP model.")
    ap.add_argument("--topk", type=int, default=8, help="Number of top texts to retrieve.")
    ap.add_argument("--batch_size", type=int, default=256, help="Batch size for text encoding.")
    ap.add_argument("--device", default="cuda", help="Device to run on.")
    args = ap.parse_args()

    print("Loading data...")
    V = torch.load(args.brain_vec_pt, map_location="cpu").float()
    ids = json.load(open(args.ids_json))
    try:
        caps = torch.load(args.captions_pt, map_location="cpu", weights_only=False)
    except Exception:
        caps = torch.load(args.captions_pt, map_location="cpu")

    if isinstance(caps[0], (list, tuple)):
        caps = [c[0] for c in caps]

    print(f"Loaded {V.shape[0]} brain vectors (dim={V.shape[1]}) and {len(caps)} captions.")

    print(f"Loading CLIP model: {args.clip_model} ({args.clip_pretrained})")
    
    # å¯¹äº ViT-bigG-14ï¼Œä½¿ç”¨ open_clip å¹¶æ˜ç¡®æŒ‡å®šä¸‹è½½æº
    if args.clip_model == "ViT-bigG-14":
        try:
            print("Loading ViT-bigG-14 via open_clip with proper pretrained weights...")
            # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æƒé‡åç§°æ ¼å¼
            model = open_clip.create_model(args.clip_model, pretrained=args.clip_pretrained)
            model = model.to(args.device)
            model.eval()
            tok = open_clip.get_tokenizer(args.clip_model)
            print(f"âœ“ Successfully loaded {args.clip_model}")
        except Exception as e:
            print(f"Failed with open_clip: {e}")
            print("Attempting manual download from open_clip repository...")
            
            # æ‰‹åŠ¨ä» open_clip çš„æƒé‡ä»“åº“åŠ è½½
            import urllib.request
            import tempfile
            
            weights_url = "https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k/resolve/main/open_clip_pytorch_model.bin"
            print(f"Downloading weights from: {weights_url}")
            
            with tempfile.NamedTemporaryFile(delete=False, suffix=".bin") as tmp_file:
                urllib.request.urlretrieve(weights_url, tmp_file.name)
                
                # åˆ›å»ºæ¨¡å‹æ¶æ„
                model = open_clip.create_model(args.clip_model, pretrained=False)
                
                # åŠ è½½æƒé‡
                state_dict = torch.load(tmp_file.name, map_location="cpu")
                model.load_state_dict(state_dict)
                model = model.to(args.device)
                model.eval()
                
                print("âœ“ Successfully loaded from manual download")
            
            tok = open_clip.get_tokenizer(args.clip_model)
    else:
        # å…¶ä»–æ¨¡å‹ä½¿ç”¨æ ‡å‡†æ–¹æ³•
        model, _, preprocess = open_clip.create_model_and_transforms(
            args.clip_model, 
            pretrained=args.clip_pretrained, 
            device=args.device
        )
        model.eval()
        tok = open_clip.get_tokenizer(args.clip_model)
    
    # éªŒè¯æ¨¡å‹ç»´åº¦
    with torch.no_grad():
        dummy_text = tok(["test"]).to(args.device)
        dummy_embed = model.encode_text(dummy_text)
        text_dim = dummy_embed.shape[-1]
    
    print(f"Model text embedding dimension: {text_dim}")
    
    # å¤„ç†ç»´åº¦ä¸åŒ¹é…çš„æƒ…å†µ
    if V.shape[1] != text_dim:
        print(f"\nâš ï¸  Dimension mismatch detected!")
        print(f"   Brain vectors: {V.shape[1]} dim")
        print(f"   Text encoder:  {text_dim} dim")
        
        if V.shape[1] == 1664 and text_dim == 1280 and args.clip_model == "ViT-bigG-14":
            print(f"\nğŸ’¡ Detected MindEye brain predictions (1664 dim)")
            print(f"   These are ViT-bigG penultimate layer features (before projection)")
            print(f"\nğŸ”„ Applying visual projection to align with text space (1280 dim)...")
            
            # è·å– visual projection å±‚
            if hasattr(model.visual, 'proj') and model.visual.proj is not None:
                proj_matrix = model.visual.proj.cpu()
                print(f"   Visual projection matrix shape: {proj_matrix.shape}")
                
                # ViT-bigG projection: (1664, 1280)
                if proj_matrix.shape == torch.Size([1664, 1280]):
                    V_projected = V @ proj_matrix  # [N, 1664] @ [1664, 1280] = [N, 1280]
                elif proj_matrix.shape == torch.Size([1280, 1664]):
                    V_projected = V @ proj_matrix.T
                else:
                    raise ValueError(f"Unexpected projection shape: {proj_matrix.shape}")
                
                # å½’ä¸€åŒ–ï¼ˆä¸ CLIP çš„è¾“å‡ºä¿æŒä¸€è‡´ï¼‰
                V_projected = V_projected / V_projected.norm(dim=-1, keepdim=True)
                V = V_projected
                print(f"   âœ“ Successfully projected to {V.shape[1]} dim")
                print(f"   âœ“ Applied L2 normalization")
            else:
                print(f"   âœ— ERROR: Visual projection layer not found in model")
                print(f"   This shouldn't happen with ViT-bigG-14")
                raise ValueError("Missing visual projection layer")
                
        else:
            print(f"\nExpected dimension combinations:")
            print(f"   1664 dim (brain) + 1280 dim (text) â†’ MindEye predictions for ViT-bigG-14")
            print(f"   1280 dim (brain) + 1280 dim (text) â†’ Direct ViT-bigG or ViT-H features")
            print(f"   1024 dim (brain) + 1024 dim (text) â†’ ViT-L-14 features")
            raise ValueError(f"Cannot resolve dimension mismatch: {V.shape[1]} vs {text_dim}")
    
    print(f"\nâœ“ Brain vectors and text encoder dimensions aligned: {V.shape[1]} dim")
    
    tok = open_clip.get_tokenizer(args.clip_model)

    print("Encoding text library...")
    with torch.no_grad(), torch.amp.autocast('cuda', enabled=(args.device=="cuda")):
        T_embs = []
        for i in tqdm(range(0, len(caps), args.batch_size), desc="Encoding Captions"):
            batch_texts = caps[i:i+args.batch_size]
            batch_tok = tok(batch_texts).to(args.device)
            batch_T = model.encode_text(batch_tok)
            batch_T = batch_T / batch_T.norm(dim=-1, keepdim=True)
            T_embs.append(batch_T.cpu())
        T = torch.cat(T_embs, 0)
    
    print(f"Text embeddings shape: {T.shape}")
    
    print("Calculating similarities and retrieving top-K...")
    Vn = V / V.norm(dim=-1, keepdim=True)
    S = Vn @ T.T

    output_records = []
    for i in tqdm(range(V.shape[0]), desc="Retrieving"):
        if i < len(ids):
            top_indices = S[i].topk(k=min(args.topk, S.shape[1])).indices.tolist()
            output_records.append({
                "id": int(ids[i]),
                "topk": [caps[j] for j in top_indices]
            })

    print(f"Saving {len(output_records)} records to {args.out_jsonl}...")
    with open(args.out_jsonl, "w", encoding="utf-8") as f:
        for record in output_records:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
            
    print("Done.")

if __name__ == "__main__":
    main()