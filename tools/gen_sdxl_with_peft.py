#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
gen_sdxl_with_peft.py

åŠŸèƒ½:
  ä¸€ä¸ªé›†å¤§æˆçš„SDXLç”Ÿæˆè„šæœ¬ï¼Œèžåˆäº†ä¸‰å¤§æ ¸å¿ƒè¾“å…¥ï¼š
  1. æ¥è‡ªå¤§è„‘çš„ç‰¹å¾å‘é‡ (é€šè¿‡ IP-Adapter æ³¨å…¥)
  2. æ¥è‡ªLLMçš„ç»“æž„åŒ–æ–‡æœ¬æç¤º
  3. (å¯é€‰) é’ˆå¯¹ç‰¹å®šè¢«è¯•çš„ä¸ªæ€§åŒ–PEFTé€‚é…å™¨ (Soft-Prompt å’Œ/æˆ– Text-Encoder LoRA)

å·¥ä½œæµç¨‹:
  - åŠ è½½ SDXL åŸºç¡€æ¨¡åž‹å’Œ IP-Adapter (Plus, ViT-H)ã€‚
  - (å¯é€‰) åŠ è½½å¹¶åº”ç”¨é’ˆå¯¹æ–‡æœ¬ç¼–ç å™¨çš„ LoRA æƒé‡ã€‚
  - åŠ è½½å¤§è„‘è§£ç å‡ºçš„ç‰¹å¾å‘é‡ (e.g., 1664D) å¹¶å°†å…¶æŠ•å½±åˆ° IP-Adapter æ‰€éœ€çš„ 1280D ç©ºé—´ã€‚
  - åœ¨ç”Ÿæˆå¾ªçŽ¯ä¸­ï¼Œä¸ºæ¯ä¸ªæ ·æœ¬ï¼š
    - (å¯é€‰) å°† Soft-Prompt åµŒå…¥ä¸Žæ–‡æœ¬æç¤ºçš„åµŒå…¥è¿›è¡Œæ‹¼æŽ¥ã€‚
    - å°†å¤§è„‘ç‰¹å¾å‘é‡ä½œä¸ºå›¾åƒåµŒå…¥ä¼ é€’ç»™ IP-Adapterã€‚
    - æ‰§è¡Œæ‰©æ•£è¿‡ç¨‹ï¼Œç”Ÿæˆæœ€ç»ˆå›¾åƒã€‚
"""

import os, json, argparse, math, gc
from typing import List, Dict, Any, Optional, Tuple

import torch
from torch import nn
from PIL import Image
from diffusers import StableDiffusionXLPipeline
# We'll apply a compatibility shim for SlicedAttnProcessor if needed (see below)
from transformers import AutoTokenizer

# -------------------------- PEFT/Soft-Prompt ç›¸å…³ç»„ä»¶ --------------------------
try:
    from peft import PeftModel
    _has_peft = True
except ImportError:
    _has_peft = False

class SoftPrompt(nn.Module):
    def __init__(self, n_tokens: int, hidden_size: int):
        super().__init__()
        self.embeds = nn.Parameter(torch.zeros(n_tokens, hidden_size))
    def load(self, path: str, map_location='cpu'):
        sd = torch.load(path, map_location=map_location)
        self.load_state_dict(sd)
    def forward(self):
        return self.embeds

def build_prompt_embeds_with_peft(
    pipe: StableDiffusionXLPipeline,
    prompt: str,
    negative_prompt: str,
    soft_prompt: Optional[SoftPrompt] = None,
    device: torch.device = torch.device("cuda")
):
    """ä¸ºSDXLæž„å»ºæ–‡æœ¬åµŒå…¥ï¼Œå¹¶å¯é€‰åœ°æ‹¼æŽ¥Soft-Promptã€‚"""
    # SDXL ä½¿ç”¨ä¸¤ä¸ªæ–‡æœ¬ç¼–ç å™¨
    text_encoder_one = pipe.text_encoder
    text_encoder_two = pipe.text_encoder_2
    tokenizer_one = pipe.tokenizer
    tokenizer_two = pipe.tokenizer_2

    tokenizers = [tokenizer_one, tokenizer_two]
    text_encoders = [text_encoder_one, text_encoder_two]
    
    prompt_embeds_list = []
    
    # èŽ·å–æ­£é¢æç¤ºçš„åµŒå…¥
    for tokenizer, text_encoder in zip(tokenizers, text_encoders):
        text_inputs = tokenizer(
            prompt,
            padding="max_length",
            max_length=tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt",
        )
        text_input_ids = text_inputs.input_ids
        
        prompt_embeds = text_encoder(text_input_ids.to(device), output_hidden_states=True)
        
        if soft_prompt is not None:
            if soft_prompt.embeds.shape[1] == prompt_embeds.hidden_states[-1].shape[-1]:
                soft_embeds = soft_prompt().to(device).unsqueeze(0)
                prompt_embeds.hidden_states = list(prompt_embeds.hidden_states)
                prompt_embeds.hidden_states[-1] = torch.cat([soft_embeds, prompt_embeds.hidden_states[-1]], dim=1)
        
        prompt_embeds_list.append(prompt_embeds[0])

    prompt_embeds = torch.cat(prompt_embeds_list, dim=-1)
    
    # èŽ·å–è´Ÿé¢æç¤ºçš„åµŒå…¥ (é€šå¸¸ä¸åŠ soft-prompt)
    negative_prompt_embeds, pooled_negative_prompt_embeds = pipe.encode_prompt(
        prompt=negative_prompt, device=device, num_images_per_prompt=1, do_classifier_free_guidance=False
    )
    
    # ä»Žæ­£é¢æç¤ºä¸­æå– pooled embedding (ä½¿ç”¨ç¬¬äºŒä¸ªç¼–ç å™¨çš„[CLS] token)
    pooled_prompt_embeds = prompt_embeds_list[1][:, 0]

    return prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, pooled_negative_prompt_embeds


# -------------------------- ä¸»é€»è¾‘ --------------------------
def main():
    AP = argparse.ArgumentParser(description="SDXL + IP-Adapter + PEFT ç”Ÿæˆè„šæœ¬ (æ˜¾å­˜ä¼˜åŒ–ç‰ˆ)")
    AP.add_argument("--adapter_dir", required=True, help="IP-Adapter local root")
    AP.add_argument("--prompts", required=True, help="Path to prompts json")
    AP.add_argument("--brain_vec_pt", required=True, help="Path to brain->CLIP vectors [N,D]")
    AP.add_argument("--proj_pt", default="", help="Optional projection ckpt for 1664->1024 mapping")
    AP.add_argument("--peft_adapter_dir", default="", help="[æ–°å¢ž] æŒ‡å‘åŒ…å«soft_tokens.ptå’Œ/æˆ–peft_text_loraçš„ç›®å½•")
    AP.add_argument("--out_dir", required=True)
    AP.add_argument("--steps", type=int, default=28)
    AP.add_argument("--cfg", type=float, default=5.0)
    AP.add_argument("--w", type=int, default=1024)
    AP.add_argument("--h", type=int, default=1024)
    AP.add_argument("--seed", type=int, default=42)
    AP.add_argument("--dtype", choices=["fp16","fp32","bf16"], default="fp16")
    AP.add_argument("--ip_scale", type=float, default=0.8)
    AP.add_argument("--enable_cpu_offload", action="store_true", help="å¯ç”¨CPU Offloadç­‰ä¸€ç³»åˆ—æ˜¾å­˜ä¼˜åŒ–æŽªæ–½ï¼Œä¼šé™ä½Žé€Ÿåº¦ã€‚")
    AP.add_argument("--limit", type=int, default=0, help="å¯é€‰ï¼šåªå¤„ç†å‰Nä¸ªæ ·æœ¬ï¼Œç”¨äºŽå¿«é€Ÿæµ‹è¯•ã€‚") 
    args = AP.parse_args()

    # --- è®¾å¤‡ / ç²¾åº¦ ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    dtype_map = {"fp16": torch.float16, "bf16": torch.bfloat16, "fp32": torch.float32}
    torch_dtype = dtype_map[args.dtype]
    print(f"[device] {device.type}, dtype={torch_dtype}")
    torch.manual_seed(args.seed)

    # --- åŠ è½½ SDXL + IP-Adapter ---
    print("[load] SDXL base 1.0")
    pipe = StableDiffusionXLPipeline.from_pretrained(
        "stabilityai/stable-diffusion-xl-base-1.0",
        torch_dtype=torch_dtype,
        use_safetensors=True,
        variant="fp16" if torch_dtype==torch.float16 else None,
    )
    
    if args.enable_cpu_offload and device.type == 'cuda':
        print("ðŸŸ¡ å¯ç”¨æ˜¾å­˜ä¼˜åŒ–: Model CPU Offloading, VAE Slicing, Sequential Offloading...")
        pipe.enable_model_cpu_offload()
        pipe.enable_sequential_cpu_offload()
    else:
        pipe = pipe.to(device)

    # å…ˆä»…å¯ç”¨ VAE slicingï¼Œæ³¨æ„åŠ›åˆ‡ç‰‡æŽ¨è¿Ÿåˆ°åŠ è½½ IP-Adapter ä¹‹åŽ
    pipe.enable_vae_slicing()

    # å…¼å®¹æ€§è¡¥ä¸ï¼šä¸º diffusers ä¸­å¯èƒ½ç¼ºå°‘é»˜è®¤ slice_size çš„ SlicedAttnProcessor æä¾›å‚æ•°
    def apply_sliced_attn_processor_patch(pipe=None):
        try:
            import inspect
            import diffusers.models.attention_processor as attn_proc_modules

            def _patch_class(cls):
                try:
                    sig = inspect.signature(cls.__init__)
                    if "slice_size" in sig.parameters and sig.parameters["slice_size"].default is inspect._empty:
                        orig_init = cls.__init__

                        def __init__(self, slice_size: int = 64, *args, **kwargs):
                            return orig_init(self, slice_size, *args, **kwargs)

                        cls.__init__ = __init__
                        return True
                except Exception:
                    pass
                return False

            patched = False
            if hasattr(attn_proc_modules, "SlicedAttnProcessor"):
                patched = _patch_class(attn_proc_modules.SlicedAttnProcessor) or patched

            # å¦‚æžœä¼ å…¥äº† pipeï¼Œåˆ™å°è¯•å¯¹ UNet ä¸­å®žé™…ä½¿ç”¨çš„ç±»è¿›è¡Œè¡¥ä¸
            if pipe is not None and hasattr(pipe, "unet") and hasattr(pipe.unet, "attn_processors"):
                for v in pipe.unet.attn_processors.values():
                    cls = v.__class__
                    if cls.__name__ == "SlicedAttnProcessor":
                        patched = _patch_class(cls) or patched

            if patched:
                print("âœ“ Applied SlicedAttnProcessor patch successfully.")
        except Exception as e:
            print(f"âš ï¸  Could not apply SlicedAttnProcessor patch: {e}.")

    apply_sliced_attn_processor_patch()

    print("[load] IP-Adapter (plus vit-h)")
    try:
        pipe.load_ip_adapter(
            "h94/IP-Adapter",
            subfolder="sdxl_models",
            weight_name="ip-adapter-plus_sdxl_vit-h.safetensors",
        )
    except TypeError as e:
        # å¦‚æžœåŠ è½½å¤±è´¥ï¼Œå°è¯•é’ˆå¯¹ pipe.unet çš„å­ç±»åšè¡¥ä¸åŽé‡è¯•
        print(f"[warn] load_ip_adapter å¤±è´¥({e}), å°è¯•åº”ç”¨ SlicedAttnProcessor è¡¥ä¸å¹¶é‡è¯•...")
        apply_sliced_attn_processor_patch(pipe)
        pipe.load_ip_adapter(
            "h94/IP-Adapter",
            subfolder="sdxl_models",
            weight_name="ip-adapter-plus_sdxl_vit-h.safetensors",
        )
    pipe.set_ip_adapter_scale(args.ip_scale)
    # æ³¨æ„ï¼šä¸è¦åœ¨åŠ è½½ IP-Adapter ä¹‹åŽè¦†ç›– UNet çš„ attention processorï¼Œä¹Ÿä¸è¦æ­¤æ—¶å¯ç”¨ attention slicingï¼Œ
    # ä»¥å…æŠŠ IP-Adapter å®‰è£…çš„è‡ªå®šä¹‰ processor è¦†ç›–æŽ‰ï¼ˆä¼šå¯¼è‡´ encoder_hidden_states ä¼ å…¥ tuple æ—¶å‡ºé”™ï¼‰ã€‚

    # --- åŠ è½½ PEFT é€‚é…å™¨ ---
    soft_prompt_instance = None
    soft_prompt_te1 = None  # embeds for text_encoder (dim ~1280)
    soft_prompt_te2 = None  # embeds for text_encoder_2 (dim ~768)
    if args.peft_adapter_dir and os.path.isdir(args.peft_adapter_dir):
        print(f"[peft] æ­£åœ¨ä»Ž {args.peft_adapter_dir} åŠ è½½é€‚é…å™¨...")
        
        lora_dir = os.path.join(args.peft_adapter_dir, 'peft_text_lora')
        if _has_peft and os.path.isdir(lora_dir):
            try:
                print("  - æ­£åœ¨åŠ è½½ Text-Encoder LoRA...")
                pipe.load_lora_weights(lora_dir)
                print("  âœ“ æˆåŠŸåŠ è½½ LoRA æƒé‡åˆ°ç®¡çº¿")
            except Exception as e:
                print(f"  âœ— åŠ è½½LoRAå¤±è´¥: {e}")

        soft_prompt_path = os.path.join(args.peft_adapter_dir, 'soft_tokens.pt')
        if os.path.isfile(soft_prompt_path):
            print("  - æ­£åœ¨åŠ è½½ Soft-Prompt...")
            sd = torch.load(soft_prompt_path, map_location='cpu')
            # å…¼å®¹å¤šç§ä¿å­˜æ ¼å¼ï¼š
            # - {'embeds': (n,dim)}
            # - {'embeds_te1': (n,1280), 'embeds_te2': (n,768)}
            if 'embeds_te1' in sd or 'embeds_te2' in sd:
                if 'embeds_te1' in sd:
                    E1 = torch.as_tensor(sd['embeds_te1'])
                    soft_prompt_te1 = nn.Parameter(E1.clone().detach())
                    print(f"  âœ“ Soft-Prompt(te1) tokens={E1.shape[0]}, dim={E1.shape[1]}")
                if 'embeds_te2' in sd:
                    E2 = torch.as_tensor(sd['embeds_te2'])
                    soft_prompt_te2 = nn.Parameter(E2.clone().detach())
                    print(f"  âœ“ Soft-Prompt(te2) tokens={E2.shape[0]}, dim={E2.shape[1]}")
                # ä¸ºäº†å¤ç”¨åŽŸæœ‰ SoftPrompt ç»“æž„ï¼Œè‹¥æŸä¸€ä¾§æœªæä¾›ï¼Œåˆ™ç½®ä¸ºç©º
                if soft_prompt_te1 is None and soft_prompt_te2 is None:
                    print("  âœ— Soft-Prompt æ–‡ä»¶æœªåŒ…å«æœ‰æ•ˆé”®ï¼Œå·²è·³è¿‡ã€‚")
                else:
                    # æ ‡è®°å­˜åœ¨ soft prompt
                    soft_prompt_instance = object()
            elif 'embeds' in sd:
                embeds = torch.as_tensor(sd['embeds'])
                n_tokens, hidden_size = embeds.shape
                # æš‚å­˜ä¸ºé€šç”¨ soft promptï¼ŒåŽç»­æ ¹æ® encoder ç»´åº¦åŒ¹é…åˆ†é…
                soft_prompt_instance = SoftPrompt(n_tokens, hidden_size)
                soft_prompt_instance.load_state_dict({'embeds': embeds})
                soft_prompt_instance = soft_prompt_instance.to(device)
                print(f"  âœ“ æˆåŠŸåŠ è½½ Soft-Prompt (tokens={n_tokens}, dim={hidden_size})")
            else:
                print("  âœ— Soft-Prompt æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œå·²è·³è¿‡ã€‚")

    # --- åŠ è½½å¹¶æŠ•å½±å¤§è„‘å‘é‡ ---
    print("[load] brain vectors")
    V = torch.load(args.brain_vec_pt, map_location="cpu").float()
    N, D = V.shape
    print(f"  brain_vec shape: ({N}, {D})")

    # æŠ•å½±é€»è¾‘
    if D == 1280:
        E1280 = V
    else:
        # éœ€è¦æŠ•å½±
        if D == 1664:
            if not args.proj_pt: raise ValueError("Brain vec is 1664D, requires --proj_pt for 1664->1024 mapping.")
            W_1664_to_1024 = torch.load(args.proj_pt, map_location="cpu")['W'].float()
            V = V @ W_1664_to_1024
            print(f"  âœ“ Projected brain vec from 1664D to {V.shape[1]}D")
        
        if V.shape[1] == 1024:
            import open_clip
            oc_model, _, _ = open_clip.create_model_and_transforms("ViT-H-14", pretrained="laion2b_s32b_b79k")
            W_1024_to_1280 = oc_model.visual.proj.float()
            E1280 = V @ W_1024_to_1280.T
            print(f"  âœ“ Projected brain vec from 1024D to 1280D")
        else:
            raise ValueError(f"Unsupported intermediate brain_vec dim: {V.shape[1]}")

    # SDXL IP-Adapter æœŸæœ›çš„ image_embeds ä¸º [B, 1280]ï¼ˆåŽç»­å†…éƒ¨ä¼šå¤„ç†ä¸ºéœ€è¦çš„å½¢çŠ¶ï¼‰
    E_tokens = E1280.to(device=device, dtype=torch_dtype)
    print(f"[embed] Final (N,1280) = {tuple(E1280.shape)}")

    # --- åŠ è½½Prompts ---
    with open(args.prompts, "r", encoding="utf-8") as f:
        raw_prompts = json.load(f)

    # --- ç”Ÿæˆå¾ªçŽ¯ ---
    os.makedirs(args.out_dir, exist_ok=True)
    B = E_tokens.shape[0]
    M = len(raw_prompts)
    T = min(B, M)
    if args.limit > 0:
        print(f"ðŸŸ¡ åº”ç”¨é™åˆ¶: å°†åªå¤„ç†å‰ {args.limit} ä¸ªæ ·æœ¬ã€‚")
        T = min(T, args.limit)
    print(f"[run] brain_vec={B}, prompts={M} -> å°†ç”Ÿæˆ {T} å¼ å›¾åƒ")


    # é¢„æ£€æµ‹ä¸¤ä¾§ text-encoder çš„éšå±‚ç»´åº¦ä¸Žæœ€å¤§é•¿åº¦
    te1_hidden = None
    te2_hidden = None
    max_len_te1 = pipe.tokenizer.model_max_length if hasattr(pipe, 'tokenizer') else 77
    max_len_te2 = pipe.tokenizer_2.model_max_length if hasattr(pipe, 'tokenizer_2') else 77
    try:
        with torch.no_grad():
            ids1 = pipe.tokenizer([" "], padding="max_length", max_length=max_len_te1, return_tensors="pt").input_ids.to(device)
            out1 = pipe.text_encoder(ids1)
            te1_hidden = out1.last_hidden_state.shape[-1]
            ids2 = pipe.tokenizer_2([" "], padding="max_length", max_length=max_len_te2, return_tensors="pt").input_ids.to(device)
            out2 = pipe.text_encoder_2(ids2)
            te2_hidden = out2.last_hidden_state.shape[-1]
            print(f"[te] dims: te1={te1_hidden}, te2={te2_hidden}, max_len: te1={max_len_te1}, te2={max_len_te2}")
    except Exception as e:
        print(f"[warn] è¯»å– text-encoder ç»´åº¦å¤±è´¥: {e}")

    def build_soft_prompt_embeds(pos: str, neg: str):
        """è¿”å›ž dictï¼ŒåŒ…å« prompt/negative çš„ token ä¸Ž pooledã€‚
        æ­£æ ·æœ¬çš„ token-embeds ä¼šæ³¨å…¥ soft-promptï¼›pooled ç›´æŽ¥å¤ç”¨ç®¡çº¿ encode_prompt çš„ç»“æžœã€‚
        """
        # å…ˆç”¨ç®¡çº¿èŽ·å–æ ‡å‡† embeddingï¼ˆä¾›è´Ÿæ ·æœ¬ä¸Ž pooled ä½¿ç”¨ï¼‰
        enc_pos = pipe.encode_prompt(prompt=pos, device=device, num_images_per_prompt=1, do_classifier_free_guidance=False)
        enc_neg = pipe.encode_prompt(prompt=neg, device=device, num_images_per_prompt=1, do_classifier_free_guidance=False)

        def pick(enc, idx):
            if isinstance(enc, torch.Tensor):
                return enc
            if isinstance(enc, (list, tuple)) and len(enc) > idx and isinstance(enc[idx], torch.Tensor):
                return enc[idx]
            return None

        base_pos_tokens = pick(enc_pos, 0)
        base_pos_pooled = pick(enc_pos, 2)
        base_neg_tokens = pick(enc_neg, 0)
        base_neg_pooled = pick(enc_neg, 2)

        if base_pos_tokens is None or base_neg_tokens is None:
            # å›žé€€ï¼šè®©ç®¡çº¿è‡ªå·±ç¼–ç 
            return {
                'prompt': pos,
                'negative_prompt': neg,
            }

        B, L, Dtot = base_pos_tokens.shape  # Dtot åº”ä¸º te1_hidden+te2_hidden
        # åˆ†æ‹†ä¸¤ä¾§ hiddenï¼ˆè‹¥æ— æ³•ç¡®å®šï¼Œåˆ™é»˜è®¤ te2_hidden å·²è¯»å‡ºï¼‰
        d1 = te1_hidden or (Dtot - (te2_hidden or 0))
        d2 = Dtot - d1
        pos_te1 = base_pos_tokens[:, :, :d1]
        pos_te2 = base_pos_tokens[:, :, d1:]

        # æ ¹æ® soft prompt ç»´åº¦åŒ¹é…æ‹¼æŽ¥ï¼ˆæˆªæ–­åˆ°å„è‡ªæœ€å¤§é•¿åº¦ï¼‰
        def apply_soft(hs: torch.Tensor, sp_param: Optional[nn.Parameter], max_len: int) -> torch.Tensor:
            if sp_param is None:
                # è‹¥æä¾›äº†é€šç”¨ SoftPromptï¼ˆsoft_prompt_instance æ˜¯ SoftPrompt å®žä¾‹ï¼‰ï¼Œä¸”ç»´åº¦åŒ¹é…åˆ™ä½¿ç”¨
                if isinstance(soft_prompt_instance, SoftPrompt) and soft_prompt_instance.embeds.shape[1] == hs.shape[-1]:
                    sp = soft_prompt_instance().to(device=device, dtype=hs.dtype)  # (n,dim)
                else:
                    return hs
            else:
                sp = sp_param.to(device=device, dtype=hs.dtype)  # (n,dim)
            sp = sp.unsqueeze(0).expand(hs.shape[0], -1, -1)  # (B,n,dim)
            new_hs = torch.cat([sp, hs], dim=1)
            if new_hs.shape[1] > max_len:
                new_hs = new_hs[:, :max_len, :]
            return new_hs

        pos_te1_new = apply_soft(pos_te1, soft_prompt_te1, max_len_te1)
        pos_te2_new = apply_soft(pos_te2, soft_prompt_te2, max_len_te2)

        # è‹¥ä¸¤ä¾§é•¿åº¦ä¸åŒï¼ŒæŒ‰è¾ƒå°é•¿åº¦å¯¹é½ï¼ˆä¸Ž pipeline ä¹ æƒ¯ä¸€è‡´ï¼‰
        L_new = min(pos_te1_new.shape[1], pos_te2_new.shape[1])
        pos_te1_new = pos_te1_new[:, :L_new, :]
        pos_te2_new = pos_te2_new[:, :L_new, :]
        pos_tokens_new = torch.cat([pos_te1_new, pos_te2_new], dim=-1)

        # ç»„è£… kwargsï¼ˆæ³¨æ„ï¼špooled ç›´æŽ¥ç”¨ encode_prompt çš„ç»“æžœï¼Œä»¥ä¿æŒç¨³å®šï¼‰
        return {
            'prompt_embeds': pos_tokens_new.to(device=device, dtype=torch_dtype),
            'negative_prompt_embeds': base_neg_tokens.to(device=device, dtype=torch_dtype),
            'pooled_prompt_embeds': base_pos_pooled.to(device=device, dtype=torch_dtype) if base_pos_pooled is not None else None,
            'negative_pooled_prompt_embeds': base_neg_pooled.to(device=device, dtype=torch_dtype) if base_neg_pooled is not None else None,
        }

    for i in range(T):
        rec = raw_prompts[i]
        pos = (rec.get("positive", "") + ", " + rec.get("style", "")).strip(", ")
        neg = rec.get("negative", "")
        
        try:
            # æž„é€  IP-Adapter embeddings: éœ€è¦ 4D tensor [batch, num_images, seq_len, embed_dim]
            # å¯¹äºŽ IP-Adapter-plusï¼Œé€šå¸¸æœŸæœ› [2, 1, 1, 1280] (uncond + cond)
            brain_embeds = E_tokens[i:i+1]  # [1, 1280]
            
            # åˆ›å»º negative (unconditioned) embeddings
            neg_embeds = torch.zeros_like(brain_embeds)  # [1, 1280]
            
            # å †å ä¸º [neg, pos] å¹¶è°ƒæ•´ä¸º 4D: [batch=2, num_images=1, seq_len=1, embed_dim=1280]
            ip_embeds = torch.cat([neg_embeds, brain_embeds], dim=0)  # [2, 1280]
            ip_embeds = ip_embeds.unsqueeze(1).unsqueeze(2)  # [2, 1, 1, 1280]
            
            # è‹¥å­˜åœ¨ soft promptï¼Œåˆ™æž„å»ºæ³¨å…¥åŽçš„ prompt_embedsï¼›å¦åˆ™äº¤ç»™ç®¡çº¿è‡ªè¡Œç¼–ç 
            if soft_prompt_instance is not None:
                prompt_kwargs = build_soft_prompt_embeds(pos, neg)
            else:
                prompt_kwargs = {'prompt': pos, 'negative_prompt': neg}

            # Debug: inspect prompt kwargs and ip_embeds types/shapes to find tuple issues
            print("[debug] About to call pipe with the following prompt kwargs:")
            for k, v in prompt_kwargs.items():
                if isinstance(v, torch.Tensor):
                    print(f"  {k}: Tensor shape={tuple(v.shape)}, dtype={v.dtype}")
                elif isinstance(v, (list, tuple)):
                    print(f"  {k}: {type(v)} len={len(v)}")
                    try:
                        for ii, vv in enumerate(v):
                            if isinstance(vv, torch.Tensor):
                                print(f"    [{ii}] Tensor shape={tuple(vv.shape)}, dtype={vv.dtype}")
                            else:
                                print(f"    [{ii}] type={type(vv)}")
                    except Exception:
                        pass
                else:
                    print(f"  {k}: type={type(v)}")

            if isinstance(ip_embeds, torch.Tensor):
                print(f"[debug] ip_embeds: Tensor shape={tuple(ip_embeds.shape)}, dtype={ip_embeds.dtype}")
            else:
                print(f"[debug] ip_embeds: type={type(ip_embeds)}")

            images = pipe(
                ip_adapter_image_embeds=[ip_embeds],
                num_inference_steps=args.steps,
                guidance_scale=args.cfg,
                width=args.w,
                height=args.h,
                **prompt_kwargs
            ).images
            
            img = images[0]
            out_path = os.path.join(args.out_dir, f"{i:02d}.png")
            img.save(out_path)
            print(f"[ok] {i:02d} -> {out_path}")

        except Exception as e:
            import traceback
            print(f"[error] {i:02d} ç”Ÿæˆå¤±è´¥: {type(e).__name__}: {e}")
            print(traceback.format_exc())
        
        finally:
            if device.type == 'cuda':
                gc.collect()
                torch.cuda.empty_cache()
                # print(f"  - (mem) Cleared CUDA cache for iteration {i}")

    print(f"[done] -> {args.out_dir}")

if __name__ == "__main__":
    main()
