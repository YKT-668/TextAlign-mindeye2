#!/usr/bin/env python
# coding: utf-8
"""
extract_all_features.py

åŠŸèƒ½:
  åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„ MindEye æ¨¡å‹ï¼Œä¸ºå¤šä¸ªè¢«è¯•æå–å¹¶åˆå¹¶ä»¥ä¸‹ä¸¤ç§ç‰¹å¾ï¼š
  1. å¤§è„‘è§£ç å‘é‡ (fMRI -> 1664D)
  2. å¯¹åº”çš„çœŸå€¼å›¾åƒçš„ ViT-H ç‰¹å¾ (Image -> 1024D)
  
  è¿™ä¸¤ä¸ªåˆå¹¶åçš„ç‰¹å¾æ–‡ä»¶ï¼Œå°†ä½œä¸ºè®­ç»ƒé€šç”¨æŠ•å½±çŸ©é˜µçš„è¾“å…¥ã€‚

ç”¨æ³•:
  python tools/extract_all_features.py \
    --mindeye_model_dir /path/to/pretrain_model \
    --out_dir /path/to/output_data_dir
"""
import glob
import os
import sys
import json
import argparse
import numpy as np
import h5py
from tqdm import tqdm
import torch
import torch.nn as nn
import webdataset as wds
from PIL import Image

# --- ä¸€äº›å¸¸é‡ï¼šå®˜æ–¹ MindEye2 çš„éšè—ç»´åº¦ & CLIP ç»´åº¦ ---
OFFICIAL_H = 4096          # å®˜æ–¹ final_subj01_pretrained_40sess_24bs ç”¨çš„æ˜¯ 4096
DEFAULT_H = 1024           # ä½ è‡ªå·±è®­ç»ƒçš„å°å·æ¨¡å‹ç”¨çš„æ˜¯ 1024ï¼Œå¯ä»¥ä¿ç•™å…¼å®¹
DEFAULT_N_BLOCKS = 4
CLIP_EMB_DIM = 1664
CLIP_SEQ_DIM = 256

# --- æ·»åŠ å¿…è¦çš„é¡¹ç›®è·¯å¾„ ---
script_dir = os.path.dirname(os.path.abspath(__file__))
proj_root = os.path.dirname(script_dir)
src_path = os.path.join(proj_root, 'src')
if src_path not in sys.path:
    sys.path.append(src_path)
if proj_root not in sys.path:
    sys.path.append(proj_root)

# --- å¯¼å…¥é¡¹ç›®æ¨¡å— ---
import importlib
utils = None
for mod_name in ("utils", "src.utils"):
    try:
        utils = importlib.import_module(mod_name)
        break
    except ModuleNotFoundError:
        continue
if utils is None:
    raise ImportError(
        f"æ— æ³•å¯¼å…¥ utilsã€‚å·²å°è¯•æ¨¡å—å ['utils','src.utils']ï¼Œå¹¶æ·»åŠ è·¯å¾„: {src_path} å’Œ {proj_root}. è¯·ç¡®è®¤é¡¹ç›®ç»“æ„ã€‚"
    )
from models import BrainNetwork, PriorNetwork, BrainDiffusionPrior
try:
    import open_clip
except ImportError:
    raise ImportError("open_clip not found. Please run `pip install open-clip-torch`.")

# ==============================================================================
# Â§1. å‚æ•°è§£æ
# ==============================================================================
parser = argparse.ArgumentParser(description="ä¸ºè®­ç»ƒé€šç”¨æŠ•å½±çŸ©é˜µæ‰¹é‡æå–å¤šè¢«è¯•ç‰¹å¾")
parser.add_argument(
    "--mindeye_model_dir", type=str, required=True,
    help="æŒ‡å‘é¢„è®­ç»ƒMindEyeæ¨¡å‹ç›®å½•çš„è·¯å¾„ (åŒ…å« last.pth å’Œ å¯é€‰ args.json)"
)
parser.add_argument(
    "--out_dir", type=str, required=True,
    help="è¾“å‡ºåˆå¹¶åçš„ç‰¹å¾æ–‡ä»¶ (.pt) çš„ç›®å½•"
)
parser.add_argument(
    "--data_path", type=str, default="/home/vipuser/MindEyeV2_Project/src",
    help="NSDæ•°æ®é›†çš„æ ¹ç›®å½•"
)
parser.add_argument(
    "--subjects", type=int, nargs='+', default=list(range(1, 9)),
    help="è¦å¤„ç†çš„è¢«è¯•IDåˆ—è¡¨ï¼Œé»˜è®¤ä¸º 1 åˆ° 8"
)
parser.add_argument(
    "--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu",
    help="è¿è¡Œè®¾å¤‡"
)
parser.add_argument(
    "--split", type=str, default="test",
    choices=["train", "test"],
    help="ä½¿ç”¨å“ªä¸ªæ•°æ®åˆ’åˆ†ï¼š'train' æˆ– 'test'ï¼ˆé»˜è®¤ testï¼‰"
)
args = parser.parse_args()

# ==============================================================================
# Â§2. æ¨¡å‹åŠ è½½
# ==============================================================================

print(f"ğŸ§  åŠ è½½é¢„è®­ç»ƒçš„ MindEye æ¨¡å‹ä»: {args.mindeye_model_dir}")

# --- 2.1 è¯»å– / æ¨æ–­æ¨¡å‹é…ç½® ---
ckpt_path = os.path.join(args.mindeye_model_dir, 'last.pth')
if not os.path.exists(ckpt_path):
    raise FileNotFoundError(f"é”™è¯¯: åœ¨æŒ‡å®šç›®å½•ä¸­æ‰¾ä¸åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶ 'last.pth': {args.mindeye_model_dir}")

# é»˜è®¤å…ˆå‡è®¾æ˜¯ä½ è‡ªå·±è®­ç»ƒçš„å°æ¨¡å‹
hidden_dim = DEFAULT_H
n_blocks = DEFAULT_N_BLOCKS

args_json_path = os.path.join(args.mindeye_model_dir, 'args.json')
if os.path.exists(args_json_path):
    # å¦‚æœç›®å½•é‡Œæœ‰ args.jsonï¼Œå°±æŒ‰é‡Œé¢çš„ä¿¡æ¯æ¥ï¼ˆå…¼å®¹ä½ è‡ªå·±è®­ç»ƒçš„æ¨¡å‹ï¼‰
    with open(args_json_path, 'r') as f:
        model_args = json.load(f)
    hidden_dim = model_args.get('hidden_dim', DEFAULT_H)
    n_blocks = model_args.get('n_blocks', DEFAULT_N_BLOCKS)
    print(f"   - æ¨¡å‹é…ç½®æ¥è‡ª args.json: hidden_dim={hidden_dim}, n_blocks={n_blocks}")
else:
    # æ²¡æœ‰ args.json çš„æƒ…å†µï¼šå¾ˆå¤§æ¦‚ç‡æ˜¯å®˜æ–¹ final_subj01_pretrained_40sess_24bs
    # è¿™é‡Œæˆ‘ä»¬æ ¹æ®ç›®å½•ååšä¸€ä¸ªç®€å•çš„ heuristics
    if "final_subj01_pretrained_40sess_24bs" in os.path.basename(args.mindeye_model_dir):
        hidden_dim = OFFICIAL_H
        n_blocks = DEFAULT_N_BLOCKS
        print("   - è­¦å‘Š: æ‰¾ä¸åˆ° args.jsonï¼Œæ£€æµ‹åˆ°æ˜¯å®˜æ–¹ subj01 40sess æ¨¡å‹ï¼Œä½¿ç”¨ OFFICIAL_H=4096.")
    else:
        hidden_dim = DEFAULT_H
        n_blocks = DEFAULT_N_BLOCKS
        print("   - è­¦å‘Š: æ‰¾ä¸åˆ° args.jsonï¼Œä½¿ç”¨é»˜è®¤é…ç½®: hidden_dim=1024, n_blocks=4.")

# --- 2.2 åŠ è½½æ‰€æœ‰è¢«è¯•ä½“ç´ æ•°ï¼Œç”¨äºæ„å»º ridge å¤´ ---
num_voxels_list = []
for s in range(1, 9):
    try:
        f = h5py.File(f'{args.data_path}/betas_all_subj0{s}_fp32_renorm.hdf5', 'r')
        num_voxels_list.append(f['betas'].shape[1])
    except FileNotFoundError:
        # æŸä¸ªè¢«è¯•ç¼ºæ•°æ®æ—¶ç”¨å ä½ç¬¦å¡«å……ï¼Œä¿è¯é•¿åº¦ä¸º8
        num_voxels_list.append(10000)

# --- 2.3 æ„å»º MindEye æ¨¡å— (åªè¦ ridge + backbone å°±å¤Ÿæ brain_clip) ---
class RidgeRegression(nn.Module):
    def __init__(self, input_sizes, out_features):
        super().__init__()
        self.linears = nn.ModuleList([nn.Linear(s, out_features) for s in input_sizes])

    def forward(self, x, subj_idx: int):
        """
        x æœŸæœ›å½¢çŠ¶:
          - [B, n_vox]
          - æˆ– [B, 1, n_vox]ï¼ˆå¤šä¸€ç»´ä¹Ÿå…¼å®¹ï¼‰
        è¿”å›:
          - [B, 1, hidden_dim]
        """
        # å¦‚æœæ˜¯ [B, 1, n_vox]ï¼Œå‹æ‰ä¸­é—´é‚£ä¸€ç»´
        if x.ndim == 3:
            x = x[:, 0, :]          # -> [B, n_vox]
        elif x.ndim == 1:
            x = x.unsqueeze(0)      # -> [1, n_vox]

        out = self.linears[subj_idx](x)  # [B, hidden_dim]
        return out.unsqueeze(1)          # [B, 1, hidden_dim]


class MindEyeModule(nn.Module):
    def __init__(self):
        super().__init__()

model = MindEyeModule()
model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim)
model.backbone = BrainNetwork(
    h=hidden_dim,
    in_dim=hidden_dim,
    seq_len=1,
    n_blocks=n_blocks,
    clip_size=CLIP_EMB_DIM,
    out_dim=CLIP_EMB_DIM * CLIP_SEQ_DIM,
    blurry_recon=False,   # è¿™é‡Œä¸å¼€ blurry reconï¼Œåªç”¨è¯­ä¹‰ä¸»å¹²æ brain_clip å³å¯
    clip_scale=1,
)
model.to(args.device)

# --- 2.4 åŠ è½½æƒé‡ï¼šç”¨ strict=False å…è®¸å¤šå‡ºæ¥çš„æ¨¡å— (diffusion_prior, blurry åˆ†æ”¯ç­‰) ---
checkpoint = torch.load(ckpt_path, map_location='cpu')
state_dict = checkpoint["model_state_dict"]

# ç›´æ¥ç”¨ strict=False åŠ è½½ï¼Œè·³è¿‡æœªç”¨åˆ°çš„æ¨¡å—é”®
load_msg = model.load_state_dict(state_dict, strict=False)
print("   - load_state_dict ç»“æœ:", load_msg)
model.eval()
print("   - âœ… MindEye æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸã€‚")

# --- 2.5 åŠ è½½ ViT-H å›¾åƒç¼–ç å™¨ ---
print("\nğŸ–¼ï¸  åŠ è½½ ViT-H/14 å›¾åƒç¼–ç å™¨ (ç”¨äºç”Ÿæˆç›®æ ‡ç‰¹å¾)...")
vith_model, _, vith_preprocess = open_clip.create_model_and_transforms(
    "ViT-H-14", pretrained="laion2b_s32b_b79k", device=args.device
)
vith_model.eval()
print("   - âœ… ViT-H/14 åŠ è½½æˆåŠŸã€‚")

# ==============================================================================
# Â§3. ç‰¹å¾æå–
# ==============================================================================
all_brain_vectors = []
all_image_vectors = []

print("\nğŸ’¾ æ‰“å¼€ COCO å›¾åƒæ•°æ®åº“...")
image_db = h5py.File(f'{args.data_path}/coco_images_224_float16.hdf5', 'r')
images_dataset = image_db['images']

def tensor_to_pil(t):
    if t.max() <= 1.0:
        t = t * 255.0
    return Image.fromarray(t.permute(1, 2, 0).to(torch.uint8).numpy())

# éå†æŒ‡å®šçš„æ¯ä¸ªè¢«è¯•
for subj_id in args.subjects:
    print(f"\n--- å¼€å§‹å¤„ç†è¢«è¯•: subj0{subj_id} ---")
    
    # 3.1 åŠ è½½ fMRI å’Œæµ‹è¯•é›†æ•°æ®
    try:
        f = h5py.File(f'{args.data_path}/betas_all_subj0{subj_id}_fp32_renorm.hdf5', 'r')
        voxels = torch.Tensor(f['betas'][:]).to('cpu')
    except FileNotFoundError:
        print(f"   - è­¦å‘Š: æ‰¾ä¸åˆ° subj0{subj_id} çš„fMRIæ•°æ®ï¼Œè·³è¿‡è¯¥è¢«è¯•ã€‚")
        continue

        # --- 3.1 åŠ è½½è¢«è¯•çš„ fMRI å’Œè¡Œä¸ºæ–‡ä»¶ï¼ˆtrain/test å¯é€‰ï¼‰ ---
    wds_root = os.path.join(args.data_path, "wds", f"subj0{subj_id}")

    # æ ¹æ® split é€‰æ‹©è¦è¯»çš„ tar æ–‡ä»¶
    urls = []
    if args.split == "test":
        # ä¼˜å…ˆç”¨ test/ï¼Œæ²¡æœ‰å°±ç”¨ new_test/
        for subdir in ["test", "new_test"]:
            pattern = os.path.join(wds_root, subdir, "*.tar")
            found = sorted(glob.glob(pattern))
            if found:
                urls = found
                print(f"   - ä½¿ç”¨ {subdir} splitï¼Œä¸‹æœ‰ {len(urls)} ä¸ª shard")
                break
    else:  # train
        pattern = os.path.join(wds_root, "train", "*.tar")
        urls = sorted(glob.glob(pattern))
        if urls:
            print(f"   - ä½¿ç”¨ train splitï¼Œä¸‹æœ‰ {len(urls)} ä¸ª shard")

    if not urls:
        print(f"   - è­¦å‘Š: åœ¨ {wds_root} ä¸‹æ‰¾ä¸åˆ° split='{args.split}' çš„ WebDatasetï¼Œè·³è¿‡è¯¥è¢«è¯•ã€‚")
        continue

    # å…ˆè·‘ä¸€éç»Ÿè®¡æ ·æœ¬æ•°ï¼Œå†é‡æ–°æ„å»ºä¸€æ¬¡ dataset ç»™ DataLoader ç”¨
    dataset = wds.WebDataset(urls).decode("torch").to_tuple("behav.npy")
    num_samples = sum(1 for _ in dataset)
    print(f"   - è¯¥ split æ€»æ ·æœ¬æ•°: {num_samples}")

    dataset = wds.WebDataset(urls).decode("torch").to_tuple("behav.npy")
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=num_samples, shuffle=False)

    # è·å–è¯¥ split å¯¹åº”çš„ fMRI ä½“ç´ å’Œå›¾åƒç´¢å¼•
    behav = next(iter(dataloader))[0]   # [N, 1, 6] ç±»ä¼¼
    subj_voxels = voxels[behav[:, 0, 5].long()]   # trial idx -> voxel è¡Œ
    subj_image_indices = behav[:, 0, 0].long()    # trial å¯¹åº”çš„å›¾åƒç´¢å¼•

    # å¯¹é‡å¤å›¾åƒå–å¹³å‡
    unique_indices, inverse_indices = torch.unique(subj_image_indices, return_inverse=True)
    print(f"   - æ‰¾åˆ° {len(unique_indices)} ä¸ªå”¯ä¸€å›¾åƒæ ·æœ¬ã€‚")
    
    # 3.2 æå–å¤§è„‘ & å›¾åƒç‰¹å¾
    with torch.no_grad(), torch.cuda.amp.autocast(enabled=(args.device == "cuda")):
        for i in tqdm(range(len(unique_indices)), desc=f"   - æå–ç‰¹å¾ (Subj {subj_id})"):
            img_idx = unique_indices[i]
            
            # æ‰€æœ‰å¯¹åº”çš„ fMRI é‡å¤
            # æ‰¾åˆ°è¯¥å›¾åƒå¯¹åº”çš„æ‰€æœ‰ fMRI é‡å¤
            fmri_locs = (subj_image_indices == img_idx).nonzero(as_tuple=False).view(-1)
            if fmri_locs.numel() == 0:
                continue

            # å–å‡ºè¿™äº›é‡å¤çš„ä½“ç´ æ•°æ®
            fmri_samples = subj_voxels[fmri_locs]  # é¢„æœŸå½¢çŠ¶: [K, n_vox]ï¼Œä½†ä¹Ÿå…¼å®¹ [K, 1, n_vox]
            if fmri_samples.ndim == 3:
                # å¦‚æœå¤šäº†ä¸€ç»´ï¼Œå‹æ‰ä¸­é—´é‚£ä¸€ç»´ -> [K, n_vox]
                fmri_samples = fmri_samples[:, 0, :]

            # åœ¨â€œé‡å¤ç»´åº¦ Kâ€ä¸Šåšå¹³å‡ï¼Œä¸è¦åŠ¨ voxel ç»´
            avg_voxel = fmri_samples.mean(dim=0, keepdim=True)   # [1, n_vox]
            avg_voxel = avg_voxel.to(args.device)

            # 1. æå–å¤§è„‘è§£ç å‘é‡ (fMRI -> 1664D)
            ridge_out = model.ridge(avg_voxel, subj_id - 1)      # [1, 1, H]
            _, brain_vec, _ = model.backbone(ridge_out)          # [1, 256, 1664]
            brain_vec_1664 = brain_vec.mean(dim=1)               # [1, 1664]
            all_brain_vectors.append(brain_vec_1664.cpu())

            
            # 2) çœŸå€¼å›¾åƒ -> ViT-H ç‰¹å¾ (shape [1, 1024])
            gt_image_data = torch.from_numpy(images_dataset[img_idx.item()]).float()
            pil_img = tensor_to_pil(gt_image_data)
            vith_image_input = vith_preprocess(pil_img).unsqueeze(0).to(args.device)
            image_vec_1024 = vith_model.encode_image(vith_image_input)
            all_image_vectors.append(image_vec_1024.cpu())

image_db.close()

# ==============================================================================
# Â§4. åˆå¹¶å¹¶ä¿å­˜
# ==============================================================================
if not all_brain_vectors or not all_image_vectors:
    print("\nâŒ é”™è¯¯: æœªèƒ½æˆåŠŸæå–ä»»ä½•ç‰¹å¾ã€‚è¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œæ•°æ®è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
    sys.exit(1)

print("\n--- ç‰¹å¾æå–å®Œæˆï¼Œæ­£åœ¨åˆå¹¶å’Œä¿å­˜... ---")

final_brain_vectors = torch.cat(all_brain_vectors, dim=0)
final_image_vectors = torch.cat(all_image_vectors, dim=0)

final_brain_vectors = nn.functional.normalize(final_brain_vectors, dim=1)
final_image_vectors = nn.functional.normalize(final_image_vectors, dim=1)

os.makedirs(args.out_dir, exist_ok=True)

out_brain_path = os.path.join(args.out_dir, "all_subjects_brain_vectors.pt")
out_image_path = os.path.join(args.out_dir, "all_subjects_gt_vith.pt")

torch.save(final_brain_vectors, out_brain_path)
torch.save(final_image_vectors, out_image_path)

print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")
print(f"âœ… é€šç”¨å¤§è„‘è§£ç å‘é‡å·²ä¿å­˜: {out_brain_path}")
print(f"   - å½¢çŠ¶: {final_brain_vectors.shape}")
print(f"âœ… é€šç”¨å›¾åƒç›®æ ‡å‘é‡å·²ä¿å­˜: {out_image_path}")
print(f"   - å½¢çŠ¶: {final_image_vectors.shape}")
print("\nç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›æ–‡ä»¶æ¥è®­ç»ƒä¸€ä¸ªé€šç”¨çš„æŠ•å½±çŸ©é˜µäº†ã€‚")
