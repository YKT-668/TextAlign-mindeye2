#!/usr/bin/env python
import os
import json
import argparse
from openai import OpenAI
from tqdm import tqdm
import time
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from queue import Queue
import random

# --- 1. å®šä¹‰ä¸ DeepSeek API äº¤äº’çš„æ ¸å¿ƒå‡½æ•° ---
def get_structured_prompt_from_deepseek(topk_texts, client, max_retries=2):
    """
    è°ƒç”¨ DeepSeek APIï¼Œå°†ä¸€ç»„Top-Kæ–‡æœ¬è½¬æ¢ä¸ºä¸€ä¸ªç»“æ„åŒ–çš„promptã€‚
    æ·»åŠ äº†é‡è¯•æœºåˆ¶å’Œæ›´çŸ­çš„è¶…æ—¶ã€‚
    """
    # ç²¾å¿ƒè®¾è®¡çš„"ç³»ç»ŸæŒ‡ä»¤"ï¼Œå‘Šè¯‰DeepSeekå®ƒçš„è§’è‰²å’Œä»»åŠ¡
    system_prompt = """
You are an expert AI art prompt engineer. Your task is to synthesize a set of descriptive sentences about a scene into a high-quality, structured prompt for a text-to-image model like Stable Diffusion XL.

The user will provide a list of sentences under "Top-K Descriptions".

Your response MUST be a single, valid JSON object, with no other text before or after it.
The JSON object must have exactly two keys:
1. "positive": A concise, vivid, and coherent description of the main scene. Focus on key objects, their actions, and the overall atmosphere. Do NOT use superlative or generic art terms like '4k', '8k', 'highly detailed', 'cinematic', 'masterpiece'.
2. "negative": A standard list of negative keywords to avoid common image generation artifacts.

Example Input:
[
    "A black and white photo of a man in a suit and tie.",
    "A man in a suit and tie is standing in front of a building.",
    "A man in a suit and tie is looking at the camera.",
    "A black and white photo of a man in a suit.",
    "A man in a suit and tie is standing in front of a building with a clock on it."
]

Example Output:
{
  "positive": "A man in a black suit and tie standing in front of a building with a large clock, looking at the camera. Black and white photo.",
  "negative": "blurry, low quality, artifacts, extra limbs, text, watermark, copyright, deformed, mutated, ugly"
}
"""
    
    # å°†Top-Kæ–‡æœ¬åˆ—è¡¨æ ¼å¼åŒ–ä¸ºç”¨æˆ·è¾“å…¥
    user_content = "Please synthesize the following descriptions into a structured prompt:\n\nTop-K Descriptions:\n" + json.dumps(topk_texts, indent=2)

    for attempt in range(max_retries + 1):
        try:
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content},
                ],
                stream=False,
                max_tokens=300,  # å‡å°‘tokenæ•°é‡ä»¥æé€Ÿ
                temperature=0.3,  # è¿›ä¸€æ­¥é™ä½æ¸©åº¦æé€Ÿ
                timeout=15,  # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
                response_format={"type": "json_object"}, # è¯·æ±‚JSONè¾“å‡º
            )
            
            # è§£æè¿”å›çš„JSONå­—ç¬¦ä¸²
            message_content = response.choices[0].message.content
            structured_prompt = json.loads(message_content)
            
            # éªŒè¯è¿”å›çš„JSONæ˜¯å¦ç¬¦åˆæˆ‘ä»¬çš„æœŸæœ›
            if "positive" in structured_prompt and "negative" in structured_prompt:
                return structured_prompt
            else:
                if attempt == max_retries:
                    print(f"âš ï¸ Warning: DeepSeek response missing required keys. Got: {message_content}")
                return None

        except Exception as e:
            if attempt == max_retries:
                print(f"âŒ Error calling DeepSeek API after {max_retries + 1} attempts: {e}")
                return None
            # éšæœºå»¶è¿Ÿé‡è¯•ï¼Œé¿å…åŒæ—¶é‡è¯•
            time.sleep(random.uniform(0.1, 0.5))
    
    return None

def process_batch_worker(batch_data, client, results_queue, progress_queue):
    """
    å·¥ä½œçº¿ç¨‹å‡½æ•°ï¼Œå¤„ç†ä¸€æ‰¹APIè¯·æ±‚
    """
    batch_results = []
    for rec in batch_data:
        topk_texts = rec.get("topk", [])
        if not topk_texts:
            continue

        # è°ƒç”¨ DeepSeek API
        structured_result = get_structured_prompt_from_deepseek(topk_texts, client)
        
        if structured_result:
            # å°†è¿”å›çš„ç»“æœä¸åŸå§‹IDç»“åˆ
            structured_result['id'] = rec.get("id")
            batch_results.append(structured_result)
        
        # æ›´æ–°è¿›åº¦
        progress_queue.put(1)
        
        # å‡å°‘å»¶è¿Ÿï¼Œä»…åœ¨å¿…è¦æ—¶æš‚åœ
        time.sleep(0.05)  # æçŸ­æš‚åœï¼Œé¿å…è¿‡è½½
    
    results_queue.put(batch_results)

# --- 2. ä¸»å‡½æ•° ---
def main():
    ap = argparse.ArgumentParser(description="Generate structured prompts from Top-K texts using DeepSeek LLM.")
    ap.add_argument("--topk_jsonl", required=True, help="Path to the input JSONL file from the retrieval step.")
    ap.add_argument("--out_json", required=True, help="Path to the output JSON file for the final prompts.")
    # API Key å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡æˆ–ç›´æ¥å‚æ•°ä¼ å…¥
    ap.add_argument("--api_key", default=os.environ.get('DEEPSEEK_API_KEY'), help="DeepSeek API Key.")
    ap.add_argument("--max_workers", type=int, default=8, help="Number of concurrent workers for API calls.")
    ap.add_argument("--batch_size", type=int, default=50, help="Batch size for processing records.")
    args = ap.parse_args()

    if not args.api_key:
        raise ValueError("DeepSeek API Key not found. Please set the DEEPSEEK_API_KEY environment variable or provide it via --api_key.")

    print(f"ğŸš€ Starting with {args.max_workers} workers, batch size {args.batch_size}")

    # è¯»å–è¾“å…¥çš„ JSONL æ–‡ä»¶
    recs = []
    with open(args.topk_jsonl, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                recs.append(json.loads(line))
    
    print(f"ğŸ“Š Processing {len(recs)} records")
    
    # åˆ†æ‰¹å¤„ç†æ•°æ®
    batches = [recs[i:i + args.batch_size] for i in range(0, len(recs), args.batch_size)]
    
    final_prompts = []
    results_queue = Queue()
    progress_queue = Queue()
    
    # åˆ›å»ºè¿›åº¦æ¡
    total_records = len(recs)
    pbar = tqdm(total=total_records, desc="Generating Prompts with DeepSeek")
    
    def progress_updater():
        """æ›´æ–°è¿›åº¦æ¡çš„çº¿ç¨‹"""
        processed = 0
        while processed < total_records:
            try:
                progress_queue.get(timeout=1)
                processed += 1
                pbar.update(1)
            except:
                continue
        pbar.close()
    
    # å¯åŠ¨è¿›åº¦æ›´æ–°çº¿ç¨‹
    progress_thread = threading.Thread(target=progress_updater, daemon=True)
    progress_thread.start()
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ‰¹æ¬¡
    with ThreadPoolExecutor(max_workers=args.max_workers) as executor:
        # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„å®¢æˆ·ç«¯å®ä¾‹
        clients = [OpenAI(
            api_key=args.api_key,
            base_url="https://api.deepseek.com"
        ) for _ in range(args.max_workers)]
        
        # æäº¤æ‰¹æ¬¡ä»»åŠ¡
        futures = []
        for i, batch in enumerate(batches):
            client = clients[i % len(clients)]  # å¾ªç¯ä½¿ç”¨å®¢æˆ·ç«¯
            future = executor.submit(process_batch_worker, batch, client, results_queue, progress_queue)
            futures.append(future)
        
        # æ”¶é›†æ‰€æœ‰ç»“æœ
        completed_batches = 0
        for future in as_completed(futures):
            try:
                future.result()  # ç¡®ä¿ä»»åŠ¡å®Œæˆ
                completed_batches += 1
            except Exception as e:
                print(f"âŒ Batch processing error: {e}")
    
    # æ”¶é›†æ‰€æœ‰ç»“æœ
    while not results_queue.empty():
        batch_results = results_queue.get()
        final_prompts.extend(batch_results)
    
    # ç­‰å¾…è¿›åº¦æ›´æ–°çº¿ç¨‹å®Œæˆ
    progress_thread.join(timeout=2)
    
    print(f"\nğŸ“ˆ Performance: Processed {len(recs)} records, generated {len(final_prompts)} prompts")
    print(f"ğŸ“Š Success rate: {len(final_prompts)}/{len(recs)} ({len(final_prompts)/len(recs)*100:.1f}%)")

    # ä¿å­˜æœ€ç»ˆç»“æœ
    os.makedirs(os.path.dirname(args.out_json), exist_ok=True)
    json.dump(final_prompts, open(args.out_json, "w"), ensure_ascii=False, indent=2)
    print(f"\nâœ“ WROTE (AI Generated): {args.out_json} ({len(final_prompts)} prompts)")

if __name__ == "__main__":
    main()
