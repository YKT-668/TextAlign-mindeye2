#è¿™ä¸ªè„šæœ¬æ˜¯äºŒæ¬¡å®éªŒæ—¶ï¼Œåˆ©ç”¨ç§»é™¤æŠ•å½±å±‚çš„ ViT-bigG-14 æ¨¡å‹ï¼Œå°† 1000 å¼ æµ‹è¯•é›†å›¾ç‰‡é‡æ–°ç¼–ç ä¸º 1664 ç»´å‘é‡ï¼Œä»è€Œç”Ÿæˆä¸ä½ è„‘ç‰¹å¾ç»´åº¦å®Œå…¨å¯¹é½çš„è¯„æµ‹æ ‡å‡†ç­”æ¡ˆï¼ˆGround Truthï¼‰
import torch
import numpy as np
import open_clip
from PIL import Image
import os
from tqdm import tqdm

# === é…ç½®åŒº ===
IMAGES_PT = "src/evals/all_images.pt"
OUTPUT_NPY = "/mnt/work/data_cache/clip_img_gt.npy"

# å…³é”®ä¿®æ­£ 1: ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹
MODEL_NAME = "ViT-bigG-14"
PRETRAINED = "laion2b_s39b_b160k" 

if not os.path.exists(IMAGES_PT):
    print(f"âŒ æ‰¾ä¸åˆ° {IMAGES_PT}")
    exit()

print(f"ğŸš€ æ­£åœ¨åŠ è½½ OpenCLIP ({MODEL_NAME})...")
# åŠ è½½æ¨¡å‹
model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=PRETRAINED)
model.eval()

# å…³é”®ä¿®æ­£ 2: é˜‰å‰²æ‰æŠ•å½±å±‚ (Projection Layer)
# æ ‡å‡† bigG è¾“å‡ºæ˜¯ 1280ï¼Œä½†æˆ‘ä»¬éœ€è¦ transformer åŸå§‹å®½åº¦ 1664
if hasattr(model.visual, 'proj'):
    print(f"âœ‚ï¸  æ£€æµ‹åˆ°æŠ•å½±å±‚ (shape={model.visual.proj.shape})ï¼Œæ­£åœ¨ç§»é™¤ä»¥è·å– 1664 ç»´ç‰¹å¾...")
    model.visual.proj = None
else:
    print("âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°æŠ•å½±å±‚ï¼Œæ¨¡å‹å¯èƒ½å·²ç»æ˜¯æ— æŠ•å½±ç‰ˆæœ¬ã€‚")

# åŠ è½½å›¾ç‰‡
print("ğŸ“‚ æ­£åœ¨åŠ è½½æµ‹è¯•é›†å›¾ç‰‡ Tensor...")
images_tensor = torch.load(IMAGES_PT)
if images_tensor.max() > 1.0:
    images_tensor = images_tensor.float() / 255.0

# å½’ä¸€åŒ–å‚æ•° (OpenCLIP æ ‡å‡†)
mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(1, 3, 1, 1)
std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(1, 3, 1, 1)

batch_size = 16
all_embs = []

print("âš¡ å¼€å§‹è®¡ç®— 1664 ç»´ç‰¹å¾ (CPUæ¨¡å¼)...")
with torch.no_grad():
    for i in tqdm(range(0, len(images_tensor), batch_size)):
        batch = images_tensor[i : i + batch_size]
        
        # Resize åˆ° 224x224 (bigG å¯èƒ½æ”¯æŒæ›´é«˜åˆ†è¾¨ç‡ï¼Œä½† MindEye é»˜è®¤ä¸º 224)
        if batch.shape[-1] != 224:
             import torch.nn.functional as F
             batch = F.interpolate(batch, size=(224, 224), mode='bicubic')

        # å½’ä¸€åŒ–
        batch_norm = (batch - mean) / std
        
        # ç¼–ç  (å› ä¸º proj=Noneï¼Œè¿™é‡Œä¼šè‡ªåŠ¨è¾“å‡º 1664 ç»´)
        embs = model.encode_image(batch_norm)
        
        # å½’ä¸€åŒ– embedding (è™½ç„¶æ²¡æœ‰æŠ•å½±ï¼Œä½†é€šå¸¸è¿˜æ˜¯åšä¸ª L2 norm æ¯”è¾ƒå®‰å…¨ï¼Œæˆ–è€…ä¿æŒåŸå§‹)
        # MindEye2 è¿™é‡Œé€šå¸¸ç›´æ¥ç”¨åŸå§‹ç‰¹å¾åš MSEï¼Œæˆ–è€… Norm ååš Cosine
        # ä¸ºäº† Retrieveï¼Œæˆ‘ä»¬é€šå¸¸åš Norm
        embs = embs / embs.norm(dim=-1, keepdim=True)
        
        all_embs.append(embs.cpu().numpy())

final_arr = np.concatenate(all_embs, axis=0)
np.save(OUTPUT_NPY, final_arr)
print(f"âœ… æˆåŠŸç”Ÿæˆä¿®æ­£ç‰ˆ clip_img_gt.npy")
print(f"ğŸ“Š æœ€ç»ˆå½¢çŠ¶: {final_arr.shape} (é¢„æœŸåº”è¯¥æ˜¯ 1000, 1664)")